{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OpenAI Cookbook","text":"<p>The OpenAI Cookbook shares example code for accomplishing common tasks with the OpenAI API.</p> <p>To run these examples, you'll need an OpenAI account and associated API key (create a free account).</p> <p>Most code examples are written in Python, though the concepts can be applied in any language.</p>"},{"location":"#recently-added","title":"Recently added \ud83c\udd95 \u2728","text":"<ul> <li>How to format inputs to ChatGPT models [Mar 1st, 2023]</li> <li>Using Vector Databases for Embeddings Search with Redis [Feb 15th, 2023]</li> <li>Website Q&amp;A with Embeddings [Feb 11th, 2023]</li> <li>File Q&amp;A with Embeddings [Feb 11th, 2023]</li> <li>Visualize Embeddings in Weights &amp; Biases [Feb 9th, 2023]</li> <li>Retrieval Enhanced Generative Question Answering with Pinecone [Feb 8th, 2023]</li> </ul>"},{"location":"#guides-examples","title":"Guides &amp; examples","text":"<ul> <li>API usage</li> <li>How to handle rate limits<ul> <li>Example parallel processing script that avoids hitting rate limits</li> </ul> </li> <li>How to count tokens with tiktoken</li> <li>How to stream completions</li> <li>ChatGPT</li> <li>How to format inputs to ChatGPT models</li> <li>GPT-3</li> <li>Guide: How to work with large language models</li> <li>Guide: Techniques to improve reliability</li> <li>How to use a multi-step prompt to write unit tests</li> <li>Text writing examples</li> <li>Text explanation examples</li> <li>Text editing examples</li> <li>Code writing examples</li> <li>Code explanation examples</li> <li>Code editing examples</li> <li>Embeddings</li> <li>Text comparison examples</li> <li>How to get embeddings</li> <li>Question answering using embeddings</li> <li>Semantic search using embeddings</li> <li>Recommendations using embeddings</li> <li>Clustering embeddings</li> <li>Visualizing embeddings in 2D or 3D</li> <li>Embedding long texts</li> <li>Fine-tuning GPT-3</li> <li>Guide: best practices for fine-tuning GPT-3 to classify text</li> <li>Fine-tuned classification</li> <li>DALL-E</li> <li>How to generate and edit images with DALL-E</li> <li>Azure OpenAI (alternative API from Microsoft Azure)</li> <li>How to get completions from Azure OpenAI</li> <li>How to get embeddings from Azure OpenAI</li> <li>How to fine-tune GPT-3 with Azure OpenAI</li> <li>Apps</li> <li>File Q and A</li> <li>Web Crawl Q and A</li> </ul>"},{"location":"#related-resources","title":"Related resources","text":"<p>Beyond the code examples here, you can learn about the OpenAI API from the following resources:</p> <ul> <li>Try out the API in the OpenAI Playground</li> <li>Read about the API in the OpenAI Documentation</li> <li>Discuss the API in the OpenAI Community Forum</li> <li>Look for help in the OpenAI Help Center</li> <li>See example prompts in the OpenAI Examples</li> <li>Play with a free research preview of ChatGPT</li> <li>Stay up to date with the OpenAI Blog</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>If there are examples or guides you'd like to see, feel free to suggest them on the issues page.</p>"},{"location":"code_editing_examples/","title":"Code editing example","text":"<p>OpenAI's edits endpoint is particularly useful for editing code.</p> <p>Unlike completions, edits takes two inputs: the text to edit and an instruction.</p> <p>For example, if you wanted to edit a Python function, you could supply the text of the function and an instruction like \"add a docstring\".</p> <p>Example text input to <code>code-davinci-edit-001</code>:</p> <pre><code>def tribonacci(n):\n    if n == 0:\n        return 0\n    elif n == 1:\n        return 1\n    elif n == 2:\n        return 1\n    elif n == 3:\n        return 2\n    else:\n        return tribonacci(n-1) + tribonacci(n-2) + tribonacci(n-3)\n</code></pre> <p>Example instruction inputs:</p> <pre><code>add a docstring\n</code></pre> <pre><code>Add typing, using Python 3.9 conventions\n</code></pre> <pre><code>improved the runtime\n</code></pre> <pre><code>Add a test.\n</code></pre> <pre><code>Translate to JavaScript (or Rust or Lisp or any language you like)\n</code></pre> <p>Example output after improving the runtime and translating to JavaScript:</p> <pre><code>function tribonacci(n) {\nlet a = 0;\nlet b = 1;\nlet c = 1;\nfor (let i = 0; i &lt; n; i++) {\n[a, b, c] = [b, c, a + b + c];\n}\nreturn a;\n}\n</code></pre> <p>As you can see, <code>code-davinci-edit-001</code> was able to successfully reduce the function's runtime from exponential down to linear, as well as convert from Python to JavaScript.</p> <p>Experiment with code editing using <code>code-davinci-edit-001</code> in the OpenAI Playground.</p>"},{"location":"code_explanation_examples/","title":"Code explanation examples","text":"<p>GPT's understanding of code can be applied to many use cases, e.g.:</p> <ul> <li>Generating in-code documentation (e.g., Python docstrings, git commit messages)</li> <li>Generating out-of-code documentation (e.g., man pages)</li> <li>An interactive code exploration tool</li> <li>Communicating program results back to users via a natural language interface</li> </ul> <p>For example, if you wanted to understand a SQL query, you could give <code>code-davinci-002</code> the following example prompt:</p> <pre><code>A SQL query:\n```\nSELECT c.customer_id\nFROM Customers c\nJOIN Streaming s\nON c.customer_id = s.customer_id\nWHERE c.signup_date BETWEEN '2020-03-01' AND '2020-03-31'\nAND s.watch_date BETWEEN c.signup_date AND DATE_ADD(c.signup_date, INTERVAL 30 DAY)\nGROUP BY c.customer_id\nHAVING SUM(s.watch_minutes) &gt; 50 * 60\n```\n\nQuestions:\n1. What does the SQL query do?\n2. Why might someone be interested in this time period?\n3. Why might a company be interested in this SQL query?\n\nAnswers:\n</code></pre> <p>Output:</p> <pre><code>1. The SQL query finds all customers who signed up in March 2020 and watched more than 50 hours of content in the first 30 days after signing up.\n2. The time period is interesting because it is the first month of the COVID-19 pandemic.\n3. A company might be interested in this SQL query because it can help them understand how the pandemic has affected their business.\n</code></pre> <p>Note that <code>code-davinci-002</code> is not trained to follow instructions and therefore usually needs examples or other structure to help steer its output, as well as stop sequences to stop generating. For easier prompting, try <code>text-davinci-003</code>.</p>"},{"location":"code_writing_examples/","title":"Code writing examples","text":"<p>GPT-3 is able to write code as well as text.</p> <p>Here's an example of <code>code-davinci-002</code> writing a SQL query:</p> <pre><code>SQL tables (and columns):\n* Customers(customer_id, signup_date)\n* Streaming(customer_id, video_id, watch_date, watch_minutes)\n\nA well-written SQL query that lists customers who signed up during March 2020 and watched more than 50 hours of video in their first 30 days:\n```\n</code></pre> <p>Output:</p> <pre><code>SELECT c.customer_id\nFROM Customers c\nJOIN Streaming s\nON c.customer_id = s.customer_id\nWHERE c.signup_date BETWEEN '2020-03-01' AND '2020-03-31'\nAND s.watch_date BETWEEN c.signup_date AND DATE_ADD(c.signup_date, INTERVAL 30 DAY)\nGROUP BY c.customer_id\nHAVING SUM(s.watch_minutes) &gt; 50 * 60\n</code></pre> <p>Helpfully, <code>code-davinci-002</code> is able to make inferences from variable names; for example, it infers that <code>watch_minutes</code> has units of minutes and therefore needs to be converted by a factor of 60 before being compared with 50 hours.</p> <p>For easier prompting, you can also try <code>text-davinci-003</code>.</p>"},{"location":"how_to_work_with_large_language_models/","title":"How to work with large language models","text":""},{"location":"how_to_work_with_large_language_models/#how-large-language-models-work","title":"How large language models work","text":"<p>Large language models are functions that map text to text. Given an input string of text, a large language model predicts the text that should come next.</p> <p>The magic of large language models is that by being trained to minimize this prediction error over vast quantities of text, the models end up learning concepts useful for these predictions. For example, they learn:</p> <ul> <li>how to spell</li> <li>how grammar works</li> <li>how to paraphrase</li> <li>how to answer questions</li> <li>how to hold a conversation</li> <li>how to write in many languages</li> <li>how to code</li> <li>etc.</li> </ul> <p>None of these capabilities are explicitly programmed in\u2014they all emerge as a result of training.</p> <p>GPT-3 powers hundreds of software products, including productivity apps, education apps, games, and more.</p>"},{"location":"how_to_work_with_large_language_models/#how-to-control-a-large-language-model","title":"How to control a large language model","text":"<p>Of all the inputs to a large language model, by far the most influential is the text prompt.</p> <p>Large language models can be prompted to produce output in a few ways:</p> <ul> <li>Instruction: Tell the model what you want</li> <li>Completion: Induce the model to complete the beginning of what you want</li> <li>Demonstration: Show the model what you want, with either:</li> <li>A few examples in the prompt</li> <li>Many hundreds or thousands of examples in a fine-tuning training dataset</li> </ul> <p>An example of each is shown below.</p>"},{"location":"how_to_work_with_large_language_models/#instruction-prompts","title":"Instruction prompts","text":"<p>Instruction-following models (e.g., <code>text-davinci-003</code> or any model beginning with <code>text-</code>) are specially designed to follow instructions. Write your instruction at the top of the prompt (or at the bottom, or both), and the model will do its best to follow the instruction and then stop. Instructions can be detailed, so don't be afraid to write a paragraph explicitly detailing the output you want.</p> <p>Example instruction prompt:</p> <pre><code>Extract the name of the author from the quotation below.\n\n\u201cSome humans theorize that intelligent species go extinct before they can expand into outer space. If they're correct, then the hush of the night sky is the silence of the graveyard.\u201d\n\u2015 Ted Chiang, Exhalation\n</code></pre> <p>Output:</p> <pre><code>Ted Chiang\n</code></pre>"},{"location":"how_to_work_with_large_language_models/#completion-prompt-example","title":"Completion prompt example","text":"<p>Completion-style prompts take advantage of how large language models try to write text they think is mostly likely to come next. To steer the model, try beginning a pattern or sentence that will be completed by the output you want to see. Relative to direct instructions, this mode of steering large language models can take more care and experimentation. In addition, the models won't necessarily know where to stop, so you will often need stop sequences or post-processing to cut off text generated beyond the desired output.</p> <p>Example completion prompt:</p> <pre><code>\u201cSome humans theorize that intelligent species go extinct before they can expand into outer space. If they're correct, then the hush of the night sky is the silence of the graveyard.\u201d\n\u2015 Ted Chiang, Exhalation\n\nThe author of this quote is\n</code></pre> <p>Output:</p> <pre><code> Ted Chiang\n</code></pre>"},{"location":"how_to_work_with_large_language_models/#demonstration-prompt-example-few-shot-learning","title":"Demonstration prompt example (few-shot learning)","text":"<p>Similar to completion-style prompts, demonstrations can show the model what you want it to do. This approach is sometimes called few-shot learning, as the model learns from a few examples provided in the prompt.</p> <p>Example demonstration prompt:</p> <pre><code>Quote:\n\u201cWhen the reasoning mind is forced to confront the impossible again and again, it has no choice but to adapt.\u201d\n\u2015 N.K. Jemisin, The Fifth Season\nAuthor: N.K. Jemisin\n\nQuote:\n\u201cSome humans theorize that intelligent species go extinct before they can expand into outer space. If they're correct, then the hush of the night sky is the silence of the graveyard.\u201d\n\u2015 Ted Chiang, Exhalation\nAuthor:\n</code></pre> <p>Output:</p> <pre><code> Ted Chiang\n</code></pre>"},{"location":"how_to_work_with_large_language_models/#fine-tuned-prompt-example","title":"Fine-tuned prompt example","text":"<p>With enough training examples, you can fine-tune a custom model. In this case, instructions become unnecessary, as the model can learn the task from the training data provided. However, it can be helpful to include separator sequences (e.g., <code>-&gt;</code> or <code>###</code> or any string that doesn't commonly appear in your inputs) to tell the model when the prompt has ended and the output should begin. Without separator sequences, there is a risk that the model continues elaborating on the input text rather than starting on the answer you want to see.</p> <p>Example fine-tuned prompt (for a model that has been custom trained on similar prompt-completion pairs):</p> <pre><code>\u201cSome humans theorize that intelligent species go extinct before they can expand into outer space. If they're correct, then the hush of the night sky is the silence of the graveyard.\u201d\n\u2015 Ted Chiang, Exhalation\n\n###\n</code></pre> <p>Output:</p> <pre><code> Ted Chiang\n</code></pre>"},{"location":"how_to_work_with_large_language_models/#code-capabilities","title":"Code Capabilities","text":"<p>Large language models aren't only great at text - they can be great at code too. OpenAI's specialized code model is called Codex.</p> <p>Codex powers more than 70 products, including:</p> <ul> <li>GitHub Copilot (autocompletes code in VS Code and other IDEs)</li> <li>Pygma (turns Figma designs into code)</li> <li>Replit (has an 'Explain code' button and other features)</li> <li>Warp (a smart terminal with AI command search)</li> <li>Machinet (writes Java unit test templates)</li> </ul> <p>Note that unlike instruction-following text models (e.g., <code>text-davinci-002</code>), Codex is not trained to follow instructions. As a result, designing good prompts can take more care.</p>"},{"location":"how_to_work_with_large_language_models/#more-prompt-advice","title":"More prompt advice","text":"<p>For more prompt examples, visit OpenAI Examples.</p> <p>In general, the input prompt is the best lever for improving model outputs. You can try tricks like:</p> <ul> <li>Give more explicit instructions. E.g., if you want the output to be a comma separated list, ask it to return a comma separated list. If you want it to say \"I don't know\" when it doesn't know the answer, tell it 'Say \"I don't know\" if you do not know the answer.'</li> <li>Supply better examples. If you're demonstrating examples in your prompt, make sure that your examples are diverse and high quality.</li> <li>Ask the model to answer as if it was an expert. Explicitly asking the model to produce high quality output or output as if it was written by an expert can induce the model to give higher quality answers that it thinks an expert would write. E.g., \"The following answer is correct, high-quality, and written by an expert.\"</li> <li>Prompt the model to write down the series of steps explaining its reasoning. E.g., prepend your answer with something like \"Let's think step by step.\" Prompting the model to give an explanation of its reasoning before its final answer can increase the likelihood that its final answer is consistent and correct.</li> </ul>"},{"location":"techniques_to_improve_reliability/","title":"Techniques to improve reliability","text":"<p>When GPT-3 fails on a task, what should you do?</p> <ul> <li>Search for a better prompt that elicits more reliable answers?</li> <li>Invest in thousands of examples to fine-tune a custom model?</li> <li>Assume the model is incapable of the task, and move on?</li> </ul> <p>There is no simple answer - it depends. However, if your task involves logical reasoning or complexity, consider trying the techniques in this article to build more reliable, high-performing prompts.</p>"},{"location":"techniques_to_improve_reliability/#why-gpt-3-fails-on-complex-tasks","title":"Why GPT-3 fails on complex tasks","text":"<p>If you were asked to multiply 13 by 17, would the answer pop immediately into your mind? For most of us, probably not. Yet, that doesn't mean humans are incapable of two-digit multiplication. With a few seconds, and some pen and paper, it's not too taxing to work out that 13 x 17 = 130 + 70 + 21 = 221.</p> <p>Similarly, if you give GPT-3 a task that's too complex to do in the time it takes to calculate its next token, it may confabulate an incorrect guess. Yet, akin to humans, that doesn't necessarily mean the model is incapable of the task. With some time and space to reason things out, the model still may be able to answer reliably.</p> <p>As an example, if you ask <code>text-davinci-002</code> the following math problem about juggling balls, it answers incorrectly:</p> <pre><code>Q: A juggler has 16 balls. Half of the balls are golf balls and half of the golf balls are blue. How many blue golf balls are there?\nA:\n</code></pre> <pre><code>There are 8 blue golf balls.\n</code></pre> <p>Does this mean that GPT-3 cannot do simple math problems? No; in fact, it turns out that by prompting the model with <code>Let's think step by step</code>, the model solves the problem reliably:</p> <pre><code>Q: A juggler has 16 balls. Half of the balls are golf balls and half of the golf balls are blue. How many blue golf balls are there?\nA: Let's think step by step.\n</code></pre> <pre><code>There are 16 balls in total. \nHalf of the balls are golf balls. \nThat means that there are 8 golf balls. \nHalf of the golf balls are blue. \nThat means that there are 4 blue golf balls.\n</code></pre> <p>Of course, it's hard to tell from only a single example whether this <code>Let's think step by step</code> trick actually works in general or just got lucky on this particular problem. But it really does work. On a benchmark of word math problems, the <code>Let's think step by step</code> trick raised GPT-3's solve rate massively, from a worthless 18% to a decent 79%!</p>"},{"location":"techniques_to_improve_reliability/#model-capabilities-depend-on-context","title":"Model capabilities depend on context","text":"<p>When learning to work with GPT-3, one common conceptual mistake is to believe that its capabilities are fixed across all contexts. E.g., if GPT-3 gets a simple logic question wrong, then it must be incapable of simple logic.</p> <p>But as the <code>Let's think step by step</code> example illustrates, apparent failures of GPT-3 can sometimes be remedied with a better prompt that helps the model steer itself toward the correct output.</p>"},{"location":"techniques_to_improve_reliability/#how-to-improve-reliability-on-complex-tasks","title":"How to improve reliability on complex tasks","text":"<p>The rest of this article shares techniques for improving reliability of large language models on complex tasks. Although some of the techniques are specific to certain types of problems, many of them are built upon general principles that can be applied to a wide range of tasks, e.g.:</p> <ul> <li>Give clearer instructions</li> <li>Split complex tasks into simpler subtasks</li> <li>Structure the instruction to keep the model on task</li> <li>Prompt the model to explain before answering</li> <li>Ask for justifications of many possible answers, and then synthesize</li> <li>Generate many outputs, and then use the model to pick the best one</li> <li>Fine-tune custom models to maximize performance</li> </ul>"},{"location":"techniques_to_improve_reliability/#split-complex-tasks-into-simpler-tasks","title":"Split complex tasks into simpler tasks","text":"<p>One way to give a model more time and space to think is to break tasks into simpler pieces.</p> <p>As an example, consider a task where we ask the model a multiple-choice question about some text - in this case, a game of Clue. When asked directly, <code>text-davinci-002</code> isn't able to put clues 3 &amp; 5 together, and answers incorrectly:</p> <pre><code>Use the following clues to answer the following multiple-choice question.\n\nClues:\n1. Miss Scarlett was the only person in the lounge.\n2. The person with the pipe was in the kitchen.\n3. Colonel Mustard was the only person in the observatory.\n4. Professor Plum was not in the library nor the billiard room.\n5. The person with the candlestick was in the observatory.\n\nQuestion: Was Colonel Mustard in the observatory with the candlestick?\n(a) Yes; Colonel Mustard was in the observatory with the candlestick\n(b) No; Colonel Mustard was not in the observatory with the candlestick\n(c) Unknown; there is not enough information to determine whether Colonel Mustard was in the observatory with the candlestick\n\nSolution:\n</code></pre> <pre><code>(c) Unknown; there is not enough information to determine whether Colonel Mustard was in the observatory with the candlestick\n</code></pre> <p>Although clues 3 and 5 establish that Colonel Mustard was the only person in the observatory and that the person in the observatory had the candlestick, the models fails to combine them into a correct answer of (a) Yes.</p> <p>However, instead of asking for the answer directly, we can split the task into three pieces:</p> <ul> <li>First, go through the clues one by one and consider whether the clue is potentially relevant</li> <li>Second, combine the relevant clues to reason out the answer to the question</li> <li>Third, write the final answer: either (a), (b), or (c)</li> </ul> <pre><code>Use the following clues to answer the following multiple-choice question, using the following procedure:\n(1) First, go through the clues one by one and consider whether the clue is potentially relevant\n(2) Second, combine the relevant clues to reason out the answer to the question\n(3) Third, map the answer to one of the multiple choice answers: either (a), (b), or (c)\n\nClues:\n1. Miss Scarlett was the only person in the lounge.\n2. The person with the pipe was in the kitchen.\n3. Colonel Mustard was the only person in the observatory.\n4. Professor Plum was not in the library nor the billiard room.\n5. The person with the candlestick was in the observatory.\n\nQuestion: Was Colonel Mustard in the observatory with the candlestick?\n(a) Yes; Colonel Mustard was in the observatory with the candlestick\n(b) No; Colonel Mustard was not in the observatory with the candlestick\n(c) Unknown; there is not enough information to determine whether Colonel Mustard was in the observatory with the candlestick\n\nSolution:\n(1) First, go through the clues one by one and consider whether the clue is potentially relevant:\n</code></pre> <pre><code>Clue 1: Miss Scarlett was the only person in the lounge. This clue is not relevant.\nClue 2: The person with the pipe was in the kitchen. This clue is not relevant.\nClue 3: Colonel Mustard was the only person in the observatory. This clue is relevant.\nClue 4: Professor Plum was not in the library nor the billiard room. This clue is not relevant.\nClue 5: The person with the candlestick was in the observatory. This clue is relevant.\n\n(2) Second, combine the relevant clues to reason out the answer to the question:\nClue 3 and Clue 5 both say that the person with the candlestick was in the observatory. Colonel Mustard was the only person in the observatory, so Colonel Mustard was the person with the candlestick.\n\n(3) Third, write the final answer:\n(a) Yes; Colonel Mustard was in the observatory with the candlestick\n</code></pre> <p>By giving the model more time and space to think, and guiding it along a reasoning plan, it's able to figure out the correct answer of (a) Yes.</p> <p>Another benefit of splitting complex instructions into smaller subtasks is that it can help keep the model focused on each subtask.</p> <p>For example, if we ask <code>text-davinci-002</code> to summarize a text in its original language, the model can lapse back into English:</p> <pre><code>Summarize the text using the original language of the text. The summary should be one sentence long.\n\nText:\n\"\"\"\nLa estad\u00edstica (la forma femenina del t\u00e9rmino alem\u00e1n Statistik, derivado a su vez del italiano statista, \"hombre de Estado\")\u200b es una ciencia que estudia la variabilidad, colecci\u00f3n, organizaci\u00f3n, an\u00e1lisis, interpretaci\u00f3n, y presentaci\u00f3n de los datos, as\u00ed como el proceso aleatorio que los genera siguiendo las leyes de la probabilidad.\u200b La estad\u00edstica es una ciencia formal deductiva, con un conocimiento propio, din\u00e1mico y en continuo desarrollo obtenido a trav\u00e9s del m\u00e9todo cient\u00edfico formal. En ocasiones, las ciencias f\u00e1cticas necesitan utilizar t\u00e9cnicas estad\u00edsticas durante su proceso de investigaci\u00f3n factual, con el fin de obtener nuevos conocimientos basados en la experimentaci\u00f3n y en la observaci\u00f3n. En estos casos, la aplicaci\u00f3n de la estad\u00edstica permite el an\u00e1lisis de datos provenientes de una muestra representativa, que busca explicar las correlaciones y dependencias de un fen\u00f3meno f\u00edsico o natural, de ocurrencia en forma aleatoria o condicional.\n\"\"\"\n\nSummary:\n</code></pre> <pre><code>The text explains that statistics is a science that studies the variability, collection, organization, analysis, interpretation, and presentation of data, as well as the random process that generates them following the laws of probability.\n ```\n\n However, if we first ask the model to identify the language of the text, and then summarize the text, it becomes more reliable:\n\n```text-davinci-002\nFirst, identify the language of the text. Second, summarize the text using the original language of the text. The summary should be one sentence long.\n\nText:\n\"\"\"\nLa estad\u00edstica (la forma femenina del t\u00e9rmino alem\u00e1n Statistik, derivado a su vez del italiano statista, \"hombre de Estado\")\u200b es una ciencia que estudia la variabilidad, colecci\u00f3n, organizaci\u00f3n, an\u00e1lisis, interpretaci\u00f3n, y presentaci\u00f3n de los datos, as\u00ed como el proceso aleatorio que los genera siguiendo las leyes de la probabilidad.\u200b La estad\u00edstica es una ciencia formal deductiva, con un conocimiento propio, din\u00e1mico y en continuo desarrollo obtenido a trav\u00e9s del m\u00e9todo cient\u00edfico formal. En ocasiones, las ciencias f\u00e1cticas necesitan utilizar t\u00e9cnicas estad\u00edsticas durante su proceso de investigaci\u00f3n factual, con el fin de obtener nuevos conocimientos basados en la experimentaci\u00f3n y en la observaci\u00f3n. En estos casos, la aplicaci\u00f3n de la estad\u00edstica permite el an\u00e1lisis de datos provenientes de una muestra representativa, que busca explicar las correlaciones y dependencias de un fen\u00f3meno f\u00edsico o natural, de ocurrencia en forma aleatoria o condicional.\n\"\"\"\n\nLanguage:\n</code></pre> <pre><code>Spanish\n\nLa estad\u00edstica es una ciencia que estudia la variabilidad, colecci\u00f3n, organizaci\u00f3n, an\u00e1lisis, interpretaci\u00f3n, y presentaci\u00f3n de los datos, as\u00ed como el proceso aleatorio que los genera siguiendo las leyes de la probabilidad.\n</code></pre>"},{"location":"techniques_to_improve_reliability/#prompt-the-model-to-explain-before-answering","title":"Prompt the model to explain before answering","text":"<p>Another powerful technique for improving the reliability of answers is to prompt the model to gradually reason out the answer rather than jumping immediately to the final answer. By 'thinking aloud' the model can be far more likely to arrive at the correct answer.</p>"},{"location":"techniques_to_improve_reliability/#zero-shot","title":"Zero-shot","text":""},{"location":"techniques_to_improve_reliability/#method","title":"Method","text":"<p>Published by Takeshi Kojima et al. in 2022, the easiest way to prompt a model to reason out the answer is to simply prepend answers with <code>Let's think step by step.</code> Figure 2 illustrates an example:</p> <p> Source: Large Language Models are Zero-Shot Reasoners by Takeshi Kojima et al. (2022).</p>"},{"location":"techniques_to_improve_reliability/#results","title":"Results","text":"<p>Applying this simple trick to the MultiArith math dataset, the authors found <code>Let's think step by step</code> quadrupled the accuracy, from 18% to 79%!</p> <p> Source: Large Language Models are Zero-Shot Reasoners by Takeshi Kojima et al. (2022).</p>"},{"location":"techniques_to_improve_reliability/#implications","title":"Implications","text":"<p>Although the <code>Let's think step by step</code> trick works well on math problems, it's not effective on all tasks. The authors found that it was most helpful for multi-step arithmetic problems, symbolic reasoning problems, strategy problems, and other reasoning problems. It didn't help with simple math problems or common sense questions, and presumably wouldn't help with many other non-reasoning tasks either.</p> <p> Source: Large Language Models are Zero-Shot Reasoners by Takeshi Kojima et al. (2022).</p> <p>To learn more, read the full paper.</p> <p>If you apply this technique to your own tasks, don't be afraid to experiment with customizing the instruction. <code>Let's think step by step</code> is rather generic, so you may find better performance with instructions that hew to a stricter format customized to your use case. For example, if you were  you can try more structured variants like <code>First, think step by step about why X might be true. Second, think step by step about why Y might be true. Third, think step by step about whether X or Y makes more sense.</code>. And you can even give the model an example format to help keep it on track, e.g.:</p> <pre><code>Using the IRS guidance below, answer the following questions using this format:\n(1) For each criterion, determine whether it is met by the vehicle purchase\n- {Criterion} Let's think step by step. {explanation} {yes or no, or if the question does not apply then N/A}.\n(2) After considering each criterion in turn, phrase the final answer as \"Because of {reasons}, the answer is likely {yes or no}.\"\n\nIRS guidance:\n\"\"\"\nYou may be eligible for a federal tax credit under Section 30D if you purchased a car or truck that meets the following criteria:\n- Does the vehicle have at least four wheels?\n- Does the vehicle weigh less than 14,000 pounds?\n- Does the vehicle draw energy from a battery with at least 4 kilowatt hours that may be recharged from an external source?\n- Was the vehicle purchased in a year before 2022?\n  - If so, has the manufacturer sold less than 200,000 qualifying vehicles? (Tesla and GM have sold more than 200,000 qualifying vehicles.)\n- Was the vehicle purchased in a year after 2022?\n  - If so, is the vehicle present in the following list of North American-assembled vehicles? (The only electric vehicles assembled in North America are the Audi Q5, BMW 330e, BMW X5, Chevrolet Bolt EUV, Chevrolet Bolt EV, Chrysler Pacifica PHEV, Ford Escape PHEV, Ford F Series, Ford Mustang MACH E, Ford Transit Van, GMC Hummer Pickup, GMC Hummer SUV, Jeep Grand Cherokee PHEV, Jeep Wrangler PHEV, Lincoln Aviator PHEV, Lincoln Corsair Plug-in, Lucid Air, Nissan Leaf, Rivian EDV, Rivian R1S, Rivian R1T, Tesla Model 3, Tesla Model S, Tesla Model X, Tesla Model Y, Volvo S60, BMW 330e, Bolt EV, Cadillac Lyriq, Mercedes EQS SUV, and Nissan Leaf.)\n\"\"\"\n\nQuestion: Can I claim a federal tax credit for my Toyota Prius Prime bought in 2021?\n\nSolution:\n\n(1) For each criterion, determine whether it is met by the vehicle purchase\n- Does the vehicle have at least four wheels? Let's think step by step.\n</code></pre> <pre><code> The Toyota Prius Prime has four wheels, so the answer is yes.\n- Does the vehicle weigh less than 14,000 pounds? Let's think step by step. The Toyota Prius Prime weighs less than 14,000 pounds, so the answer is yes.\n- Does the vehicle draw energy from a battery with at least 4 kilowatt hours that may be recharged from an external source? Let's think step by step. The Toyota Prius Prime has a battery with at least 4 kilowatt hours that may be recharged from an external source, so the answer is yes.\n- Was the vehicle purchased in a year before 2022? Let's think step by step. The Toyota Prius Prime was purchased in 2021, which is before 2022, so the answer is yes.\n- Was the vehicle purchased in a year after 2022? N/A\n- If so, is the vehicle present in the following list of North American-assembled vehicles? N/A\n\n(2) After considering each criterion in turn, phrase the final answer as \"Because of {reasons}, the answer is likely {yes or no}.\"\n\nBecause the Toyota Prius Prime meets all of the criteria for a federal tax credit, the answer is likely yes.\n</code></pre>"},{"location":"techniques_to_improve_reliability/#few-shot-examples","title":"Few-shot examples","text":""},{"location":"techniques_to_improve_reliability/#method_1","title":"Method","text":"<p>Prompting the model to reason out its answers can be done in many ways. One way is to demonstrate with a few examples ('few-shot'), as studied by Jason Wei and Denny Zhou et al. from Google. Here's an example few-shot chain-of-thought prompt:</p> <p> Source: Chain of Thought Prompting Elicits Reasoning in Large Language Models Jason Wei and Denny Zhou et al. (2022)</p> <p>More demonstrations of reasoning chains written by human labelers:</p> <p> Source: Chain of Thought Prompting Elicits Reasoning in Large Language Models Jason Wei and Denny Zhou et al. (2022)</p> <p>(Note that it has been called into question whether pears actually float)</p>"},{"location":"techniques_to_improve_reliability/#results_1","title":"Results","text":"<p>Testing on grade school math problems, the authors found that chain of thought prompting tripled the solve rate, from 18% to 57%.</p> <p> Source: Chain of Thought Prompting Elicits Reasoning in Large Language Models Jason Wei and Denny Zhou et al. (2022)</p> <p>In addition to math problems, chain of thought prompting also lifted performance on questions related to sports understanding, coin flip tracking, and last letter concatenation. In most cases, not many examples were need to saturate the performance gains (less than 8 or so).</p> <p> Source: Chain of Thought Prompting Elicits Reasoning in Large Language Models Jason Wei and Denny Zhou et al. (2022)</p> <p>To learn more, read the full paper.</p>"},{"location":"techniques_to_improve_reliability/#implications_1","title":"Implications","text":"<p>One advantage of the few-shot example-based approach relative to the <code>Let's think step by step</code> technique is that you can more easily specify the format, length, and style of reasoning that you want the model to perform before landing on its final answer. This can be be particularly helpful in cases where the model isn't initially reasoning in the right way or depth.</p>"},{"location":"techniques_to_improve_reliability/#fine-tuned","title":"Fine-tuned","text":""},{"location":"techniques_to_improve_reliability/#method_2","title":"Method","text":"<p>In general, to eke out maximum performance on a task, you'll need to fine-tune a custom model. However, fine-tuning a model using explanations may take thousands of example explanations, which are costly to write.</p> <p>In 2022, Eric Zelikman and Yuhuai Wu et al. published a clever procedure for using a few-shot prompt to generate a dataset of explanations that could be used to fine-tune a model. The idea is to use a few-shot prompt to generate candidate explanations, and only keep the explanations that produce the correct answer. Then, to get additional explanations for some of the incorrect answers, retry the the few-shot prompt but with correct answers given as part of the question. The authors called their procedure STaR (Self-taught Reasoner):</p> <p> Source: STaR: Bootstrapping Reasoning With Reasoning by Eric Zelikman and Yujuai Wu et al. (2022)</p> <p>With this technique, you can combine the benefits of fine-tuning with the benefits of chain-of-thought prompting without needing to write thousands of example explanations.</p>"},{"location":"techniques_to_improve_reliability/#results_2","title":"Results","text":"<p>When the authors applied this technique to a Common Sense Q&amp;A dataset, they found that STaR outperformed both chain-of-thought prompting alone (73% &gt; 37%) and fine-tuning alone (73% &gt; 60%):</p> <p> Source: STaR: Bootstrapping Reasoning With Reasoning by Eric Zelikman and Yujuai Wu et al. (2022)</p> <p>To learn more, read the full paper.</p>"},{"location":"techniques_to_improve_reliability/#implications_2","title":"Implications","text":"<p>Using a few-shot prompt to extend or modify a fine-tuning dataset is an idea that can be generalized beyond explanation writing. For example, if you have large quantities of unstructured text that you want to train on, you may find opportunities to use a prompt to extract a structured dataset from your unstructured text, and then fine-tune a custom model on that structured dataset.</p>"},{"location":"techniques_to_improve_reliability/#extensions-to-chain-of-thought-prompting","title":"Extensions to chain-of-thought prompting","text":"<p>A number of extensions of chain-of-thought prompting have been published as well.</p>"},{"location":"techniques_to_improve_reliability/#selection-inference-prompting","title":"Selection-inference prompting","text":""},{"location":"techniques_to_improve_reliability/#method_3","title":"Method","text":"<p>Published by Antonia Creswell et al., one extension of the chain-of-thought technique is to split the single prompt for generating explanations and answers into smaller parts. First, a prompt selects a relevant subset of facts from the text ('selection prompt'). Then, a second prompt infers a conclusion from the selected facts ('inference prompt'). These prompts are then alternated in a loop to generate multiple steps of reasoning and eventually land on a final answer. The authors illustrate the idea in the following figure:</p> <p> Source: Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning by Antonia Creswell et al. (2022)</p>"},{"location":"techniques_to_improve_reliability/#results_3","title":"Results","text":"<p>When applied to a 7B-parameter model, the authors found that selection-inference prompting substantially improved performance relative to chain-of-thought prompting on the bAbi and Proof Writer benchmark tasks (both of which require longer sequences of reasoning steps). The best performance they achieved combined both selection-inference prompting with fine-tuning.</p> <p> Source: Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning by Antonia Creswell et al. (2022)</p>"},{"location":"techniques_to_improve_reliability/#implications_3","title":"Implications","text":"<p>Although the gains on these benchmarks were large, these benchmarks were specifically chosen because they required longer sequences of reasoning. On problems that don't require reasoning with many steps, the gains are likely smaller.</p> <p>The results highlight a couple of general lessons for working with large language models. One, splitting up complex tasks into smaller tasks is a great way to improve reliability and performance; the more atomic the task, the less room there is for the model to err. Two, getting maximum performance often means combining fine-tuning with whatever approach you've chosen.</p> <p>To learn more, read the full paper.</p>"},{"location":"techniques_to_improve_reliability/#faithful-reasoning-architecture","title":"Faithful reasoning architecture","text":"<p>A few months after publishing the selection-inference prompting technique, the authors extended the technique in a follow-up paper, with ideas for:</p> <ul> <li>figuring out when the selection-inference cycle should stop or continue</li> <li>adding a value function to help search over multiple reasoning paths</li> <li>reducing hallucination of fake facts by fine-tuning a model to reason about sentence labels (e.g., sen1) rather than writing out the sentences themselves</li> </ul>"},{"location":"techniques_to_improve_reliability/#method_4","title":"Method","text":"<p>In the original selection-inference technique, specialized 'selection' and 'inference' prompts are alternated to select facts and make inferences from those facts, combining to generate a sequence of reasoning steps.</p> <p>The authors extend this technique with two additional components.</p> <p>First, the authors add a 'halter' model that, after each inference step, is asked whether the inferences thus far are sufficient to answer the question. If yes, then the model generates a final answer.</p> <p>The halter models brings a couple of advantages:</p> <ul> <li>it can tell the selection-inference process to stop or keep going, as necessary.</li> <li>if the process never halts, you'll get no answer, which is often preferable to a hallucinated guess</li> </ul> <p> Source: Faithful Reasoning Using Large Language Models by Antonia Creswell et al. (2022)</p> <p> Source: Faithful Reasoning Using Large Language Models by Antonia Creswell et al. (2022)</p> <p>Second, the authors add a value function, which is used to assess the quality of reasoning steps and search over multiple reasoning trajectories. This echoes a common theme for increasing reliability; instead of generating a single answer from the model, generate a set of answers and then use some type of value function / discriminator / verifier model to pick the best one.</p> <p> Source: Faithful Reasoning Using Large Language Models by Antonia Creswell et al. (2022)</p> <p>In addition to these two extensions, the authors also use a trick to reduce hallucination of fake facts. Rather than asking the model to write out factual sentences, they fine-tune a model to work with sentence labels (e.g., sen1) instead. This helps prevent the model from hallucinating fake facts not mentioned in the prompt context.</p> <p> Source: Faithful Reasoning Using Large Language Models by Antonia Creswell et al. (2022)</p>"},{"location":"techniques_to_improve_reliability/#results_4","title":"Results","text":"<p>The authors evaluated their technique on two benchmarks: the ProofWriter task (not shown) and EntailmentBankQA (shown). The technique increased accuracy substantially, especially on harder reasoning problems.</p> <p> Source: Faithful Reasoning Using Large Language Models by Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)</p> <p>In addition, their sentence label manipulation trick essentially eliminated hallucination!</p> <p> Source: Faithful Reasoning Using Large Language Models by Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)</p>"},{"location":"techniques_to_improve_reliability/#implications_4","title":"Implications","text":"<p>This paper illustrates a number of helpful lessons for improving the reliability of large language models:</p> <ul> <li>Split complex tasks into smaller, more reliable subtasks</li> <li>Generate your answer in a step-by-step fashion, evaluating it along the way</li> <li>Generate many possible answers and use another model or function to pick the ones that look best</li> <li>Reduce hallucination by constraining what the model can say (e.g., by using sentence labels instead of sentences)</li> <li>Maximize performance of models by fine-tuning them on specialized tasks</li> </ul> <p>To learn more, read the full paper.</p>"},{"location":"techniques_to_improve_reliability/#least-to-most-prompting","title":"Least-to-most prompting","text":"<p>In addition to doing poorly on long reasoning chains (where selection-inference shines), chain-of-thought prompting can especially struggle when the examples are short but the task is long.</p>"},{"location":"techniques_to_improve_reliability/#method_5","title":"Method","text":"<p>Least-to-most prompting is another technique that splits up reasoning tasks into smaller, more reliable subtasks. The idea is to elicit a subtask from the model by prompting it with something like <code>To solve {question}, we need to first solve: \"</code>. Then, with that subtask in hand, the model can generate a solution. The solution is appended to the original question and the process is repeated until a final answer is produced.</p> <p> Source: Least-to-most Prompting Enables Complex Reasoning in Large Language Models by Denny Zhou et al. (2022)</p>"},{"location":"techniques_to_improve_reliability/#results_5","title":"Results","text":"<p>When applied to benchmarks involving long reasoning chains using <code>code-davinci-002</code> (which is optimized for code but can still understand text), the authors measured gains as large as 16% -&gt; 99.7%!</p> <p> Source: Least-to-most Prompting Enables Complex Reasoning in Large Language Models by Denny Zhou et al. (2022)</p>"},{"location":"techniques_to_improve_reliability/#implications_5","title":"Implications","text":"<p>Although the above gains from least-to-most prompting are impressive, they are measured on a very narrow set of tasks that require long reasoning chains.</p> <p>Still, they illustrate a common theme: increase reliability by (a) breaking complex tasks into smaller subtasks and (b) giving the model more time and space to work out the answer.</p> <p>To learn more, read the full paper.</p>"},{"location":"techniques_to_improve_reliability/#related-ideas","title":"Related ideas","text":""},{"location":"techniques_to_improve_reliability/#maieutic-prompting","title":"Maieutic prompting","text":""},{"location":"techniques_to_improve_reliability/#method_6","title":"Method","text":"<p>In contrast to the previous techniques, which try to maximize the likelihood of correct answers, another approach is to use GPT-3 to generate a tree of possible explanations (both correct and incorrect), and then analyze their relationships to guess at which set is correct. This technique was coined maieutic prompting by Jaehun Jung et al. in May 2022 (maieutic means relating to the Socratic method of asking questions to elicit ideas).</p> <p>The method is complicated, and works as follows:</p> <ul> <li>First, build a maieutic tree, where each node is a statement that could be true or false:</li> <li>Start with a multiple-choice question or true/false statement (e.g. <code>War cannot have a tie</code>)</li> <li>For each possible answer to the question, use the model to generate a corresponding explanation (with a prompt like <code>War cannot have a tie? True, because</code>)</li> <li>Then, prompt the model with the question and the generated explanation, and ask it to produce the answer. If reversing the explanation (with a prefix like <code>It is wrong to say that {explanation}</code>) reverses the answer, then the explanation is considered 'logically integral.'</li> <li>If an explanation is not logically integral, then repeat the above process recursively, with each explanation turned into a True or False question, and generate more explanations for each new question.</li> <li>After all of the recursive explaining is done, you end up with a tree of explanations, where each leaf on the tree has the property that reversing the explanation reverses the model's answer.</li> <li>Second, convert the tree into a graph of relations:</li> <li>For each node in the tree, calculate the model's relative belief in each node (inferred from the probability of getting an answer of <code>True</code> to given an explanation)</li> <li>For each pair of nodes in the tree, use the model to identify whether they are entailed (implied) or contradicted</li> <li>Third, find the most consistent set of beliefs and take those to be true:</li> <li>Specifically, using the strength of belief in each node and the logical relationships between them, formulate the problem as a weighted maximum satisfiability problem (MAX-SAT)</li> <li>Use a solver to the find the most self-consistent set of beliefs, and take those as true</li> </ul> <p> Source: Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations by Jaehun Jung et al. (2022)</p>"},{"location":"techniques_to_improve_reliability/#results_6","title":"Results","text":"<p> Source: Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations by Jaehun Jung et al. (2022)</p>"},{"location":"techniques_to_improve_reliability/#implications_6","title":"Implications","text":"<p>Beyond the complexity, one limitation of this method is that it appears to only apply to questions that can be posed as multiple-choice.</p> <p>To learn more, read the full paper.</p>"},{"location":"techniques_to_improve_reliability/#extensions","title":"Extensions","text":""},{"location":"techniques_to_improve_reliability/#self-consistency","title":"Self-consistency","text":""},{"location":"techniques_to_improve_reliability/#method_7","title":"Method","text":"<p>For tasks with a discrete set of answers, one simple way to improve reliability is to sample multiple explanations &amp; answers from the model (using a positive temperature) and then pick the final answer that appears most often.</p> <p> Source: Self-Consistency Improves Chain of Thought Reasoning in Language Models by Xuezhi Wang et al. (2022)</p>"},{"location":"techniques_to_improve_reliability/#results_7","title":"Results","text":"<p>This technique lifted accuracies by anywhere from 1 to 24 percentage points on a suite of math and reasoning benchmarks. (Plotted below are results from Google's LaMDA model; using Google's larger PaLM model, the baselines were higher but the gains were a bit smaller.)</p> <p> Source: Self-Consistency Improves Chain of Thought Reasoning in Language Models by Xuezhi Wang et al. (2022)</p>"},{"location":"techniques_to_improve_reliability/#implications_7","title":"Implications","text":"<p>Although this technique is simple to implement, it can be costly. Generating a set of 10 answers will increase your costs by 10x.</p> <p>Also, as with many of these techniques, it applies only to tasks with a limited set of answers. For open-ended tasks where each answer is unique (such as writing a poem), it's not obvious what it would mean to pick the most common answer.</p> <p>Lastly, this technique ought to be most beneficial when there are multiple paths or phrasings to reach an answer; if there's only one path, then the technique may not help at all. An extreme example: If the task was to generate a single token answer, then taking the most common token from 100 generations would be no different than taking the token with the highest logprobs (which you can get with a single generation at temperature=0).</p>"},{"location":"techniques_to_improve_reliability/#verifiers","title":"Verifiers","text":"<p>Another key technique for improving task performance is to train a verifier or discriminator model to evaluate the outputs of the main generative model. If the discriminator rejects the output, then you can resample the generative model until you get an acceptable output. In many cases, it's easier to judge an answer than it is to create an answer, which helps explain the power of this method.</p>"},{"location":"techniques_to_improve_reliability/#method_8","title":"Method","text":"<p>In 2021, OpenAI researchers applied this technique to grade school math problems, using the following procedure:</p> <ul> <li>First, they fine-tuned a model on questions and solutions</li> <li>For each problem in the training set, they generated 100 solutions</li> <li>Each of those 100 solutions was automatically labeled as either correct or incorrect, based on whether the final answer was correct</li> <li>Using those solutions, with some labeled correct and some labeled incorrect, they fine-tuned a verifier model to classify whether a question and candidate solution was correct or incorrect</li> <li>Finally, at test time, the generative model creates 100 solutions to each problem, and the one with the highest score according to the verifier model is picked as the final answer</li> </ul> <p> Source: Training Verifiers to Solve Math Word Problems by Karl Cobbe et al. (2021)</p>"},{"location":"techniques_to_improve_reliability/#results_8","title":"Results","text":"<p>With a 175B GPT-3 model and 8,000 training examples, this technique substantially lifted grade school math accuracy from ~33% to ~55%.</p> <p> Source: Training Verifiers to Solve Math Word Problems by Karl Cobbe et al. (2021)</p>"},{"location":"techniques_to_improve_reliability/#implications_8","title":"Implications","text":"<p>Similar to the self-consistency technique, this method can get expensive, as generating, say, 100 solutions per task will increase your costs by roughly ~100x.</p>"},{"location":"techniques_to_improve_reliability/#theories-of-reliability","title":"Theories of reliability","text":"<p>Although the techniques above vary in their approach, they all share the goal of improving reliability on complex tasks. Mainly they do this by:</p> <ul> <li>decomposing unreliable operations into smaller, more reliable operations (e.g., selection-inference prompting)</li> <li>using multiple steps or multiple relationships to make the system's reliability greater than any individual component (e.g., maieutic prompting)</li> </ul>"},{"location":"techniques_to_improve_reliability/#probabilistic-graphical-models","title":"Probabilistic graphical models","text":"<p>This paradigm of trying to build a reliable system out of less reliable components is reminiscent of probabilistic programming, and many of the analysis techniques of that field can be applied to this one.</p> <p>In the paper Language Model Cascades, David Dohan et al. interpret the above techniques in the paradigm of probabilistic graphical models:</p>"},{"location":"techniques_to_improve_reliability/#chain-of-thought-prompting","title":"Chain of thought prompting","text":"<p> Source: Language Model Cascades by David Dohan et al. (2022)</p>"},{"location":"techniques_to_improve_reliability/#fine-tuned-chain-of-thought-prompting-self-taught-reasoner","title":"Fine-tuned chain of thought prompting / Self-taught reasoner","text":"<p> Source: Language Model Cascades by David Dohan et al. (2022)</p>"},{"location":"techniques_to_improve_reliability/#selection-inference-prompting_1","title":"Selection-inference prompting","text":"<p> Source: Language Model Cascades by David Dohan et al. (2022)</p>"},{"location":"techniques_to_improve_reliability/#verifiers_1","title":"Verifiers","text":"<p> Source: Language Model Cascades by David Dohan et al. (2022)</p>"},{"location":"techniques_to_improve_reliability/#implications_9","title":"Implications","text":"<p>Although formulating these techniques as probabilistic graphical models may not be immediately useful for solving any particular problem, the framework may be helpful in selecting, combining, and discovering new techniques.</p>"},{"location":"techniques_to_improve_reliability/#closing-thoughts","title":"Closing thoughts","text":"<p>Research into large language models is very active and evolving rapidly. Not only do researchers continue to improve the models, they are also continue to improve our understanding of how to best employ the models. To underscore the pace of these developments, note that all of the papers shared above were published within the past 12 months (as I write in Sep 2022).</p> <p>In the future, expect better models and better techniques to be published. Even if the specific techniques here are eclipsed by future best practices, the general principles behind them will likely remain a key part of any expert user's toolkit.</p>"},{"location":"techniques_to_improve_reliability/#bibliography","title":"Bibliography","text":"Lesson Paper Date Break complex tasks into simpler subtasks (and consider exposing the intermediate outputs to users) AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts 2021 Oct You can improve output by generating many candidates, and then picking the one that looks best Training Verifiers to Solve Math Word Problems 2021 Oct On reasoning tasks, models do better when they reason step-by-step before answering Chain of Thought Prompting Elicits Reasoning in Large Language Models 2022 Jan You can improve step-by-step reasoning by generating many explanation-answer outputs, and picking the most popular answer Self-Consistency Improves Chain of Thought Reasoning in Language Models 2022 Mar If you want to fine-tune a step-by-step reasoner, you can do it with multiple-choice question &amp; answer data alone STaR: Bootstrapping Reasoning With Reasoning 2022 Mar The step-by-step reasoning method works great even with zero examples Large Language Models are Zero-Shot Reasoners 2022 May You can do better than step-by-step reasoning by alternating a \u2018selection\u2019 prompt and an \u2018inference\u2019 prompt Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning 2022 May On long reasoning problems, you can improve step-by-step reasoning by splitting the problem into pieces to solve incrementally Least-to-most Prompting Enables Complex Reasoning in Large Language Models 2022 May You can have the model analyze both good and bogus explanations to figure out which set of explanations are most consistent Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations 2022 May You can think about these techniques in terms of probabilistic programming, where systems comprise unreliable components Language Model Cascades 2022 Jul You can eliminate hallucination with sentence label manipulation, and you can reduce wrong answers with a 'halter' prompt Faithful Reasoning Using Large Language Models 2022 Aug"},{"location":"text_comparison_examples/","title":"Text comparison examples","text":"<p>The OpenAI API embeddings endpoint can be used to measure relatedness or similarity between pieces of text.</p> <p>By leveraging GPT-3's understanding of text, these embeddings achieved state-of-the-art results on benchmarks in unsupervised learning and transfer learning settings.</p> <p>Embeddings can be used for semantic search, recommendations, cluster analysis, near-duplicate detection, and more.</p> <p>For more information, read OpenAI's blog post announcements:</p> <ul> <li>Introducing Text and Code Embeddings (Jan 2022)</li> <li>New and Improved Embedding Model (Dec 2022)</li> </ul>"},{"location":"text_comparison_examples/#semantic-search","title":"Semantic search","text":"<p>Embeddings can be used for search either by themselves or as a feature in a larger system.</p> <p>The simplest way to use embeddings for search is as follows:</p> <ul> <li>Before the search (precompute):</li> <li>Split your text corpus into chunks smaller than the token limit (8,191 tokens for <code>text-embedding-ada-002</code>)</li> <li>Embed each chunk of text</li> <li>Store those embeddings in your own database or in a vector search provider like Pinecone, Weaviate or Qdrant</li> <li>At the time of the search (live compute):</li> <li>Embed the search query</li> <li>Find the closest embeddings in your database</li> <li>Return the top results</li> </ul> <p>An example of how to use embeddings for search is shown in Semantic_text_search_using_embeddings.ipynb.</p> <p>In more advanced search systems, the the cosine similarity of embeddings can be used as one feature among many in ranking search results.</p>"},{"location":"text_comparison_examples/#question-answering","title":"Question answering","text":"<p>The best way to get reliably honest answers from GPT-3 is to give it source documents in which it can locate correct answers. Using the semantic search procedure above, you can cheaply search a corpus of documents for relevant information and then give that information to GPT-3, via the prompt, to answer a question. We demonstrate in Question_answering_using_embeddings.ipynb.</p>"},{"location":"text_comparison_examples/#recommendations","title":"Recommendations","text":"<p>Recommendations are quite similar to search, except that instead of a free-form text query, the inputs are items in a set.</p> <p>An example of how to use embeddings for recommendations is shown in Recommendation_using_embeddings.ipynb.</p> <p>Similar to search, these cosine similarity scores can either be used on their own to rank items or as features in larger ranking algorithms.</p>"},{"location":"text_comparison_examples/#customizing-embeddings","title":"Customizing Embeddings","text":"<p>Although OpenAI's embedding model weights cannot be fine-tuned, you can nevertheless use training data to customize embeddings to your application.</p> <p>In Customizing_embeddings.ipynb, we provide an example method for customizing your embeddings using training data. The idea of the method is to train a custom matrix to multiply embedding vectors by in order to get new customized embeddings. With good training data, this custom matrix will help emphasize the features relevant to your training labels. You can equivalently consider the matrix multiplication as (a) a modification of the embeddings or (b) a modification of the distance function used to measure the distances between embeddings.</p>"},{"location":"text_editing_examples/","title":"Text editing examples","text":"<p>In addition to the completions API endpoint, OpenAI offers an edits API endpoint. Read more at:</p> <ul> <li>Blog post announcement (Mar 2022)</li> <li>Edit API documentation</li> </ul> <p>In contrast to completions, which only take a single text input, edits take two text inputs: the instruction and the text to be modified. For example:</p> <p>Instruction input:</p> <pre><code>Fix the OCR errors\n</code></pre> <p>Text input:</p> <pre><code>Therewassomehostilityntheenergybehindthe researchreportedinPerceptrons....Part of ourdrivecame,aswequiteplainlyacknoweldgednourbook,fromhe facthatfundingndresearchnergywerebeingdissipatedon. . .misleadingttemptsouseconnectionistmethodsnpracticalappli-cations.\n</code></pre> <p>Output:</p> <pre><code>There was some hostility in the energy behind the research reported in Perceptrons....Part of our drive came, as we quite plainly acknowledged in our book, from the fact that funding and research energy were being dissipated on...misleading attempts to use connectionist methods in practical applications.\n</code></pre> <p>In general, instructions can be imperative, present tense, or past tense. Experiment to see what works best for your use case.</p>"},{"location":"text_editing_examples/#translation","title":"Translation","text":"<p>One application of the edit API is translation.</p> <p>Large language models are excellent at translating across common languages. In 2021, GPT-3 set a new state-of-the-art record in unsupervised translation on the WMT14 English-French benchmark.</p> <p>Here's an example of how to translate text using the edits endpoint:</p> <p>Instruction input:</p> <pre><code>translation into French\n</code></pre> <p>Text input:</p> <pre><code>That's life.\n</code></pre> <p>Output:</p> <pre><code>C'est la vie.\n</code></pre> <p>Of course, many tasks that can be accomplished with the edits endpoint can also be done by the completions endpoint too. For example, you can request a translate by prepending an instruction as follows:</p> <pre><code>Translate the following text from English to French.\n\nEnglish: That's life.\nFrench:\n</code></pre> <p>Output:</p> <pre><code> C'est la vie.\n</code></pre> <p>Tips for translation:</p> <ul> <li>Performance is best on the most common languages</li> <li>We've seen better performance when the instruction is given in the final language (so if translating into French, give the instruction <code>Traduire le texte de l'anglais au fran\u00e7ais.</code> rather than <code>Translate the following text from English to French.</code>)</li> <li>Backtranslation (as described here) can also increase performance</li> <li>Text with colons and heavy punctuation can trip up the instruction-following models, especially if the instruction uses colons (e.g., <code>English: {english text} French:</code>)</li> <li>The edits endpoint sometimes repeats the original text input alongside the translation, which can be monitored and filtered</li> </ul> <p>When it comes to translation, large language models particularly shine at combining other instructions alongside translation. For example, you can ask GPT-3 to translate Slovenian to English but keep all LaTeX typesetting commands unchanged. The following notebook details how we translated a Slovenian math book into English:</p> <p>Translation of a Slovenian math book into English</p>"},{"location":"text_explanation_examples/","title":"Text explanation examples","text":"<p>Large language models are useful for distilling information from long texts. Applications include:</p> <ul> <li>Answering questions about a piece of text, e.g.:</li> <li>Querying an knowledge base to help people look up things they don't know</li> <li>Querying an unfamiliar document to understand what it contains</li> <li>Querying a document with structured questions in order to extract tags, classes, entities, etc.</li> <li>Summarizing text, e.g.:</li> <li>Summarizing long documents</li> <li>Summarizing back-and-forth emails or message threads</li> <li>Summarizing detailed meeting notes with key points and next steps</li> <li>Classifying text, e.g.:</li> <li>Classifying customer feedback messages by topic or type</li> <li>Classifying documents by topic or type</li> <li>Classifying the tone or sentiment of text</li> <li>Extracting entities, e.g.:</li> <li>Extracting contact information from a customer message</li> <li>Extracting names of people or companies or products from a document</li> <li>Extracting things mentioned in customer reviews or feedback</li> </ul> <p>Below are some simple examples of each.</p>"},{"location":"text_explanation_examples/#answering-questions-about-a-piece-of-text","title":"Answering questions about a piece of text","text":"<p>Here's an example prompt for answering questions about a piece of text:</p> <pre><code>Using the following text, answer the following question. If the answer is not contained within the text, say \"I don't know.\"\n\nText:\n\"\"\"\nOklo Mine (sometimes Oklo Reactor or Oklo Mines), located in Oklo, Gabon on the west coast of Central Africa, is believed to be the only natural nuclear fission reactor. Oklo consists of 16 sites at which self-sustaining nuclear fission reactions are thought to have taken place approximately 1.7 billion years ago, and ran for hundreds of thousands of years. It is estimated to have averaged under 100 kW of thermal power during that time.\n\"\"\"\n\nQuestion: How many natural fission reactors have ever been discovered?\n\nAnswer:\n</code></pre> <p>Output:</p> <pre><code> One. Oklo Mine is believed to be the only natural nuclear fission reactor.\n</code></pre> <p>If the text you wish to ask about is longer than the token limit (~4,000 tokens for <code>text-davinci-002</code>/<code>-003</code> and ~2,000 tokens for earlier models), you can split the text into smaller pieces, rank them by relevance, and then ask your question only using the most-relevant-looking pieces. This is demonstrated in Question_answering_using_embeddings.ipynb.</p> <p>In the same way that students do better on tests when allowed to access notes, GPT-3 does better at answering questions when it's given text containing the answer. Without notes, GPT-3 has to rely on its own long-term memory (i.e., internal weights), which are more prone to result in confabulated or hallucinated answers.</p>"},{"location":"text_explanation_examples/#summarization","title":"Summarization","text":"<p>Here's a simple example prompt to summarize a piece of text:</p> <pre><code>Summarize the following text.\n\nText:\n\"\"\"\nTwo independent experiments reported their results this morning at CERN, Europe's high-energy physics laboratory near Geneva in Switzerland. Both show convincing evidence of a new boson particle weighing around 125 gigaelectronvolts, which so far fits predictions of the Higgs previously made by theoretical physicists.\n\n\"As a layman I would say: 'I think we have it'. Would you agree?\" Rolf-Dieter Heuer, CERN's director-general, asked the packed auditorium. The physicists assembled there burst into applause.\n\"\"\"\n\nSummary:\n</code></pre> <p>Output:</p> <pre><code>CERN's director-general asked a packed auditorium if they agreed that two independent experiments had found convincing evidence of a new boson particle that fits predictions of the Higgs, to which the physicists assembled there responded with applause.\n</code></pre> <p>The triple quotation marks <code>\"\"\"</code> used in these example prompts aren't special; GPT-3 can recognize most delimiters, including <code>&lt;&gt;</code>, <code>{}</code>, or <code>###</code>. For long pieces of text, we recommend using some kind of delimiter to help disambiguate where one section of text ends and the next begins.</p>"},{"location":"text_explanation_examples/#classification","title":"Classification","text":"<p>If you want to classify the text, the best approach depends on whether the classes are known in advance.</p> <p>If your classes are known in advance, classification is often best done with a fine-tuned model, as demonstrated in Fine-tuned_classification.ipynb.</p> <p>If your classes are not known in advance (e.g., they are set by a user or generated on the fly), you can try zero-shot classification by either giving an instruction containing the classes or even by using embeddings to see which class label (or other classified texts) are most similar to the text (as demonstrated in Zero-shot_classification.ipynb).</p>"},{"location":"text_explanation_examples/#entity-extraction","title":"Entity extraction","text":"<p>Here's an example prompt for entity extraction:</p> <pre><code>From the text below, extract the following entities in the following format:\nCompanies: &lt;comma-separated list of companies mentioned&gt;\nPeople &amp; titles: &lt;comma-separated list of people mentioned (with their titles or roles appended in parentheses)&gt;\n\nText:\n\"\"\"\nIn March 1981, United States v. AT&amp;T came to trial under Assistant Attorney General William Baxter. AT&amp;T chairman Charles L. Brown thought the company would be gutted. He realized that AT&amp;T would lose and, in December 1981, resumed negotiations with the Justice Department. Reaching an agreement less than a month later, Brown agreed to divestiture\u2014the best and only realistic alternative. AT&amp;T's decision allowed it to retain its research and manufacturing arms. The decree, titled the Modification of Final Judgment, was an adjustment of the Consent Decree of 14 January 1956. Judge Harold H. Greene was given the authority over the modified decree....\n\nIn 1982, the U.S. government announced that AT&amp;T would cease to exist as a monopolistic entity. On 1 January 1984, it was split into seven smaller regional companies, Bell South, Bell Atlantic, NYNEX, American Information Technologies, Southwestern Bell, US West, and Pacific Telesis, to handle regional phone services in the U.S. AT&amp;T retains control of its long distance services, but was no longer protected from competition.\n\"\"\"\n</code></pre> <p>Output:</p> <pre><code>Companies: AT&amp;T, Bell South, Bell Atlantic, NYNEX, American Information Technologies, Southwestern Bell, US West, Pacific Telesis\nPeople &amp; titles: William Baxter (Assistant Attorney General), Charles L. Brown (AT&amp;T chairman), Harold H. Greene (Judge)\n</code></pre>"},{"location":"text_writing_examples/","title":"Text writing examples","text":"<p>Large language models are excellent at writing. They can assist with all sorts of tasks:</p> <ul> <li>Blog posts</li> <li>Email copy</li> <li>Ad copy</li> <li>Website copy</li> <li>Product descriptions</li> <li>Memos</li> <li>Storytelling</li> <li>Brainstorming</li> <li>Question generation</li> <li>etc.</li> </ul> <p>The easiest way to prompt GPT-3 is to tell it what you'd like. For example, if you want it to write an email, you could use a prompt like this:</p> <pre><code>Write an email to a colleague named Jill congratulating her on her promotion. The tone should be warm yet professional. Mention how you admire the work she's been putting in.  Include a joke about how her pet lizard Max enjoys eating grasshoppers. Mention how you're looking forward to the team off-site next week.\n</code></pre> <p>Output:</p> <pre><code>Subject: Congratulations on Your Promotion, Jill!\n\nDear Jill,\n\nCongratulations on your promotion! I'm so proud of you and all the hard work you've been putting in. It's been inspiring to watch you grow in your career.\n\nI'm sure Max the lizard is also celebrating with you. I hear he loves eating grasshoppers, so you should treat him to a special snack!\n\nI'm looking forward to seeing you at the team off-site next week. I'm sure it will be a great opportunity to celebrate your success.\n\nAll the best,\n\n[Your Name]\n</code></pre> <p>In general, writing can work with any style of prompt, whether it's an instruction or a series of examples to follow. Experiment to see what works best for your use case.</p> <p>Writing also works with any type of model, though they each have strengths and weaknesses.</p> Advantages Disadvantages Instruction-following models(e.g., <code>text-davinci-003</code>) Easiest to use Less diverse; less creative; sometimes harder to steer tone, style, etc. Base models(e.g., <code>davinci</code>) Potentially more creative and diverse Harder to prompt well, more expensive (as examples in the prompt cost extra tokens) Fine-tuned models Can train off of many examples; cheaper than including examples in the prompt Hard to gather training data; training makes iteration slower and more expensive"},{"location":"apps/file-q-and-a/","title":"File Q&amp;A","text":"<p>File Q&amp;A is a Next.js app that lets you find answers in your files using OpenAI APIs. You can upload files and ask questions related to their content, and the app will use embeddings and GPT to generate answers from the most relevant files.</p> <p>This repo contains two versions of the app:</p> <ul> <li><code>/nextjs</code>: A standalone Next.js app that stores embeddings locally in the browser. You will need an OpenAI API key to use this app. Read more in its README.</li> <li><code>/nextjs-with-flask-server</code>: A Next.js app that uses a Flask server as a proxy to access the OpenAI APIs, and Pinecone as a vector database to store embeddings. You will need an OpenAI API key and a Pinecone API key to use this app. Read more in its README.</li> </ul> <p>To run either version of the app, please follow the instructions in the respective README.md files in the subdirectories.</p>"},{"location":"apps/file-q-and-a/#how-it-works","title":"How it works","text":"<p>When a file is uploaded, text is extracted from the file. This text is then split into shorter text chunks, and an embedding is created for each text chunk. When the user asks a question, an embedding is created for the question, and a similarity search is performed to find the file chunk embeddings that are most similar to the question (i.e. have highest cosine similarities with the question embedding). An API call is then made to the completions endpoint, with the question and the most relevant file chunks are included in the prompt. The generative model then gives the answer to the question found in the file chunks, if the answer can be found in the extracts.</p>"},{"location":"apps/file-q-and-a/#limitations","title":"Limitations","text":"<p>The app may sometimes generate answers that are not in the files, or hallucinate about the existence of files that are not uploaded.</p>"},{"location":"apps/file-q-and-a/nextjs/","title":"File Q&amp;A","text":"<p>File Q&amp;A is a Next.js app that lets you find answers in your files using OpenAI APIs. You can upload files and ask questions related to their content, and the app will use embeddings and GPT to generate answers from the most relevant files.</p>"},{"location":"apps/file-q-and-a/nextjs/#requirements","title":"Requirements","text":"<p>To run the app, you need an OpenAI API key. You can create a new API key here.</p>"},{"location":"apps/file-q-and-a/nextjs/#set-up","title":"Set Up","text":"<p>If you don't have Node.js and npm already, install them from https://nodejs.org/en/download/.</p> <p>In your terminal, navigate to the <code>nextjs</code> directory of this example app, and then install dependencies:</p> <pre><code>npm install\n</code></pre> <p>Copy the .env.local.example file into a .env.local file and fill out the OpenAI API key field.</p>"},{"location":"apps/file-q-and-a/nextjs/#development","title":"Development","text":"<p>Run the development server:</p> <pre><code>npm run dev\n</code></pre> <p>Open http://localhost:3000 with your browser to see the app.</p>"},{"location":"apps/file-q-and-a/nextjs/#deployment","title":"Deployment","text":"<p>You can deploy the app on Vercel, the platform from the creators of Next.js. Check out the Next.js deployment documentation for more details.</p>"},{"location":"apps/file-q-and-a/nextjs/#limitations","title":"Limitations","text":"<p>Uploaded files and generated embeddings don't persist on browser refresh. If you want to store more embeddings, we recommend using a vector database (e.g. Pinecone, Weaviate, Milvus, Qdrant, Redis, FAISS, etc.). The <code>nextjs-with-flask-server</code> version of this demo uses a Pinecone vector database.</p> <p>The app may sometimes generate answers that are not in the files, or hallucinate about the existence of files that are not uploaded.</p>"},{"location":"apps/file-q-and-a/nextjs-with-flask-server/","title":"File Q&amp;A with Next.js and Flask","text":"<p>File Q&amp;A is a web app that lets you find answers in your files. You can upload files and ask questions related to their content, and the app will use embeddings and GPT to generate answers from the most relevant files. \\</p>"},{"location":"apps/file-q-and-a/nextjs-with-flask-server/#requirements","title":"Requirements","text":"<p>To run the app, you need:</p> <ul> <li>An OpenAI API key. You can create a new API key here.</li> <li>A Pinecone API key and index name. You can create a new account and index here.</li> <li>Python 3.7 or higher and pipenv for the Flask server.</li> <li>Node.js and npm for the Next.js client.</li> </ul>"},{"location":"apps/file-q-and-a/nextjs-with-flask-server/#set-up-and-development","title":"Set-Up and Development","text":""},{"location":"apps/file-q-and-a/nextjs-with-flask-server/#server","title":"Server","text":"<p>Fill out the config.yaml file with your Pinecone API key, index name and environment.</p> <p>Run the Flask server:</p> <pre><code>cd server\nbash script/start \"&lt;your OPENAI_API_KEY&gt;\"\n</code></pre>"},{"location":"apps/file-q-and-a/nextjs-with-flask-server/#client","title":"Client","text":"<p>Navigate to the client directory and install Node dependencies:</p> <pre><code>cd client\nnpm install\n</code></pre> <p>Run the Next.js client:</p> <pre><code>cd client\nnpm run dev\n</code></pre> <p>Open http://localhost:3000 with your browser to see the app.</p>"},{"location":"apps/file-q-and-a/nextjs-with-flask-server/#limitations","title":"Limitations","text":"<p>The app may sometimes generate answers that are not in the files, or hallucinate about the existence of files that are not uploaded.</p>"},{"location":"apps/file-q-and-a/nextjs-with-flask-server/server/answer_question/","title":"Answer question","text":"In\u00a0[\u00a0]: Copied! <pre>from utils import get_embedding\nfrom flask import jsonify\nfrom config import *\nfrom flask import current_app\n</pre> from utils import get_embedding from flask import jsonify from config import * from flask import current_app In\u00a0[\u00a0]: Copied! <pre>import openai\n</pre> import openai In\u00a0[\u00a0]: Copied! <pre>from config import *\n</pre> from config import * In\u00a0[\u00a0]: Copied! <pre>TOP_K = 10\n</pre> TOP_K = 10 In\u00a0[\u00a0]: Copied! <pre>def get_answer_from_files(question, session_id, pinecone_index):\n    logging.info(f\"Getting answer for question: {question}\")\n\n    search_query_embedding = get_embedding(question, EMBEDDINGS_MODEL)\n\n    try:\n        query_response = pinecone_index.query(\n            namespace=session_id,\n            top_k=TOP_K,\n            include_values=False,\n            include_metadata=True,\n            vector=search_query_embedding,\n        )\n        logging.info(\n            f\"[get_answer_from_files] received query response from Pinecone: {query_response}\")\n\n        files_string = \"\"\n        file_text_dict = current_app.config[\"file_text_dict\"]\n\n        for i in range(len(query_response.matches)):\n            result = query_response.matches[i]\n            file_chunk_id = result.id\n            score = result.score\n            filename = result.metadata[\"filename\"]\n            file_text = file_text_dict.get(file_chunk_id)\n            file_string = f\"###\\n\\\"{filename}\\\"\\n{file_text}\\n\"\n            if score &lt; COSINE_SIM_THRESHOLD and i &gt; 0:\n                logging.info(\n                    f\"[get_answer_from_files] score {score} is below threshold {COSINE_SIM_THRESHOLD} and i is {i}, breaking\")\n                break\n            files_string += file_string\n\n        prompt = f\"Given a question, try to answer it using the content of the file extracts below, and if you cannot answer, or find \" \\\n            f\"a relevant file, just output \\\"I couldn't find the answer to that question in your files.\\\".\\n\\n\" \\\n            f\"If the answer is not contained in the files or if there are no file extracts, respond with \\\"I couldn't find the answer \" \\\n            f\"to that question in your files.\\\" If the question is not actually a question, respond with \\\"That's not a valid question.\\\"\\n\\n\" \\\n            f\"In the cases where you can find the answer, first give the answer. Then explain how you found the answer from the source or sources, \" \\\n            f\"and use the exact filenames of the source files you mention. Do not make up the names of any other files other than those mentioned \"\\\n            f\"in the files context. Give the answer in markdown format.\" \\\n            f\"Use the following format:\\n\\nQuestion: &lt;question&gt;\\n\\nFiles:\\n&lt;###\\n\\\"filename 1\\\"\\nfile text&gt;\\n&lt;###\\n\\\"filename 2\\\"\\nfile text&gt;...\\n\\n\"\\\n            f\"Answer: &lt;answer or \\\"I couldn't find the answer to that question in your files\\\" or \\\"That's not a valid question.\\\"&gt;\\n\\n\" \\\n            f\"Question: {question}\\n\\n\" \\\n            f\"Files:\\n{files_string}\\n\" \\\n            f\"Answer:\"\n\n        logging.info(f\"[get_answer_from_files] prompt: {prompt}\")\n\n        response = openai.Completion.create(\n            prompt=prompt,\n            temperature=0,\n            max_tokens=1000,\n            top_p=1,\n            frequency_penalty=0,\n            presence_penalty=0,\n            engine=GENERATIVE_MODEL,\n        )\n\n        answer = response.choices[0].text.strip()\n        logging.info(f\"[get_answer_from_files] answer: {answer}\")\n\n        return jsonify({\"answer\": answer})\n\n    except Exception as e:\n        logging.info(f\"[get_answer_from_files] error: {e}\")\n        return str(e)\n</pre> def get_answer_from_files(question, session_id, pinecone_index):     logging.info(f\"Getting answer for question: {question}\")      search_query_embedding = get_embedding(question, EMBEDDINGS_MODEL)      try:         query_response = pinecone_index.query(             namespace=session_id,             top_k=TOP_K,             include_values=False,             include_metadata=True,             vector=search_query_embedding,         )         logging.info(             f\"[get_answer_from_files] received query response from Pinecone: {query_response}\")          files_string = \"\"         file_text_dict = current_app.config[\"file_text_dict\"]          for i in range(len(query_response.matches)):             result = query_response.matches[i]             file_chunk_id = result.id             score = result.score             filename = result.metadata[\"filename\"]             file_text = file_text_dict.get(file_chunk_id)             file_string = f\"###\\n\\\"{filename}\\\"\\n{file_text}\\n\"             if score &lt; COSINE_SIM_THRESHOLD and i &gt; 0:                 logging.info(                     f\"[get_answer_from_files] score {score} is below threshold {COSINE_SIM_THRESHOLD} and i is {i}, breaking\")                 break             files_string += file_string          prompt = f\"Given a question, try to answer it using the content of the file extracts below, and if you cannot answer, or find \" \\             f\"a relevant file, just output \\\"I couldn't find the answer to that question in your files.\\\".\\n\\n\" \\             f\"If the answer is not contained in the files or if there are no file extracts, respond with \\\"I couldn't find the answer \" \\             f\"to that question in your files.\\\" If the question is not actually a question, respond with \\\"That's not a valid question.\\\"\\n\\n\" \\             f\"In the cases where you can find the answer, first give the answer. Then explain how you found the answer from the source or sources, \" \\             f\"and use the exact filenames of the source files you mention. Do not make up the names of any other files other than those mentioned \"\\             f\"in the files context. Give the answer in markdown format.\" \\             f\"Use the following format:\\n\\nQuestion: \\n\\nFiles:\\n&lt;###\\n\\\"filename 1\\\"\\nfile text&gt;\\n&lt;###\\n\\\"filename 2\\\"\\nfile text&gt;...\\n\\n\"\\             f\"Answer: \\n\\n\" \\             f\"Question: {question}\\n\\n\" \\             f\"Files:\\n{files_string}\\n\" \\             f\"Answer:\"          logging.info(f\"[get_answer_from_files] prompt: {prompt}\")          response = openai.Completion.create(             prompt=prompt,             temperature=0,             max_tokens=1000,             top_p=1,             frequency_penalty=0,             presence_penalty=0,             engine=GENERATIVE_MODEL,         )          answer = response.choices[0].text.strip()         logging.info(f\"[get_answer_from_files] answer: {answer}\")          return jsonify({\"answer\": answer})      except Exception as e:         logging.info(f\"[get_answer_from_files] error: {e}\")         return str(e)"},{"location":"apps/file-q-and-a/nextjs-with-flask-server/server/app/","title":"App","text":"In\u00a0[\u00a0]: Copied! <pre>from __future__ import print_function\nfrom config import *\n</pre> from __future__ import print_function from config import * In\u00a0[\u00a0]: Copied! <pre>import tiktoken\nimport pinecone\nimport uuid\nimport sys\nimport logging\n</pre> import tiktoken import pinecone import uuid import sys import logging In\u00a0[\u00a0]: Copied! <pre>from flask import Flask, jsonify\nfrom flask_cors import CORS, cross_origin\nfrom flask import request\n</pre> from flask import Flask, jsonify from flask_cors import CORS, cross_origin from flask import request In\u00a0[\u00a0]: Copied! <pre>from handle_file import handle_file\nfrom answer_question import get_answer_from_files\n</pre> from handle_file import handle_file from answer_question import get_answer_from_files In\u00a0[\u00a0]: Copied! <pre>logging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n    handlers=[\n        logging.FileHandler(\"debug.log\"),\n        logging.StreamHandler(sys.stdout)\n    ]\n)\n</pre> logging.basicConfig(     level=logging.INFO,     format=\"%(asctime)s [%(levelname)s] %(message)s\",     handlers=[         logging.FileHandler(\"debug.log\"),         logging.StreamHandler(sys.stdout)     ] ) In\u00a0[\u00a0]: Copied! <pre>logging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n    handlers=[\n        logging.FileHandler(\"debug.log\"),\n        logging.StreamHandler(sys.stdout)\n    ]\n)\n</pre> logging.basicConfig(     level=logging.INFO,     format=\"%(asctime)s [%(levelname)s] %(message)s\",     handlers=[         logging.FileHandler(\"debug.log\"),         logging.StreamHandler(sys.stdout)     ] ) In\u00a0[\u00a0]: Copied! <pre>def load_pinecone_index() -&gt; pinecone.Index:\n\"\"\"\n    Load index from Pinecone, raise error if the index can't be found.\n    \"\"\"\n    pinecone.init(\n        api_key=PINECONE_API_KEY,\n        environment=PINECONE_ENV,\n    )\n    index_name = PINECONE_INDEX\n    if not index_name in pinecone.list_indexes():\n        print(pinecone.list_indexes())\n        raise KeyError(f\"Index '{index_name}' does not exist.\")\n    index = pinecone.Index(index_name)\n\n    return index\n</pre> def load_pinecone_index() -&gt; pinecone.Index:     \"\"\"     Load index from Pinecone, raise error if the index can't be found.     \"\"\"     pinecone.init(         api_key=PINECONE_API_KEY,         environment=PINECONE_ENV,     )     index_name = PINECONE_INDEX     if not index_name in pinecone.list_indexes():         print(pinecone.list_indexes())         raise KeyError(f\"Index '{index_name}' does not exist.\")     index = pinecone.Index(index_name)      return index In\u00a0[\u00a0]: Copied! <pre>def create_app():\n    pinecone_index = load_pinecone_index()\n    tokenizer = tiktoken.get_encoding(\"gpt2\")\n    session_id = str(uuid.uuid4().hex)\n    app = Flask(__name__)\n    app.pinecone_index = pinecone_index\n    app.tokenizer = tokenizer\n    app.session_id = session_id\n    # log session id\n    logging.info(f\"session_id: {session_id}\")\n    app.config[\"file_text_dict\"] = {}\n    CORS(app, supports_credentials=True)\n\n    return app\n</pre> def create_app():     pinecone_index = load_pinecone_index()     tokenizer = tiktoken.get_encoding(\"gpt2\")     session_id = str(uuid.uuid4().hex)     app = Flask(__name__)     app.pinecone_index = pinecone_index     app.tokenizer = tokenizer     app.session_id = session_id     # log session id     logging.info(f\"session_id: {session_id}\")     app.config[\"file_text_dict\"] = {}     CORS(app, supports_credentials=True)      return app In\u00a0[\u00a0]: Copied! <pre>app = create_app()\n</pre> app = create_app() In\u00a0[\u00a0]: Copied! <pre>@app.route(f\"/process_file\", methods=[\"POST\"])\n@cross_origin(supports_credentials=True)\ndef process_file():\n    try:\n        file = request.files['file']\n        logging.info(str(file))\n        handle_file(\n            file, app.session_id, app.pinecone_index, app.tokenizer)\n        return jsonify({\"success\": True})\n    except Exception as e:\n        logging.error(str(e))\n        return jsonify({\"success\": False})\n</pre> @app.route(f\"/process_file\", methods=[\"POST\"]) @cross_origin(supports_credentials=True) def process_file():     try:         file = request.files['file']         logging.info(str(file))         handle_file(             file, app.session_id, app.pinecone_index, app.tokenizer)         return jsonify({\"success\": True})     except Exception as e:         logging.error(str(e))         return jsonify({\"success\": False}) In\u00a0[\u00a0]: Copied! <pre>@app.route(f\"/answer_question\", methods=[\"POST\"])\n@cross_origin(supports_credentials=True)\ndef answer_question():\n    try:\n        params = request.get_json()\n        question = params[\"question\"]\n\n        answer_question_response = get_answer_from_files(\n            question, app.session_id, app.pinecone_index)\n        return answer_question_response\n    except Exception as e:\n        return str(e)\n</pre> @app.route(f\"/answer_question\", methods=[\"POST\"]) @cross_origin(supports_credentials=True) def answer_question():     try:         params = request.get_json()         question = params[\"question\"]          answer_question_response = get_answer_from_files(             question, app.session_id, app.pinecone_index)         return answer_question_response     except Exception as e:         return str(e) In\u00a0[\u00a0]: Copied! <pre>@app.route(\"/healthcheck\", methods=[\"GET\"])\n@cross_origin(supports_credentials=True)\ndef healthcheck():\n    return \"OK\"\n</pre> @app.route(\"/healthcheck\", methods=[\"GET\"]) @cross_origin(supports_credentials=True) def healthcheck():     return \"OK\" In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    app.run(debug=True, port=SERVER_PORT, threaded=True)\n</pre> if __name__ == \"__main__\":     app.run(debug=True, port=SERVER_PORT, threaded=True)"},{"location":"apps/file-q-and-a/nextjs-with-flask-server/server/config/","title":"Config","text":"In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\nimport logging\nimport sys\nfrom pprint import pformat\nimport yaml\n</pre> from pathlib import Path import logging import sys from pprint import pformat import yaml In\u00a0[\u00a0]: Copied! <pre># Load config items from config.yaml.\n# Use Path.resolve() to get the absolute path of the parent directory\nyaml_dir = Path(__file__).resolve().parent\nyaml_path = yaml_dir / \"config.yaml\"  # Use Path / operator to join paths\n</pre> # Load config items from config.yaml. # Use Path.resolve() to get the absolute path of the parent directory yaml_dir = Path(__file__).resolve().parent yaml_path = yaml_dir / \"config.yaml\"  # Use Path / operator to join paths In\u00a0[\u00a0]: Copied! <pre>def load_yaml_config(path):\n\"\"\"Load a yaml file and return a dictionary of its contents.\"\"\"\n    try:\n        with open(path, \"r\") as stream:\n            return yaml.safe_load(stream)\n    except yaml.YAMLError as exc:\n        logging.error(f\"Failed to load {path}: {exc}\")\n        return None\n</pre> def load_yaml_config(path):     \"\"\"Load a yaml file and return a dictionary of its contents.\"\"\"     try:         with open(path, \"r\") as stream:             return yaml.safe_load(stream)     except yaml.YAMLError as exc:         logging.error(f\"Failed to load {path}: {exc}\")         return None In\u00a0[\u00a0]: Copied! <pre># Load the config and update the global variables\nyaml_config = load_yaml_config(yaml_path)\nif yaml_config is not None:\n    logging.info(f\"Loaded config from {yaml_path}:\")\n    logging.info(pformat(yaml_config))\n    globals().update(yaml_config)\nelse:\n    logging.error(f\"Could not load config from {yaml_path}.\")\n    sys.exit(1)  # Exit the program if the config is invalid\n</pre> # Load the config and update the global variables yaml_config = load_yaml_config(yaml_path) if yaml_config is not None:     logging.info(f\"Loaded config from {yaml_path}:\")     logging.info(pformat(yaml_config))     globals().update(yaml_config) else:     logging.error(f\"Could not load config from {yaml_path}.\")     sys.exit(1)  # Exit the program if the config is invalid In\u00a0[\u00a0]: Copied! <pre># Set a default value for SERVER_PORT if not specified in the config\nSERVER_PORT = yaml_config.get(\"SERVER_PORT\", None)\n</pre> # Set a default value for SERVER_PORT if not specified in the config SERVER_PORT = yaml_config.get(\"SERVER_PORT\", None) In\u00a0[\u00a0]: Copied! <pre># Use Path.resolve() to get the absolute path of the current directory\nSERVER_DIR = Path(__file__).resolve().parent\n</pre> # Use Path.resolve() to get the absolute path of the current directory SERVER_DIR = Path(__file__).resolve().parent"},{"location":"apps/file-q-and-a/nextjs-with-flask-server/server/handle_file/","title":"Handle file","text":"In\u00a0[\u00a0]: Copied! <pre>import logging\nimport sys\nimport docx2txt\n</pre> import logging import sys import docx2txt In\u00a0[\u00a0]: Copied! <pre>from PyPDF2 import PdfReader\nfrom numpy import array, average\nfrom flask import current_app\nfrom config import *\n</pre> from PyPDF2 import PdfReader from numpy import array, average from flask import current_app from config import * In\u00a0[\u00a0]: Copied! <pre>from utils import get_embeddings, get_pinecone_id_for_file_chunk\n</pre> from utils import get_embeddings, get_pinecone_id_for_file_chunk In\u00a0[\u00a0]: Copied! <pre># Set up logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n    handlers=[\n        logging.FileHandler(\"debug.log\"),\n        logging.StreamHandler(sys.stdout)\n    ]\n)\n</pre> # Set up logging logging.basicConfig(     level=logging.INFO,     format=\"%(asctime)s [%(levelname)s] %(message)s\",     handlers=[         logging.FileHandler(\"debug.log\"),         logging.StreamHandler(sys.stdout)     ] ) In\u00a0[\u00a0]: Copied! <pre># Handle a file by extracting its text, creating embeddings, and upserting them to Pinecone\ndef handle_file(file, session_id, pinecone_index, tokenizer):\n\"\"\"Handle a file by extracting its text, creating embeddings, and upserting them to Pinecone.\"\"\"\n    filename = file.filename\n    logging.info(\"[handle_file] Handling file: {}\".format(filename))\n\n    # Get the file text dict from the current app config\n    file_text_dict = current_app.config[\"file_text_dict\"]\n\n    # Extract text from the file\n    try:\n        extracted_text = extract_text_from_file(file)\n    except ValueError as e:\n        logging.error(\n            \"[handle_file] Error extracting text from file: {}\".format(e))\n        raise e\n\n    # Save extracted text to file text dict\n    file_text_dict[filename] = extracted_text\n\n    # Handle the extracted text as a string\n    return handle_file_string(filename, session_id, extracted_text, pinecone_index, tokenizer, file_text_dict)\n</pre> # Handle a file by extracting its text, creating embeddings, and upserting them to Pinecone def handle_file(file, session_id, pinecone_index, tokenizer):     \"\"\"Handle a file by extracting its text, creating embeddings, and upserting them to Pinecone.\"\"\"     filename = file.filename     logging.info(\"[handle_file] Handling file: {}\".format(filename))      # Get the file text dict from the current app config     file_text_dict = current_app.config[\"file_text_dict\"]      # Extract text from the file     try:         extracted_text = extract_text_from_file(file)     except ValueError as e:         logging.error(             \"[handle_file] Error extracting text from file: {}\".format(e))         raise e      # Save extracted text to file text dict     file_text_dict[filename] = extracted_text      # Handle the extracted text as a string     return handle_file_string(filename, session_id, extracted_text, pinecone_index, tokenizer, file_text_dict) In\u00a0[\u00a0]: Copied! <pre># Extract text from a file based on its mimetype\ndef extract_text_from_file(file):\n\"\"\"Return the text content of a file.\"\"\"\n    if file.mimetype == \"application/pdf\":\n        # Extract text from pdf using PyPDF2\n        reader = PdfReader(file)\n        extracted_text = \"\"\n        for page in reader.pages:\n            extracted_text += page.extract_text()\n    elif file.mimetype == \"text/plain\":\n        # Read text from plain text file\n        extracted_text = file.read().decode(\"utf-8\")\n        file.close()\n    elif file.mimetype == \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\":\n        # Extract text from docx using docx2txt\n        extracted_text = docx2txt.process(file)\n    else:\n        # Unsupported file type\n        raise ValueError(\"Unsupported file type: {}\".format(file.mimetype))\n\n    return extracted_text\n</pre> # Extract text from a file based on its mimetype def extract_text_from_file(file):     \"\"\"Return the text content of a file.\"\"\"     if file.mimetype == \"application/pdf\":         # Extract text from pdf using PyPDF2         reader = PdfReader(file)         extracted_text = \"\"         for page in reader.pages:             extracted_text += page.extract_text()     elif file.mimetype == \"text/plain\":         # Read text from plain text file         extracted_text = file.read().decode(\"utf-8\")         file.close()     elif file.mimetype == \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\":         # Extract text from docx using docx2txt         extracted_text = docx2txt.process(file)     else:         # Unsupported file type         raise ValueError(\"Unsupported file type: {}\".format(file.mimetype))      return extracted_text In\u00a0[\u00a0]: Copied! <pre># Handle a file string by creating embeddings and upserting them to Pinecone\ndef handle_file_string(filename, session_id, file_body_string, pinecone_index, tokenizer, file_text_dict):\n\"\"\"Handle a file string by creating embeddings and upserting them to Pinecone.\"\"\"\n    logging.info(\"[handle_file_string] Starting...\")\n\n    # Clean up the file string by replacing newlines and double spaces\n    clean_file_body_string = file_body_string.replace(\n        \"\\n\", \"; \").replace(\"  \", \" \")\n    # Add the filename to the text to embed\n    text_to_embed = \"Filename is: {}; {}\".format(\n        filename, clean_file_body_string)\n\n    # Create embeddings for the text\n    try:\n        text_embeddings, average_embedding = create_embeddings_for_text(\n            text_to_embed, tokenizer)\n        logging.info(\n            \"[handle_file_string] Created embedding for {}\".format(filename))\n    except Exception as e:\n        logging.error(\n            \"[handle_file_string] Error creating embedding: {}\".format(e))\n        raise e\n\n    # Get the vectors array of triples: file_chunk_id, embedding, metadata for each embedding\n    # Metadata is a dict with keys: filename, file_chunk_index\n    vectors = []\n    for i, (text_chunk, embedding) in enumerate(text_embeddings):\n        id = get_pinecone_id_for_file_chunk(session_id, filename, i)\n        file_text_dict[id] = text_chunk\n        vectors.append(\n            (id, embedding, {\"filename\": filename, \"file_chunk_index\": i}))\n\n        logging.info(\n            \"[handle_file_string] Text chunk {}: {}\".format(i, text_chunk))\n\n    # Split the vectors array into smaller batches of max length 2000\n    batch_size = MAX_PINECONE_VECTORS_TO_UPSERT_PATCH_SIZE\n    batches = [vectors[i:i+batch_size] for i in range(0, len(vectors), batch_size)]\n\n    # Upsert each batch to Pinecone\n    for batch in batches:\n        try:\n            pinecone_index.upsert(\n                vectors=batch, namespace=session_id)\n\n            logging.info(\n                \"[handle_file_string] Upserted batch of embeddings for {}\".format(filename))\n        except Exception as e:\n            logging.error(\n                \"[handle_file_string] Error upserting batch of embeddings to Pinecone: {}\".format(e))\n            raise e\n</pre> # Handle a file string by creating embeddings and upserting them to Pinecone def handle_file_string(filename, session_id, file_body_string, pinecone_index, tokenizer, file_text_dict):     \"\"\"Handle a file string by creating embeddings and upserting them to Pinecone.\"\"\"     logging.info(\"[handle_file_string] Starting...\")      # Clean up the file string by replacing newlines and double spaces     clean_file_body_string = file_body_string.replace(         \"\\n\", \"; \").replace(\"  \", \" \")     # Add the filename to the text to embed     text_to_embed = \"Filename is: {}; {}\".format(         filename, clean_file_body_string)      # Create embeddings for the text     try:         text_embeddings, average_embedding = create_embeddings_for_text(             text_to_embed, tokenizer)         logging.info(             \"[handle_file_string] Created embedding for {}\".format(filename))     except Exception as e:         logging.error(             \"[handle_file_string] Error creating embedding: {}\".format(e))         raise e      # Get the vectors array of triples: file_chunk_id, embedding, metadata for each embedding     # Metadata is a dict with keys: filename, file_chunk_index     vectors = []     for i, (text_chunk, embedding) in enumerate(text_embeddings):         id = get_pinecone_id_for_file_chunk(session_id, filename, i)         file_text_dict[id] = text_chunk         vectors.append(             (id, embedding, {\"filename\": filename, \"file_chunk_index\": i}))          logging.info(             \"[handle_file_string] Text chunk {}: {}\".format(i, text_chunk))      # Split the vectors array into smaller batches of max length 2000     batch_size = MAX_PINECONE_VECTORS_TO_UPSERT_PATCH_SIZE     batches = [vectors[i:i+batch_size] for i in range(0, len(vectors), batch_size)]      # Upsert each batch to Pinecone     for batch in batches:         try:             pinecone_index.upsert(                 vectors=batch, namespace=session_id)              logging.info(                 \"[handle_file_string] Upserted batch of embeddings for {}\".format(filename))         except Exception as e:             logging.error(                 \"[handle_file_string] Error upserting batch of embeddings to Pinecone: {}\".format(e))             raise e In\u00a0[\u00a0]: Copied! <pre># Compute the column-wise average of a list of lists\ndef get_col_average_from_list_of_lists(list_of_lists):\n\"\"\"Return the average of each column in a list of lists.\"\"\"\n    if len(list_of_lists) == 1:\n        return list_of_lists[0]\n    else:\n        list_of_lists_array = array(list_of_lists)\n        average_embedding = average(list_of_lists_array, axis=0)\n        return average_embedding.tolist()\n</pre> # Compute the column-wise average of a list of lists def get_col_average_from_list_of_lists(list_of_lists):     \"\"\"Return the average of each column in a list of lists.\"\"\"     if len(list_of_lists) == 1:         return list_of_lists[0]     else:         list_of_lists_array = array(list_of_lists)         average_embedding = average(list_of_lists_array, axis=0)         return average_embedding.tolist() In\u00a0[\u00a0]: Copied! <pre># Create embeddings for a text using a tokenizer and an OpenAI engine\ndef create_embeddings_for_text(text, tokenizer):\n\"\"\"Return a list of tuples (text_chunk, embedding) and an average embedding for a text.\"\"\"\n    token_chunks = list(chunks(text, TEXT_EMBEDDING_CHUNK_SIZE, tokenizer))\n    text_chunks = [tokenizer.decode(chunk) for chunk in token_chunks]\n\n    # Split text_chunks into shorter arrays of max length 10\n    text_chunks_arrays = [text_chunks[i:i+MAX_TEXTS_TO_EMBED_BATCH_SIZE] for i in range(0, len(text_chunks), MAX_TEXTS_TO_EMBED_BATCH_SIZE)]\n\n    # Call get_embeddings for each shorter array and combine the results\n    embeddings = []\n    for text_chunks_array in text_chunks_arrays:\n        embeddings_response = get_embeddings(text_chunks_array, EMBEDDINGS_MODEL)\n        embeddings.extend([embedding[\"embedding\"] for embedding in embeddings_response])\n\n    text_embeddings = list(zip(text_chunks, embeddings))\n\n    average_embedding = get_col_average_from_list_of_lists(embeddings)\n\n    return (text_embeddings, average_embedding)\n</pre> # Create embeddings for a text using a tokenizer and an OpenAI engine def create_embeddings_for_text(text, tokenizer):     \"\"\"Return a list of tuples (text_chunk, embedding) and an average embedding for a text.\"\"\"     token_chunks = list(chunks(text, TEXT_EMBEDDING_CHUNK_SIZE, tokenizer))     text_chunks = [tokenizer.decode(chunk) for chunk in token_chunks]      # Split text_chunks into shorter arrays of max length 10     text_chunks_arrays = [text_chunks[i:i+MAX_TEXTS_TO_EMBED_BATCH_SIZE] for i in range(0, len(text_chunks), MAX_TEXTS_TO_EMBED_BATCH_SIZE)]      # Call get_embeddings for each shorter array and combine the results     embeddings = []     for text_chunks_array in text_chunks_arrays:         embeddings_response = get_embeddings(text_chunks_array, EMBEDDINGS_MODEL)         embeddings.extend([embedding[\"embedding\"] for embedding in embeddings_response])      text_embeddings = list(zip(text_chunks, embeddings))      average_embedding = get_col_average_from_list_of_lists(embeddings)      return (text_embeddings, average_embedding) In\u00a0[\u00a0]: Copied! <pre># Split a text into smaller chunks of size n, preferably ending at the end of a sentence\ndef chunks(text, n, tokenizer):\n    tokens = tokenizer.encode(text)\n\"\"\"Yield successive n-sized chunks from text.\"\"\"\n    i = 0\n    while i &lt; len(tokens):\n        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n        j = min(i + int(1.5 * n), len(tokens))\n        while j &gt; i + int(0.5 * n):\n            # Decode the tokens and check for full stop or newline\n            chunk = tokenizer.decode(tokens[i:j])\n            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n                break\n            j -= 1\n        # If no end of sentence found, use n tokens as the chunk size\n        if j == i + int(0.5 * n):\n            j = min(i + n, len(tokens))\n        yield tokens[i:j]\n        i = j\n</pre> # Split a text into smaller chunks of size n, preferably ending at the end of a sentence def chunks(text, n, tokenizer):     tokens = tokenizer.encode(text)     \"\"\"Yield successive n-sized chunks from text.\"\"\"     i = 0     while i &lt; len(tokens):         # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens         j = min(i + int(1.5 * n), len(tokens))         while j &gt; i + int(0.5 * n):             # Decode the tokens and check for full stop or newline             chunk = tokenizer.decode(tokens[i:j])             if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):                 break             j -= 1         # If no end of sentence found, use n tokens as the chunk size         if j == i + int(0.5 * n):             j = min(i + n, len(tokens))         yield tokens[i:j]         i = j"},{"location":"apps/file-q-and-a/nextjs-with-flask-server/server/utils/","title":"Utils","text":"In\u00a0[\u00a0]: Copied! <pre>import openai\nimport logging\nimport sys\nimport time\n</pre> import openai import logging import sys import time In\u00a0[\u00a0]: Copied! <pre>from config import *\n</pre> from config import * In\u00a0[\u00a0]: Copied! <pre>logging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n    handlers=[\n        logging.FileHandler(\"debug.log\"),\n        logging.StreamHandler(sys.stdout)\n    ]\n)\n</pre> logging.basicConfig(     level=logging.INFO,     format=\"%(asctime)s [%(levelname)s] %(message)s\",     handlers=[         logging.FileHandler(\"debug.log\"),         logging.StreamHandler(sys.stdout)     ] ) In\u00a0[\u00a0]: Copied! <pre>def get_pinecone_id_for_file_chunk(session_id, filename, chunk_index):\n    return str(session_id+\"-!\"+filename+\"-!\"+str(chunk_index))\n</pre> def get_pinecone_id_for_file_chunk(session_id, filename, chunk_index):     return str(session_id+\"-!\"+filename+\"-!\"+str(chunk_index)) In\u00a0[\u00a0]: Copied! <pre>def get_embedding(text, engine):\n    return openai.Engine(id=engine).embeddings(input=[text])[\"data\"][0][\"embedding\"]\n</pre> def get_embedding(text, engine):     return openai.Engine(id=engine).embeddings(input=[text])[\"data\"][0][\"embedding\"] In\u00a0[\u00a0]: Copied! <pre>def get_embeddings(text_array, engine):\n    # Parameters for exponential backoff\n    max_retries = 5 # Maximum number of retries\n    base_delay = 1 # Base delay in seconds\n    factor = 2 # Factor to multiply the delay by after each retry\n    while True:\n        try:\n            return openai.Engine(id=engine).embeddings(input=text_array)[\"data\"]\n        except Exception as e:\n            if max_retries &gt; 0:\n                logging.info(f\"Request failed. Retrying in {base_delay} seconds.\")\n                time.sleep(base_delay)\n                max_retries -= 1\n                base_delay *= factor\n            else:\n                raise e\n</pre> def get_embeddings(text_array, engine):     # Parameters for exponential backoff     max_retries = 5 # Maximum number of retries     base_delay = 1 # Base delay in seconds     factor = 2 # Factor to multiply the delay by after each retry     while True:         try:             return openai.Engine(id=engine).embeddings(input=text_array)[\"data\"]         except Exception as e:             if max_retries &gt; 0:                 logging.info(f\"Request failed. Retrying in {base_delay} seconds.\")                 time.sleep(base_delay)                 max_retries -= 1                 base_delay *= factor             else:                 raise e"},{"location":"apps/web-crawl-q-and-a/web-qa/","title":"Web qa","text":"In\u00a0[20]: Copied! <pre>import requests\nimport re\nimport urllib.request\nfrom bs4 import BeautifulSoup\nfrom collections import deque\nfrom html.parser import HTMLParser\nfrom urllib.parse import urlparse\nimport os\n\n# Regex pattern to match a URL\nHTTP_URL_PATTERN = r'^http[s]*://.+'\n\n# Define root domain to crawl\ndomain = \"openai.com\"\nfull_url = \"https://openai.com/\"\n\n# Create a class to parse the HTML and get the hyperlinks\nclass HyperlinkParser(HTMLParser):\n    def __init__(self):\n        super().__init__()\n        # Create a list to store the hyperlinks\n        self.hyperlinks = []\n\n    # Override the HTMLParser's handle_starttag method to get the hyperlinks\n    def handle_starttag(self, tag, attrs):\n        attrs = dict(attrs)\n\n        # If the tag is an anchor tag and it has an href attribute, add the href attribute to the list of hyperlinks\n        if tag == \"a\" and \"href\" in attrs:\n            self.hyperlinks.append(attrs[\"href\"])\n\n# Function to get the hyperlinks from a URL\ndef get_hyperlinks(url):\n    \n    # Try to open the URL and read the HTML\n    try:\n        # Open the URL and read the HTML\n        with urllib.request.urlopen(url) as response:\n\n            # If the response is not HTML, return an empty list\n            if not response.info().get('Content-Type').startswith(\"text/html\"):\n                return []\n            \n            # Decode the HTML\n            html = response.read().decode('utf-8')\n    except Exception as e:\n        print(e)\n        return []\n\n    # Create the HTML Parser and then Parse the HTML to get hyperlinks\n    parser = HyperlinkParser()\n    parser.feed(html)\n\n    return parser.hyperlinks\n\n# Function to get the hyperlinks from a URL that are within the same domain\ndef get_domain_hyperlinks(local_domain, url):\n    clean_links = []\n    for link in set(get_hyperlinks(url)):\n        clean_link = None\n\n        # If the link is a URL, check if it is within the same domain\n        if re.search(HTTP_URL_PATTERN, link):\n            # Parse the URL and check if the domain is the same\n            url_obj = urlparse(link)\n            if url_obj.netloc == local_domain:\n                clean_link = link\n\n        # If the link is not a URL, check if it is a relative link\n        else:\n            if link.startswith(\"/\"):\n                link = link[1:]\n            elif link.startswith(\"#\") or link.startswith(\"mailto:\"):\n                continue\n            clean_link = \"https://\" + local_domain + \"/\" + link\n\n        if clean_link is not None:\n            if clean_link.endswith(\"/\"):\n                clean_link = clean_link[:-1]\n            clean_links.append(clean_link)\n\n    # Return the list of hyperlinks that are within the same domain\n    return list(set(clean_links))\n\n\ndef crawl(url):\n    # Parse the URL and get the domain\n    local_domain = urlparse(url).netloc\n\n    # Create a queue to store the URLs to crawl\n    queue = deque([url])\n\n    # Create a set to store the URLs that have already been seen (no duplicates)\n    seen = set([url])\n\n    # Create a directory to store the text files\n    if not os.path.exists(\"text/\"):\n            os.mkdir(\"text/\")\n\n    if not os.path.exists(\"text/\"+local_domain+\"/\"):\n            os.mkdir(\"text/\" + local_domain + \"/\")\n\n    # Create a directory to store the csv files\n    if not os.path.exists(\"processed\"):\n            os.mkdir(\"processed\")\n\n    # While the queue is not empty, continue crawling\n    while queue:\n\n        # Get the next URL from the queue\n        url = queue.pop()\n        print(url) # for debugging and to see the progress\n\n        # Save text from the url to a &lt;url&gt;.txt file\n        with open('text/'+local_domain+'/'+url[8:].replace(\"/\", \"_\") + \".txt\", \"w\") as f:\n\n            # Get the text from the URL using BeautifulSoup\n            soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n\n            # Get the text but remove the tags\n            text = soup.get_text()\n\n            # If the crawler gets to a page that requires JavaScript, it will stop the crawl\n            if (\"You need to enable JavaScript to run this app.\" in text):\n                print(\"Unable to parse page \" + url + \" due to JavaScript being required\")\n            \n            # Otherwise, write the text to the file in the text directory\n            f.write(text)\n\n        # Get the hyperlinks from the URL and add them to the queue\n        for link in get_domain_hyperlinks(local_domain, url):\n            if link not in seen:\n                queue.append(link)\n                seen.add(link)\n\ncrawl(full_url)\n</pre> import requests import re import urllib.request from bs4 import BeautifulSoup from collections import deque from html.parser import HTMLParser from urllib.parse import urlparse import os  # Regex pattern to match a URL HTTP_URL_PATTERN = r'^http[s]*://.+'  # Define root domain to crawl domain = \"openai.com\" full_url = \"https://openai.com/\"  # Create a class to parse the HTML and get the hyperlinks class HyperlinkParser(HTMLParser):     def __init__(self):         super().__init__()         # Create a list to store the hyperlinks         self.hyperlinks = []      # Override the HTMLParser's handle_starttag method to get the hyperlinks     def handle_starttag(self, tag, attrs):         attrs = dict(attrs)          # If the tag is an anchor tag and it has an href attribute, add the href attribute to the list of hyperlinks         if tag == \"a\" and \"href\" in attrs:             self.hyperlinks.append(attrs[\"href\"])  # Function to get the hyperlinks from a URL def get_hyperlinks(url):          # Try to open the URL and read the HTML     try:         # Open the URL and read the HTML         with urllib.request.urlopen(url) as response:              # If the response is not HTML, return an empty list             if not response.info().get('Content-Type').startswith(\"text/html\"):                 return []                          # Decode the HTML             html = response.read().decode('utf-8')     except Exception as e:         print(e)         return []      # Create the HTML Parser and then Parse the HTML to get hyperlinks     parser = HyperlinkParser()     parser.feed(html)      return parser.hyperlinks  # Function to get the hyperlinks from a URL that are within the same domain def get_domain_hyperlinks(local_domain, url):     clean_links = []     for link in set(get_hyperlinks(url)):         clean_link = None          # If the link is a URL, check if it is within the same domain         if re.search(HTTP_URL_PATTERN, link):             # Parse the URL and check if the domain is the same             url_obj = urlparse(link)             if url_obj.netloc == local_domain:                 clean_link = link          # If the link is not a URL, check if it is a relative link         else:             if link.startswith(\"/\"):                 link = link[1:]             elif link.startswith(\"#\") or link.startswith(\"mailto:\"):                 continue             clean_link = \"https://\" + local_domain + \"/\" + link          if clean_link is not None:             if clean_link.endswith(\"/\"):                 clean_link = clean_link[:-1]             clean_links.append(clean_link)      # Return the list of hyperlinks that are within the same domain     return list(set(clean_links))   def crawl(url):     # Parse the URL and get the domain     local_domain = urlparse(url).netloc      # Create a queue to store the URLs to crawl     queue = deque([url])      # Create a set to store the URLs that have already been seen (no duplicates)     seen = set([url])      # Create a directory to store the text files     if not os.path.exists(\"text/\"):             os.mkdir(\"text/\")      if not os.path.exists(\"text/\"+local_domain+\"/\"):             os.mkdir(\"text/\" + local_domain + \"/\")      # Create a directory to store the csv files     if not os.path.exists(\"processed\"):             os.mkdir(\"processed\")      # While the queue is not empty, continue crawling     while queue:          # Get the next URL from the queue         url = queue.pop()         print(url) # for debugging and to see the progress          # Save text from the url to a .txt file         with open('text/'+local_domain+'/'+url[8:].replace(\"/\", \"_\") + \".txt\", \"w\") as f:              # Get the text from the URL using BeautifulSoup             soup = BeautifulSoup(requests.get(url).text, \"html.parser\")              # Get the text but remove the tags             text = soup.get_text()              # If the crawler gets to a page that requires JavaScript, it will stop the crawl             if (\"You need to enable JavaScript to run this app.\" in text):                 print(\"Unable to parse page \" + url + \" due to JavaScript being required\")                          # Otherwise, write the text to the file in the text directory             f.write(text)          # Get the hyperlinks from the URL and add them to the queue         for link in get_domain_hyperlinks(local_domain, url):             if link not in seen:                 queue.append(link)                 seen.add(link)  crawl(full_url) <pre>https://openai.com/\nhttps://openai.com/blog/tags/announcements\nhttps://openai.com/blog/introducing-openai\nhttps://openai.com/blog/authors/ilya\nhttps://openai.com/blog/requests-for-research-2\nhttps://openai.com/blog/authors/diederik\nhttps://openai.com/blog/block-sparse-gpu-kernels\nhttps://openai.com/blog/authors/alec\nhttps://openai.com/blog/fine-tuning-gpt-2\nhttps://openai.com/blog/authors/paul\nhttps://openai.com/blog/concrete-ai-safety-problems\nhttps://openai.com/blog/learning-to-summarize-with-human-feedback\nhttps://openai.com/blog/authors/long\nhttps://openai.com/blog/authors/lowe\nhttps://openai.com/blog/learning-to-cooperate-compete-and-communicate\nhttps://openai.com/blog/authors/jean\nhttps://openai.com/blog/authors/igor\nhttps://openai.com/blog/neural-mmo\nhttps://openai.com/blog/authors/phillip\nhttps://openai.com/blog/evolved-policy-gradients\nhttps://openai.com/blog/authors/richard\nhttps://openai.com/blog/better-exploration-with-parameter-noise\nhttps://openai.com/blog/authors/xi\nhttps://openai.com/blog/authors/matthias\nhttps://openai.com/blog/solving-rubiks-cube\nhttps://openai.com/blog/authors/ilge\nhttps://openai.com/blog/vpt\nhttps://openai.com/blog/authors/brandon\nhttps://openai.com/blog/authors/raul\nhttps://openai.com/blog/authors/bowen\nhttps://openai.com/blog/authors/jie\nhttps://openai.com/blog/tags/five\nhttps://openai.com/blog/openai-five-benchmark-results\nhttps://openai.com/blog/openai-five/#rapid\nhttps://openai.com/blog/authors/henrique\nhttps://openai.com/blog/authors/susan\nhttps://openai.com/blog/authors/brooke\nhttps://openai.com/blog/authors/michael-petrov\nhttps://openai.com/blog/multimodal-neurons\nhttps://openai.com/blog/authors/shan\nhttps://openai.com/blog/authors/daniela\nhttps://openai.com/blog/authors/nick\nhttps://openai.com/blog/authors/chris\nhttps://openai.com/blog/introducing-activation-atlases\nhttps://openai.com/blog/authors/ludwig-schubert\nhttps://openai.com/blog/authors/justin\nhttps://openai.com/blog/authors/gabriel\nhttps://openai.com/blog/microscope\nhttps://openai.com/blog/authors/przemyslaw\nhttps://openai.com/blog/authors/david\nhttps://openai.com/blog/authors/jakub-pachocki\nhttps://openai.com/blog/authors/christy\nhttps://openai.com/blog/improving-language-model-behavior\nhttps://openai.com/blog/authors/irene\nhttps://openai.com/blog/gpt-2-1-5b-release\nhttps://openai.com/blog/authors/jack-clark\nhttps://openai.com/blog/cooperation-on-safety\nhttps://openai.com/blog/authors/amanda\nhttps://openai.com/blog/ai-safety-needs-social-scientists\nhttps://openai.com/blog/adversarial-example-research\nhttps://openai.com/blog/authors/sandy\nhttps://openai.com/blog/authors/ian\nhttps://openai.com/blog/machine-learning-unconference\nhttps://openai.com/events/code-of-conduct.txt\nhttps://openai.com/blog/authors/rocky\nhttps://openai.com/blog/authors/nicolas\nhttps://openai.com/blog/preparing-for-malicious-uses-of-ai\nhttps://openai.com/blog/authors/michael\nhttps://openai.com/blog/spam-detection-in-the-physical-world\nhttps://openai.com/blog/authors/rachel\nhttps://openai.com/blog/authors/alex-ray\nhttps://openai.com/blog/generalizing-from-simulation\nhttps://openai.com/blog/authors/lerrel\nhttps://openai.com/blog/authors/xue\nhttps://openai.com/blog/faster-robot-simulation-in-python\nhttps://openai.com/blog/safety-gym\nhttps://openai.com/blog/authors/joshua\nhttps://openai.com/blog/spinning-up-in-deep-rl\nhttps://openai.com/blog/spinning-up-in-deep-rl-workshop-review\nhttps://openai.com/blog/hackathon-follow-up\nhttps://openai.com/blog/authors/parnian\nhttps://openai.com/blog/openai-hackathon\nhttps://openai.com/events/hackathon.txt\nhttps://openai.com/blog/authors/josh-tobin\nhttps://openai.com/blog/report-from-the-self-organizing-conference\nhttps://openai.com/blog/faulty-reward-functions\nhttps://openai.com/blog/authors/miles\nhttps://openai.com/blog/language-model-safety-and-misuse\nhttps://openai.com/blog/authors/tyna\nhttps://openai.com/blog/webgpt\nhttps://openai.com/blog/authors/jacob-hilton\nhttps://openai.com/blog/measuring-goodharts-law\nhttps://openai.com/careers/research-engineer\nhttps://openai.com/blog/authors/leo\nhttps://openai.com/blog/learning-to-summarize-with-human-feedback/#optimizingtherewardmodel\nhttps://openai.com/blog/procgen-benchmark\nhttps://openai.com/blog/first-retro-contest-retrospective\nhttps://openai.com/blog/authors/oleg\nhttps://openai.com/blog/roboschool\nhttps://openai.com/blog/gym-retro\nhttps://openai.com/blog/authors/vicki\nhttps://openai.com/blog/retro-contest\nhttps://openai.com/blog/authors/alex\nhttps://openai.com/blog/reptile\nhttps://openai.com/blog/dall-e-2-pre-training-mitigations\nhttps://openai.com/blog/authors/larissa\nhttps://openai.com/blog/openai-scholars-2018-final-projects\nhttps://openai.com/blog/authors/karl\nhttps://openai.com/blog/grade-school-math\nhttps://openai.com/blog/authors/vineet\nhttps://openai.com/blog/authors/christopher\nhttps://openai.com/blog/quantifying-generalization-in-reinforcement-learning\nhttps://openai.com/blog/authors/reiichiro\nhttps://openai.com/blog/authors/suchir\nhttps://openai.com/blog/authors/katie\nhttps://openai.com/blog/authors/sandhini\nhttps://openai.com/blog/authors/pamela\nhttps://openai.com/blog/authors/steven\nhttps://openai.com/blog/authors/gretchen\nhttps://openai.com/blog/authors/jan\nhttps://openai.com/blog/critiques\nhttps://openai.com/blog/authors/william-saunders\nhttps://openai.com/blog/authors/catherine\nhttps://openai.com/blog/our-approach-to-alignment-research\nhttps://openai.com/blog/best-practices-for-deploying-language-models\nhttps://openai.com/blog/instruction-following/#limitations\nhttps://openai.com/blog/economic-impacts\nhttps://openai.com/blog/authors/sam-manning\nhttps://openai.com/scholars\nhttps://openai.com/blog/openai-scholars-2021-final-projects\nhttps://openai.com/blog/openai-scholars-2020-final-projects\nhttps://openai.com/resources\nhttps://openai.com/blog/openai-scholars-spring-2020\nhttps://openai.com/blog/openai-scholars-class-of-19\nhttps://openai.com/blog/openai-scholars-2019-final-projects\nhttps://openai.com/blog/authors/jonathan\nhttps://openai.com/blog/discovering-types-for-entity-disambiguation\nhttps://openai.com/blog/openai-five-benchmark\nhttps://openai.com/blog/openai-five-defeats-dota-2-world-champions/#arena\nhttps://openai.com/blog/openai-five/#ourapproach\nhttps://openai.com/blog/more-on-dota-2/#botexploits\nhttps://openai.com/blog/openai-five-benchmark-results/#training\nhttps://openai.com/blog/openai-five-finals\nhttps://openai.com/five/#overview\nhttps://openai.com/blog/dota-2\nhttps://openai.com/the-international\nhttps://openai.com/blog/more-on-dota-2\nhttps://openai.com/blog/the-international-2018-results\nhttps://openai.com/blog/openai-five-defeats-dota-2-world-champions/#cooperativemode\nhttps://openai.com/blog/openai-five-defeats-dota-2-world-champions\nhttps://openai.com/blog/authors/jeff\nhttps://openai.com/blog/authors/adrien\nhttps://openai.com/blog/authors/joost\nhttps://openai.com/blog/authors/peter-zhokhov\nhttps://openai.com/blog/authors/glenn\nhttps://openai.com/blog/authors/peter\nhttps://openai.com/blog/authors/raphael\nhttps://openai.com/blog/authors/lilian\nhttps://openai.com/blog/techniques-for-training-large-neural-networks\nhttps://openai.com/blog/authors/alex-paino\nhttps://openai.com/blog/authors/nikolas\nhttps://openai.com/blog/openai-five\nhttps://openai.com/blog/authors/bob\nhttps://openai.com/blog/authors/qiming\nhttps://openai.com/blog/authors/wojciech\nhttps://openai.com/blog/authors/arthur\nhttps://openai.com/blog/authors/mateusz\nhttps://openai.com/blog/authors/maciek\nhttps://openai.com/blog/authors/jerry\nhttps://openai.com/blog/authors/lei\nhttps://openai.com/blog/how-to-train-your-openai-five\nhttps://openai.com/blog/authors/jonas-schneider\nhttps://openai.com/jobs/#robotics\nhttps://openai.com/interview-guide\nhttps://openai.com/blog/learning-dexterity\nhttps://openai.com/blog/authors/rafal\nhttps://openai.com/blog/ingredients-for-robotics-research\nhttps://openai.com/blog/authors/vikash\nhttps://openai.com/blog/authors/marcin\nhttps://openai.com/blog/authors/prafulla\nhttps://openai.com/blog/authors/szymon-sidor\nhttps://openai.com/blog/openai-baselines-dqn\nhttps://openai.com/blog/authors/tamim\nhttps://openai.com/blog/learning-montezumas-revenge-from-a-single-demonstration\nhttps://openai.com/blog/authors/bradly\nhttps://openai.com/blog/authors/rein\nhttps://openai.com/blog/authors/jonathan-ho\nhttps://openai.com/blog/learning-a-hierarchy\nhttps://openai.com/blog/authors/peter-chen\nhttps://openai.com/blog/authors/kevin\nhttps://openai.com/blog/authors/filip\nhttps://openai.com/five\nhttps://openai.com/blog/authors/yilun\nhttps://openai.com/blog/authors/joseph\nhttps://openai.com/blog/interpretable-machine-learning-through-teaching\nhttps://openai.com/blog/authors/smitha\nhttps://openai.com/blog/learning-to-model-other-minds\nhttps://openai.com/blog/authors/shimon\nhttps://openai.com/blog/authors/maruan\nhttps://openai.com/blog/authors/jakob-foerster\nhttps://openai.com/blog/nonlinear-computation-in-linear-networks\nhttps://openai.com/blog/energy-based-models\nhttps://openai.com/blog/emergent-tool-use\nhttps://openai.com/blog/authors/ingmar\nhttps://openai.com/blog/authors/todor\nhttps://openai.com/blog/learning-concepts-with-energy-functions\nhttps://openai.com/blog/authors/yi\nhttps://openai.com/blog/authors/pieter\nhttps://openai.com/blog/authors/aviv\nhttps://openai.com/blog/instruction-following\nhttps://openai.com/blog/learning-to-communicate\nhttps://openai.com/blog/authors/jon\nhttps://openai.com/blog/summarizing-books\nhttps://openai.com/blog/authors/chelsea\nhttps://openai.com/blog/gathering_human_feedback\nhttps://openai.com/blog/authors/dario-amodei\nhttps://openai.com/blog/science-of-ai\nhttps://openai.com/blog/authors/jared\nhttps://openai.com/blog/authors/sam\nhttps://openai.com/blog/gpt-2-6-month-follow-up\nhttps://openai.com/blog/better-language-models/#update\nhttps://openai.com/blog/authors/david-luan\nhttps://openai.com/blog/authors/danny\nhttps://openai.com/blog/ai-and-efficiency\nhttps://openai.com/blog/authors/david-lansky\nhttps://openai.com/blog/authors/tom\nhttps://openai.com/blog/testing-robustness\nhttps://openai.com/blog/authors/jacob\nhttps://openai.com/blog/authors/yi-sun\nhttps://openai.com/blog/authors/daniel\nhttps://openai.com/blog/authors/dan\nhttps://openai.com/blog/deep-reinforcement-learning-from-human-preferences\nhttps://openai.com/blog/authors/geoffrey\nhttps://openai.com/blog/debate\nhttps://openai.com/blog/authors/jeffrey\nhttps://openai.com/blog/authors/nisan\nhttps://openai.com/blog/amplifying-ai-training\nhttps://openai.com/blog/authors/daniel-ziegler\nhttps://openai.com/blog/baselines-acktr-a2c\nhttps://openai.com/blog/authors/yuhuai\nhttps://openai.com/blog/authors/shun\nhttps://openai.com/blog/authors/elman\nhttps://openai.com/blog/openai-baselines-ppo\nhttps://openai.com/blog/language-unsupervised\nhttps://openai.com/blog/tags/baselines\nhttps://openai.com/blog/authors/scott\nhttps://openai.com/blog/sparse-transformer\nhttps://openai.com/blog/authors/rewon\nhttps://openai.com/blog/glow\nhttps://openai.com/blog/authors/john\nhttps://openai.com/blog/openai-gym-beta\nhttps://openai.com/blog/authors/tim\nhttps://openai.com/jobs\nhttps://openai.com/blog/formal-math\nhttps://openai.com/blog/authors/stanislas\nhttps://openai.com/blog/authors/jesse\nhttps://openai.com/blog/generative-models\nhttps://openai.com/blog/authors/andrej\nhttps://openai.com/blog/distill\nhttps://openai.com/blog/authors/vicki-cheung\nhttps://openai.com/blog/jukebox\nhttps://openai.com/projects/five\nhttps://openai.com/blog/authors/christine\nhttps://openai.com/blog/authors/jong\nhttps://openai.com/blog/authors/heewoo\nhttps://openai.com/blog/musenet\nhttps://openai.com/blog/better-language-models\nhttps://openai.com/blog/robots-that-learn\nhttps://openai.com/blog/authors/ankur\nhttps://openai.com/blog/authors/erika-reinhardt\nhttps://openai.com/blog/deep-double-descent\nhttps://openai.com/blog/authors/tristan\nhttps://openai.com/blog/authors/preetum\nhttps://openai.com/blog/authors/boaz\nhttps://openai.com/blog/authors/yamini\nhttps://openai.com/blog/authors/gal\nhttps://openai.com/blog/tags/gpt-2\nhttps://openai.com/blog/clip\nhttps://openai.com/blog/ai-and-compute\nhttps://openai.com/blog/authors/girish\nhttps://openai.com/blog/special-projects\nhttps://openai.com/blog/authors/sam-altman\nhttps://openai.com/blog/unsupervised-sentiment-neuron\nhttps://openai.com/blog/dall-e\nhttps://openai.com/blog/authors/aditya\nhttps://openai.com/blog/authors/mark\nhttps://openai.com/blog/authors/mikhail\nhttps://openai.com/blog/authors/vedant\nhttps://openai.com/blog/competitive-self-play\nhttps://openai.com/blog/authors/trapit\nhttps://openai.com/blog/meta-learning-for-wrestling\nhttps://openai.com/blog/authors/yura\nhttps://openai.com/blog/reinforcement-learning-with-prediction-based-rewards\nhttps://openai.com/blog/authors/harri\nhttps://openai.com/blog/image-gpt\nhttps://openai.com/blog/evolution-strategies\nhttps://openai.com/blog/infrastructure-for-deep-learning\nhttps://openai.com/blog/generative-models/#gan\nhttps://openai.com/blog/generative-models#improving-gans\nhttps://openai.com/blog/tags/multimodal\nhttps://openai.com/gpt-3\nhttps://openai.com/javascript:setMathjaxCookie()\nHTTP Error 404: Not Found\nhttps://openai.com/abs/2005.14165v1\nHTTP Error 404: Not Found\nhttps://openai.com/list/cs.CL/new\nHTTP Error 404: Not Found\nhttps://openai.com/abs/2005.14165v3\nHTTP Error 404: Not Found\nhttps://openai.com/auth/show-endorsers/2005.14165\nHTTP Error 404: Not Found\nhttps://openai.com/list/cs/recent\nHTTP Error 404: Not Found\nhttps://openai.com/abs/2005.14165?context=cs\nHTTP Error 404: Not Found\nhttps://openai.com/{url_path('ignore_me')}\nHTTP Error 404: Not Found\nhttps://openai.com/abs/2005.14165v2\nHTTP Error 404: Not Found\nhttps://openai.com/show-email/b5cb66e9/2005.14165\nHTTP Error 404: Not Found\nhttps://openai.com/prevnext?id=2005.14165&amp;function=next&amp;context=cs.CL\nHTTP Error 404: Not Found\nhttps://openai.com/format/2005.14165\nHTTP Error 404: Not Found\nhttps://openai.com/prevnext?id=2005.14165&amp;function=prev&amp;context=cs.CL\nHTTP Error 404: Not Found\nhttps://openai.com/pdf/2005.14165\nHTTP Error 404: Not Found\nhttps://openai.com/tb/2005.14165\nHTTP Error 404: Not Found\nhttps://openai.com/list/cs.CL/2005\nHTTP Error 404: Not Found\nhttps://openai.com/list/cs.CL/recent\nHTTP Error 404: Not Found\nhttps://openai.com/blog/dall-e-2\nhttps://openai.com/blog/authors/openai\nhttps://openai.com/blog/improving-verifiability\nhttps://openai.com/blog/dall-e-2-extending-creativity\nhttps://openai.com/blog/the-international\nhttps://openai.com/blog/symposium-2019\nhttps://openai.com/blog/tags/culture\nhttps://openai.com/blog/learning-day\nhttps://openai.com/blog/openai-fellows-fall-2018\nhttps://openai.com/blog/neurips-2020\nhttps://openai.com/blog/tags/community\nhttps://openai.com/blog/universe\nhttps://openai.com/blog/openai-gym-beta/#rl\nhttps://openai.com/blog/openai-technical-goals/#goal4\nhttps://openai.com/blog/authors/elon\nhttps://openai.com/blog/scaling-kubernetes-to-7500-nodes\nhttps://openai.com/blog/scaling-kubernetes-to-2500-nodes\nhttps://openai.com/blog/authors/christopher-berner\nhttps://openai.com/blog/authors/bchess\nhttps://openai.com/blog/authors/eric\nhttps://openai.com/blog/forecasting-misuse\nhttps://openai.com/forecasting-misuse-paper\nhttps://openai.com/prevnext?id=2301.04246&amp;function=prev&amp;context=cs.CY\nHTTP Error 404: Not Found\nhttps://openai.com/auth/show-endorsers/2301.04246\nHTTP Error 404: Not Found\nhttps://openai.com/format/2301.04246\nHTTP Error 404: Not Found\nhttps://openai.com/pdf/2301.04246\nHTTP Error 404: Not Found\nhttps://openai.com/show-email/64c5c6bd/2301.04246\nHTTP Error 404: Not Found\nhttps://openai.com/list/cs.CY/recent\nHTTP Error 404: Not Found\nhttps://openai.com/prevnext?id=2301.04246&amp;function=next&amp;context=cs.CY\nHTTP Error 404: Not Found\nhttps://openai.com/list/cs.CY/new\nHTTP Error 404: Not Found\nhttps://openai.com/list/cs.CY/2301\nHTTP Error 404: Not Found\nhttps://openai.com/abs/2301.04246?context=cs\nHTTP Error 404: Not Found\nhttps://openai.com/blog/authors/greg\nhttps://openai.com/blog/dall-e-api-now-available-in-public-beta\nhttps://openai.com/blog/api-no-waitlist\nhttps://openai.com/blog/dall-e-introducing-outpainting\nhttps://openai.com/blog/team-update\nhttps://openai.com/blog/chatgpt-plus\nhttps://openai.com/blog/openai-api\nhttps://openai.com/jobs/#applied-ai\nhttps://openai.com/blog/authors/mira\nhttps://openai.com/join\nUnable to parse page https://openai.com/join due to JavaScript being required\nHTTP Error 403: Forbidden\nhttps://openai.com/blog/tags/residency\nhttps://openai.com/blog/openai-licenses-gpt-3-technology-to-microsoft\nhttps://openai.com/blog/microsoft\nhttps://openai.com/blog/team-update-august\nhttps://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text\nhttps://openai.com/blog/authors/lama\nhttps://openai.com/blog/authors/scott-aaronson\nhttps://openai.com/blog/authors/jan-hendrik-kirchner\nhttps://openai.com/blog/tags/api\nhttps://openai.com/blog/openai-fellows\nhttps://openai.com/blog/tags/scholars\nhttps://openai.com/blog/openai-and-microsoft-extend-partnership\nhttps://openai.com/blog/dall-e-now-available-without-waitlist\nhttps://openai.com/blog/helen-toner-joins\nhttps://openai.com/blog/team-update-january\nhttps://openai.com/blog/team-plus-plus#interns\nhttps://openai.com/blog/openai-codex\nhttps://openai.com/blog/openai-scholars-2019\nhttps://openai.com/blog/authors/ashley\nhttps://openai.com/blog/openai-scholars\nhttps://openai.com/blog/dall-e-now-available-in-beta\nhttps://openai.com/blog/new-and-improved-embedding-model\nhttps://openai.com/blog/authors/ryan\nhttps://openai.com/blog/authors/arvind\nhttps://openai.com/blog/authors/ted\nhttps://openai.com/blog/dall-e-2-update\nhttps://openai.com/blog/authors/joanne\nhttps://openai.com/blog/tags/fellows\nhttps://openai.com/blog/openai-summer-fellows-2018\nhttps://openai.com/blog/authors/maddie\nhttps://openai.com/blog/codex-apps\nhttps://openai.com/blog/codex\nHTTP Error 404: Not Found\nhttps://openai.com/blog/new-and-improved-content-moderation-tooling\nhttps://openai.com/blog/authors/teddy\nhttps://openai.com/blog/authors/angela\nhttps://openai.com/blog/authors/chong\nhttps://openai.com/blog/welcome-pieter-and-shivon\nhttps://openai.com/blog/openai-technical-goals\nhttps://openai.com/blog/procgen-minerl-competitions\nhttps://openai.com/blog/will-hurd-joins\nhttps://openai.com/blog/fund\nhttps://openai.com/news\nHTTP Error 404: Not Found\nhttps://openai.com/news/introducing-our-first-investments\nHTTP Error 404: Not Found\nhttps://openai.com/blog/introducing-text-and-code-embeddings\nhttps://openai.com/blog/authors/boris\nhttps://openai.com/blog/openai-scholars-2018-meet-our-scholars\nhttps://openai.com/blog/team-plus-plus\nhttps://openai.com/blog/gpt-3-apps\nhttps://openai.com/jobs/#open\nhttps://openai.com/blog/customized-gpt-3\nhttps://openai.com/blog/authors/luke\nhttps://openai.com/blog/authors/rachel-lim\nhttps://openai.com/blog/authors/michael-wu\nhttps://openai.com/blog/openai-supporters\nhttps://openai.com/blog/openai-residency\nhttps://openai.com/blog/leadership-team-update\nhttps://openai.com/blog/organizational-update\nhttps://openai.com/blog/openai-fellows-interns-2019\nhttps://openai.com/blog/openai-scholars-2020\nhttps://openai.com/blog/gpt-3-edit-insert\nhttps://openai.com/blog/authors/mo\nhttps://openai.com/blog/openai-pytorch\nhttps://openai.com/blog/openai-scholars-2019-meet-our-scholars\nhttps://openai.com/blog/openai-charter\nhttps://openai.com/blog/openai-and-microsoft\nhttps://openai.com/blog/openai-lp\nhttps://openai.com/blog/reducing-bias-and-improving-safety-in-dall-e-2\nhttps://openai.com/terms\nhttps://openai.com/api/policies/service-terms\nhttps://openai.com/api/policies/sharing-publication\nhttps://openai.com/api/policies/terms\nhttps://openai.com/security/disclosure\nhttps://openai.com/blog/whisper\nhttps://openai.com/blog/authors/tao\nhttps://openai.com/research\nhttps://openai.com/api/docs\nUnable to parse page https://openai.com/api/docs due to JavaScript being required\nHTTP Error 403: Forbidden\nhttps://openai.com/dall-e-2\nhttps://openai.com/privacy\nhttps://openai.com/api\nhttps://openai.com/blog\nhttps://openai.com/blog/triton\nhttps://openai.com/blog/authors/philippe\nhttps://openai.com/jobs/#acceleration\nhttps://openai.com/blog/robust-adversarial-inputs\nhttps://openai.com/blog/authors/anish-athalye\nhttps://openai.com/blog/tags/milestones\nhttps://openai.com/alignment\nhttps://openai.com\nhttps://openai.com/publications\nhttps://openai.com/charter\nhttps://openai.com/blog/tags/research\nhttps://openai.com/fund\nhttps://openai.com/about\nhttps://openai.com/timeline\nhttps://openai.com/careers\nhttps://openai.com/api/examples\nUnable to parse page https://openai.com/api/examples due to JavaScript being required\nHTTP Error 403: Forbidden\nhttps://openai.com/api/login\nUnable to parse page https://openai.com/api/login due to JavaScript being required\nHTTP Error 403: Forbidden\nhttps://openai.com/newsroom\nhttps://openai.com/api/policies\nhttps://openai.com/api/pricing\nhttps://openai.com/contact-sales\nhttps://openai.com/api/pricing/#faq-fine-tuning-pricing-calculation\nhttps://openai.com/blog/tags/events\nhttps://openai.com/blog/chatgpt\n</pre> In\u00a0[1]: Copied! <pre>def remove_newlines(serie):\n    serie = serie.str.replace('\\n', ' ')\n    serie = serie.str.replace('\\\\n', ' ')\n    serie = serie.str.replace('  ', ' ')\n    serie = serie.str.replace('  ', ' ')\n    return serie\n</pre> def remove_newlines(serie):     serie = serie.str.replace('\\n', ' ')     serie = serie.str.replace('\\\\n', ' ')     serie = serie.str.replace('  ', ' ')     serie = serie.str.replace('  ', ' ')     return serie In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n\n# Create a list to store the text files\ntexts=[]\n\n# Get all the text files in the text directory\nfor file in os.listdir(\"text/\" + domain + \"/\"):\n\n    # Open the file and read the text\n    with open(\"text/\" + domain + \"/\" + file, \"r\") as f:\n        text = f.read()\n\n        # Omit the first 11 lines and the last 4 lines, then replace -, _, and #update with spaces.\n        texts.append((file[11:-4].replace('-',' ').replace('_', ' ').replace('#update',''), text))\n\n# Create a dataframe from the list of texts\ndf = pd.DataFrame(texts, columns = ['fname', 'text'])\n\n# Set the text column to be the raw text with the newlines removed\ndf['text'] = df.fname + \". \" + remove_newlines(df.text)\ndf.to_csv('processed/scraped.csv')\ndf.head()\n</pre> import pandas as pd  # Create a list to store the text files texts=[]  # Get all the text files in the text directory for file in os.listdir(\"text/\" + domain + \"/\"):      # Open the file and read the text     with open(\"text/\" + domain + \"/\" + file, \"r\") as f:         text = f.read()          # Omit the first 11 lines and the last 4 lines, then replace -, _, and #update with spaces.         texts.append((file[11:-4].replace('-',' ').replace('_', ' ').replace('#update',''), text))  # Create a dataframe from the list of texts df = pd.DataFrame(texts, columns = ['fname', 'text'])  # Set the text column to be the raw text with the newlines removed df['text'] = df.fname + \". \" + remove_newlines(df.text) df.to_csv('processed/scraped.csv') df.head() In\u00a0[26]: Copied! <pre>import tiktoken\n\n# Load the cl100k_base tokenizer which is designed to work with the ada-002 model\ntokenizer = tiktoken.get_encoding(\"cl100k_base\")\n\ndf = pd.read_csv('processed/scraped.csv', index_col=0)\ndf.columns = ['title', 'text']\n\n# Tokenize the text and save the number of tokens to a new column\ndf['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n\n# Visualize the distribution of the number of tokens per row using a histogram\ndf.n_tokens.hist()\n</pre> import tiktoken  # Load the cl100k_base tokenizer which is designed to work with the ada-002 model tokenizer = tiktoken.get_encoding(\"cl100k_base\")  df = pd.read_csv('processed/scraped.csv', index_col=0) df.columns = ['title', 'text']  # Tokenize the text and save the number of tokens to a new column df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))  # Visualize the distribution of the number of tokens per row using a histogram df.n_tokens.hist() Out[26]: <pre>&lt;AxesSubplot: &gt;</pre> In\u00a0[27]: Copied! <pre>max_tokens = 500\n\n# Function to split the text into chunks of a maximum number of tokens\ndef split_into_many(text, max_tokens = max_tokens):\n\n    # Split the text into sentences\n    sentences = text.split('. ')\n\n    # Get the number of tokens for each sentence\n    n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]\n    \n    chunks = []\n    tokens_so_far = 0\n    chunk = []\n\n    # Loop through the sentences and tokens joined together in a tuple\n    for sentence, token in zip(sentences, n_tokens):\n\n        # If the number of tokens so far plus the number of tokens in the current sentence is greater \n        # than the max number of tokens, then add the chunk to the list of chunks and reset\n        # the chunk and tokens so far\n        if tokens_so_far + token &gt; max_tokens:\n            chunks.append(\". \".join(chunk) + \".\")\n            chunk = []\n            tokens_so_far = 0\n\n        # If the number of tokens in the current sentence is greater than the max number of \n        # tokens, go to the next sentence\n        if token &gt; max_tokens:\n            continue\n\n        # Otherwise, add the sentence to the chunk and add the number of tokens to the total\n        chunk.append(sentence)\n        tokens_so_far += token + 1\n\n    return chunks\n    \n\nshortened = []\n\n# Loop through the dataframe\nfor row in df.iterrows():\n\n    # If the text is None, go to the next row\n    if row[1]['text'] is None:\n        continue\n\n    # If the number of tokens is greater than the max number of tokens, split the text into chunks\n    if row[1]['n_tokens'] &gt; max_tokens:\n        shortened += split_into_many(row[1]['text'])\n    \n    # Otherwise, add the text to the list of shortened texts\n    else:\n        shortened.append( row[1]['text'] )\n</pre> max_tokens = 500  # Function to split the text into chunks of a maximum number of tokens def split_into_many(text, max_tokens = max_tokens):      # Split the text into sentences     sentences = text.split('. ')      # Get the number of tokens for each sentence     n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]          chunks = []     tokens_so_far = 0     chunk = []      # Loop through the sentences and tokens joined together in a tuple     for sentence, token in zip(sentences, n_tokens):          # If the number of tokens so far plus the number of tokens in the current sentence is greater          # than the max number of tokens, then add the chunk to the list of chunks and reset         # the chunk and tokens so far         if tokens_so_far + token &gt; max_tokens:             chunks.append(\". \".join(chunk) + \".\")             chunk = []             tokens_so_far = 0          # If the number of tokens in the current sentence is greater than the max number of          # tokens, go to the next sentence         if token &gt; max_tokens:             continue          # Otherwise, add the sentence to the chunk and add the number of tokens to the total         chunk.append(sentence)         tokens_so_far += token + 1      return chunks       shortened = []  # Loop through the dataframe for row in df.iterrows():      # If the text is None, go to the next row     if row[1]['text'] is None:         continue      # If the number of tokens is greater than the max number of tokens, split the text into chunks     if row[1]['n_tokens'] &gt; max_tokens:         shortened += split_into_many(row[1]['text'])          # Otherwise, add the text to the list of shortened texts     else:         shortened.append( row[1]['text'] ) In\u00a0[28]: Copied! <pre>df = pd.DataFrame(shortened, columns = ['text'])\ndf['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\ndf.n_tokens.hist()\n</pre> df = pd.DataFrame(shortened, columns = ['text']) df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x))) df.n_tokens.hist() Out[28]: <pre>&lt;AxesSubplot: &gt;</pre> In\u00a0[29]: Copied! <pre>import openai\n\ndf['embeddings'] = df.text.apply(lambda x: openai.Embedding.create(input=x, engine='text-embedding-ada-002')['data'][0]['embedding'])\ndf.to_csv('processed/embeddings.csv')\ndf.head()\n</pre> import openai  df['embeddings'] = df.text.apply(lambda x: openai.Embedding.create(input=x, engine='text-embedding-ada-002')['data'][0]['embedding']) df.to_csv('processed/embeddings.csv') df.head() Out[29]: text n_tokens embeddings 0 blog authors maddie.   Maddie Hall - OpenAI   ... 175 [-0.012958061881363392, -0.006103983614593744,... 1 blog authors tom.   Tom Brown - OpenAI        ... 228 [-0.0053874170407652855, -0.009962032549083233... 2 blog openai scholars 2019 final projects.   Op... 492 [0.0019150723237544298, -0.0070442273281514645... 3 In this project, I used curiosity-driven explo... 478 [-0.0067560747265815735, 0.0004431474662851542... 4 Results revealed that the optimal RL policies ... 499 [-0.012868616729974747, 0.0029640409629791975,... In\u00a0[30]: Copied! <pre>from openai.embeddings_utils import distances_from_embeddings\n\ndf['embeddings'] = df.text.apply(lambda x: openai.Embedding.create(input=x, engine='text-embedding-ada-002')['data'][0]['embedding'])\n\ndf.to_csv('processed/embeddings.csv')\ndf.head()\n</pre> from openai.embeddings_utils import distances_from_embeddings  df['embeddings'] = df.text.apply(lambda x: openai.Embedding.create(input=x, engine='text-embedding-ada-002')['data'][0]['embedding'])  df.to_csv('processed/embeddings.csv') df.head() Out[30]: text n_tokens embeddings 0 blog authors maddie.   Maddie Hall - OpenAI   ... 175 [-0.012958061881363392, -0.006103983614593744,... 1 blog authors tom.   Tom Brown - OpenAI        ... 228 [-0.0053874170407652855, -0.009962032549083233... 2 blog openai scholars 2019 final projects.   Op... 492 [0.0019150723237544298, -0.0070442273281514645... 3 In this project, I used curiosity-driven explo... 478 [-0.0067560747265815735, 0.0004431474662851542... 4 Results revealed that the optimal RL policies ... 499 [-0.012868616729974747, 0.0029640409629791975,... In\u00a0[31]: Copied! <pre>import pandas as pd\nimport numpy as np\nfrom openai.embeddings_utils import distances_from_embeddings, cosine_similarity\n\ndf=pd.read_csv('processed/embeddings.csv', index_col=0)\ndf['embeddings'] = df['embeddings'].apply(eval).apply(np.array)\n\ndf.head()\n</pre> import pandas as pd import numpy as np from openai.embeddings_utils import distances_from_embeddings, cosine_similarity  df=pd.read_csv('processed/embeddings.csv', index_col=0) df['embeddings'] = df['embeddings'].apply(eval).apply(np.array)  df.head() Out[31]: text n_tokens embeddings 0 blog authors maddie.   Maddie Hall - OpenAI   ... 175 [-0.012958061881363392, -0.006103983614593744,... 1 blog authors tom.   Tom Brown - OpenAI        ... 228 [-0.0053874170407652855, -0.009962032549083233... 2 blog openai scholars 2019 final projects.   Op... 492 [0.0019150723237544298, -0.0070442273281514645... 3 In this project, I used curiosity-driven explo... 478 [-0.0067560747265815735, 0.0004431474662851542... 4 Results revealed that the optimal RL policies ... 499 [-0.012868616729974747, 0.0029640409629791975,... In\u00a0[32]: Copied! <pre>def create_context(\n    question, df, max_len=1800, size=\"ada\"\n):\n\"\"\"\n    Create a context for a question by finding the most similar context from the dataframe\n    \"\"\"\n\n    # Get the embeddings for the question\n    q_embeddings = openai.Embedding.create(input=question, engine='text-embedding-ada-002')['data'][0]['embedding']\n\n    # Get the distances from the embeddings\n    df['distances'] = distances_from_embeddings(q_embeddings, df['embeddings'].values, distance_metric='cosine')\n\n\n    returns = []\n    cur_len = 0\n\n    # Sort by distance and add the text to the context until the context is too long\n    for i, row in df.sort_values('distances', ascending=True).iterrows():\n        \n        # Add the length of the text to the current length\n        cur_len += row['n_tokens'] + 4\n        \n        # If the context is too long, break\n        if cur_len &gt; max_len:\n            break\n        \n        # Else add it to the text that is being returned\n        returns.append(row[\"text\"])\n\n    # Return the context\n    return \"\\n\\n###\\n\\n\".join(returns)\n\ndef answer_question(\n    df,\n    model=\"text-davinci-003\",\n    question=\"Am I allowed to publish model outputs to Twitter, without a human review?\",\n    max_len=1800,\n    size=\"ada\",\n    debug=False,\n    max_tokens=150,\n    stop_sequence=None\n):\n\"\"\"\n    Answer a question based on the most similar context from the dataframe texts\n    \"\"\"\n    context = create_context(\n        question,\n        df,\n        max_len=max_len,\n        size=size,\n    )\n    # If debug, print the raw model response\n    if debug:\n        print(\"Context:\\n\" + context)\n        print(\"\\n\\n\")\n\n    try:\n        # Create a completions using the question and context\n        response = openai.Completion.create(\n            prompt=f\"Answer the question based on the context below, and if the question can't be answered based on the context, say \\\"I don't know\\\"\\n\\nContext: {context}\\n\\n---\\n\\nQuestion: {question}\\nAnswer:\",\n            temperature=0,\n            max_tokens=max_tokens,\n            top_p=1,\n            frequency_penalty=0,\n            presence_penalty=0,\n            stop=stop_sequence,\n            model=model,\n        )\n        return response[\"choices\"][0][\"text\"].strip()\n    except Exception as e:\n        print(e)\n        return \"\"\n</pre> def create_context(     question, df, max_len=1800, size=\"ada\" ):     \"\"\"     Create a context for a question by finding the most similar context from the dataframe     \"\"\"      # Get the embeddings for the question     q_embeddings = openai.Embedding.create(input=question, engine='text-embedding-ada-002')['data'][0]['embedding']      # Get the distances from the embeddings     df['distances'] = distances_from_embeddings(q_embeddings, df['embeddings'].values, distance_metric='cosine')       returns = []     cur_len = 0      # Sort by distance and add the text to the context until the context is too long     for i, row in df.sort_values('distances', ascending=True).iterrows():                  # Add the length of the text to the current length         cur_len += row['n_tokens'] + 4                  # If the context is too long, break         if cur_len &gt; max_len:             break                  # Else add it to the text that is being returned         returns.append(row[\"text\"])      # Return the context     return \"\\n\\n###\\n\\n\".join(returns)  def answer_question(     df,     model=\"text-davinci-003\",     question=\"Am I allowed to publish model outputs to Twitter, without a human review?\",     max_len=1800,     size=\"ada\",     debug=False,     max_tokens=150,     stop_sequence=None ):     \"\"\"     Answer a question based on the most similar context from the dataframe texts     \"\"\"     context = create_context(         question,         df,         max_len=max_len,         size=size,     )     # If debug, print the raw model response     if debug:         print(\"Context:\\n\" + context)         print(\"\\n\\n\")      try:         # Create a completions using the question and context         response = openai.Completion.create(             prompt=f\"Answer the question based on the context below, and if the question can't be answered based on the context, say \\\"I don't know\\\"\\n\\nContext: {context}\\n\\n---\\n\\nQuestion: {question}\\nAnswer:\",             temperature=0,             max_tokens=max_tokens,             top_p=1,             frequency_penalty=0,             presence_penalty=0,             stop=stop_sequence,             model=model,         )         return response[\"choices\"][0][\"text\"].strip()     except Exception as e:         print(e)         return \"\" Out[32]: <pre>'No, you are not allowed to publish model outputs to Twitter without a human review. You must manually review each generation before sharing or while streaming, and indicate that the content is AI-generated in a way no user could reasonably miss or misunderstand.'</pre> In\u00a0[33]: Copied! <pre>answer_question(df, question=\"What day is it?\", debug=False)\n</pre> answer_question(df, question=\"What day is it?\", debug=False) Out[33]: <pre>\"I don't know.\"</pre> In\u00a0[34]: Copied! <pre>answer_question(df, question=\"What is our newest embeddings model?\")\n</pre> answer_question(df, question=\"What is our newest embeddings model?\") Out[34]: <pre>'The newest embeddings model is text-embedding-ada-002.'</pre>"},{"location":"apps/web-crawl-q-and-a/web-qa/","title":"Web qa","text":"In\u00a0[\u00a0]: Copied! <pre>%matplotlib inline\n</pre> %matplotlib inline In\u00a0[\u00a0]: Copied! <pre>import requests\nimport re\nimport urllib.request\nfrom bs4 import BeautifulSoup\nfrom collections import deque\nfrom html.parser import HTMLParser\nfrom urllib.parse import urlparse\nimport os\nimport pandas as pd\nimport tiktoken\nimport openai\nimport numpy as np\nfrom openai.embeddings_utils import distances_from_embeddings, cosine_similarity\n\n# Regex pattern to match a URL\nHTTP_URL_PATTERN = r'^http[s]*://.+'\n\n# Define root domain to crawl\ndomain = \"openai.com\"\nfull_url = \"https://openai.com/\"\n\n# Create a class to parse the HTML and get the hyperlinks\nclass HyperlinkParser(HTMLParser):\n    def __init__(self):\n        super().__init__()\n        # Create a list to store the hyperlinks\n        self.hyperlinks = []\n\n    # Override the HTMLParser's handle_starttag method to get the hyperlinks\n    def handle_starttag(self, tag, attrs):\n        attrs = dict(attrs)\n\n        # If the tag is an anchor tag and it has an href attribute, add the href attribute to the list of hyperlinks\n        if tag == \"a\" and \"href\" in attrs:\n            self.hyperlinks.append(attrs[\"href\"])\n</pre> import requests import re import urllib.request from bs4 import BeautifulSoup from collections import deque from html.parser import HTMLParser from urllib.parse import urlparse import os import pandas as pd import tiktoken import openai import numpy as np from openai.embeddings_utils import distances_from_embeddings, cosine_similarity  # Regex pattern to match a URL HTTP_URL_PATTERN = r'^http[s]*://.+'  # Define root domain to crawl domain = \"openai.com\" full_url = \"https://openai.com/\"  # Create a class to parse the HTML and get the hyperlinks class HyperlinkParser(HTMLParser):     def __init__(self):         super().__init__()         # Create a list to store the hyperlinks         self.hyperlinks = []      # Override the HTMLParser's handle_starttag method to get the hyperlinks     def handle_starttag(self, tag, attrs):         attrs = dict(attrs)          # If the tag is an anchor tag and it has an href attribute, add the href attribute to the list of hyperlinks         if tag == \"a\" and \"href\" in attrs:             self.hyperlinks.append(attrs[\"href\"]) In\u00a0[\u00a0]: Copied! <pre># Function to get the hyperlinks from a URL\ndef get_hyperlinks(url):\n    \n    # Try to open the URL and read the HTML\n    try:\n        # Open the URL and read the HTML\n        with urllib.request.urlopen(url) as response:\n\n            # If the response is not HTML, return an empty list\n            if not response.info().get('Content-Type').startswith(\"text/html\"):\n                return []\n            \n            # Decode the HTML\n            html = response.read().decode('utf-8')\n    except Exception as e:\n        print(e)\n        return []\n\n    # Create the HTML Parser and then Parse the HTML to get hyperlinks\n    parser = HyperlinkParser()\n    parser.feed(html)\n\n    return parser.hyperlinks\n</pre> # Function to get the hyperlinks from a URL def get_hyperlinks(url):          # Try to open the URL and read the HTML     try:         # Open the URL and read the HTML         with urllib.request.urlopen(url) as response:              # If the response is not HTML, return an empty list             if not response.info().get('Content-Type').startswith(\"text/html\"):                 return []                          # Decode the HTML             html = response.read().decode('utf-8')     except Exception as e:         print(e)         return []      # Create the HTML Parser and then Parse the HTML to get hyperlinks     parser = HyperlinkParser()     parser.feed(html)      return parser.hyperlinks In\u00a0[\u00a0]: Copied! <pre># Function to get the hyperlinks from a URL that are within the same domain\ndef get_domain_hyperlinks(local_domain, url):\n    clean_links = []\n    for link in set(get_hyperlinks(url)):\n        clean_link = None\n\n        # If the link is a URL, check if it is within the same domain\n        if re.search(HTTP_URL_PATTERN, link):\n            # Parse the URL and check if the domain is the same\n            url_obj = urlparse(link)\n            if url_obj.netloc == local_domain:\n                clean_link = link\n\n        # If the link is not a URL, check if it is a relative link\n        else:\n            if link.startswith(\"/\"):\n                link = link[1:]\n            elif link.startswith(\"#\") or link.startswith(\"mailto:\"):\n                continue\n            clean_link = \"https://\" + local_domain + \"/\" + link\n\n        if clean_link is not None:\n            if clean_link.endswith(\"/\"):\n                clean_link = clean_link[:-1]\n            clean_links.append(clean_link)\n\n    # Return the list of hyperlinks that are within the same domain\n    return list(set(clean_links))\n</pre> # Function to get the hyperlinks from a URL that are within the same domain def get_domain_hyperlinks(local_domain, url):     clean_links = []     for link in set(get_hyperlinks(url)):         clean_link = None          # If the link is a URL, check if it is within the same domain         if re.search(HTTP_URL_PATTERN, link):             # Parse the URL and check if the domain is the same             url_obj = urlparse(link)             if url_obj.netloc == local_domain:                 clean_link = link          # If the link is not a URL, check if it is a relative link         else:             if link.startswith(\"/\"):                 link = link[1:]             elif link.startswith(\"#\") or link.startswith(\"mailto:\"):                 continue             clean_link = \"https://\" + local_domain + \"/\" + link          if clean_link is not None:             if clean_link.endswith(\"/\"):                 clean_link = clean_link[:-1]             clean_links.append(clean_link)      # Return the list of hyperlinks that are within the same domain     return list(set(clean_links))  In\u00a0[\u00a0]: Copied! <pre>def crawl(url):\n    # Parse the URL and get the domain\n    local_domain = urlparse(url).netloc\n\n    # Create a queue to store the URLs to crawl\n    queue = deque([url])\n\n    # Create a set to store the URLs that have already been seen (no duplicates)\n    seen = set([url])\n\n    # Create a directory to store the text files\n    if not os.path.exists(\"text/\"):\n            os.mkdir(\"text/\")\n\n    if not os.path.exists(\"text/\"+local_domain+\"/\"):\n            os.mkdir(\"text/\" + local_domain + \"/\")\n\n    # Create a directory to store the csv files\n    if not os.path.exists(\"processed\"):\n            os.mkdir(\"processed\")\n\n    # While the queue is not empty, continue crawling\n    while queue:\n\n        # Get the next URL from the queue\n        url = queue.pop()\n        print(url) # for debugging and to see the progress\n\n        # Save text from the url to a &lt;url&gt;.txt file\n        with open('text/'+local_domain+'/'+url[8:].replace(\"/\", \"_\") + \".txt\", \"w\", encoding=\"UTF-8\") as f:\n\n            # Get the text from the URL using BeautifulSoup\n            soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n\n            # Get the text but remove the tags\n            text = soup.get_text()\n\n            # If the crawler gets to a page that requires JavaScript, it will stop the crawl\n            if (\"You need to enable JavaScript to run this app.\" in text):\n                print(\"Unable to parse page \" + url + \" due to JavaScript being required\")\n            \n            # Otherwise, write the text to the file in the text directory\n            f.write(text)\n\n        # Get the hyperlinks from the URL and add them to the queue\n        for link in get_domain_hyperlinks(local_domain, url):\n            if link not in seen:\n                queue.append(link)\n                seen.add(link)\n\ncrawl(full_url)\n</pre> def crawl(url):     # Parse the URL and get the domain     local_domain = urlparse(url).netloc      # Create a queue to store the URLs to crawl     queue = deque([url])      # Create a set to store the URLs that have already been seen (no duplicates)     seen = set([url])      # Create a directory to store the text files     if not os.path.exists(\"text/\"):             os.mkdir(\"text/\")      if not os.path.exists(\"text/\"+local_domain+\"/\"):             os.mkdir(\"text/\" + local_domain + \"/\")      # Create a directory to store the csv files     if not os.path.exists(\"processed\"):             os.mkdir(\"processed\")      # While the queue is not empty, continue crawling     while queue:          # Get the next URL from the queue         url = queue.pop()         print(url) # for debugging and to see the progress          # Save text from the url to a .txt file         with open('text/'+local_domain+'/'+url[8:].replace(\"/\", \"_\") + \".txt\", \"w\", encoding=\"UTF-8\") as f:              # Get the text from the URL using BeautifulSoup             soup = BeautifulSoup(requests.get(url).text, \"html.parser\")              # Get the text but remove the tags             text = soup.get_text()              # If the crawler gets to a page that requires JavaScript, it will stop the crawl             if (\"You need to enable JavaScript to run this app.\" in text):                 print(\"Unable to parse page \" + url + \" due to JavaScript being required\")                          # Otherwise, write the text to the file in the text directory             f.write(text)          # Get the hyperlinks from the URL and add them to the queue         for link in get_domain_hyperlinks(local_domain, url):             if link not in seen:                 queue.append(link)                 seen.add(link)  crawl(full_url) In\u00a0[\u00a0]: Copied! <pre>def remove_newlines(serie):\n    serie = serie.str.replace('\\n', ' ')\n    serie = serie.str.replace('\\\\n', ' ')\n    serie = serie.str.replace('  ', ' ')\n    serie = serie.str.replace('  ', ' ')\n    return serie\n</pre> def remove_newlines(serie):     serie = serie.str.replace('\\n', ' ')     serie = serie.str.replace('\\\\n', ' ')     serie = serie.str.replace('  ', ' ')     serie = serie.str.replace('  ', ' ')     return serie  In\u00a0[\u00a0]: Copied! <pre># Create a list to store the text files\ntexts=[]\n\n# Get all the text files in the text directory\nfor file in os.listdir(\"text/\" + domain + \"/\"):\n\n    # Open the file and read the text\n    with open(\"text/\" + domain + \"/\" + file, \"r\", encoding=\"UTF-8\") as f:\n        text = f.read()\n\n        # Omit the first 11 lines and the last 4 lines, then replace -, _, and #update with spaces.\n        texts.append((file[11:-4].replace('-',' ').replace('_', ' ').replace('#update',''), text))\n\n# Create a dataframe from the list of texts\ndf = pd.DataFrame(texts, columns = ['fname', 'text'])\n\n# Set the text column to be the raw text with the newlines removed\ndf['text'] = df.fname + \". \" + remove_newlines(df.text)\ndf.to_csv('processed/scraped.csv')\ndf.head()\n</pre> # Create a list to store the text files texts=[]  # Get all the text files in the text directory for file in os.listdir(\"text/\" + domain + \"/\"):      # Open the file and read the text     with open(\"text/\" + domain + \"/\" + file, \"r\", encoding=\"UTF-8\") as f:         text = f.read()          # Omit the first 11 lines and the last 4 lines, then replace -, _, and #update with spaces.         texts.append((file[11:-4].replace('-',' ').replace('_', ' ').replace('#update',''), text))  # Create a dataframe from the list of texts df = pd.DataFrame(texts, columns = ['fname', 'text'])  # Set the text column to be the raw text with the newlines removed df['text'] = df.fname + \". \" + remove_newlines(df.text) df.to_csv('processed/scraped.csv') df.head() In\u00a0[\u00a0]: Copied! <pre># Load the cl100k_base tokenizer which is designed to work with the ada-002 model\ntokenizer = tiktoken.get_encoding(\"cl100k_base\")\n\ndf = pd.read_csv('processed/scraped.csv', index_col=0)\ndf.columns = ['title', 'text']\n\n# Tokenize the text and save the number of tokens to a new column\ndf['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n\n# Visualize the distribution of the number of tokens per row using a histogram\ndf.n_tokens.hist()\n</pre> # Load the cl100k_base tokenizer which is designed to work with the ada-002 model tokenizer = tiktoken.get_encoding(\"cl100k_base\")  df = pd.read_csv('processed/scraped.csv', index_col=0) df.columns = ['title', 'text']  # Tokenize the text and save the number of tokens to a new column df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))  # Visualize the distribution of the number of tokens per row using a histogram df.n_tokens.hist() In\u00a0[\u00a0]: Copied! <pre>max_tokens = 500\n\n# Function to split the text into chunks of a maximum number of tokens\ndef split_into_many(text, max_tokens = max_tokens):\n\n    # Split the text into sentences\n    sentences = text.split('. ')\n\n    # Get the number of tokens for each sentence\n    n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]\n    \n    chunks = []\n    tokens_so_far = 0\n    chunk = []\n\n    # Loop through the sentences and tokens joined together in a tuple\n    for sentence, token in zip(sentences, n_tokens):\n\n        # If the number of tokens so far plus the number of tokens in the current sentence is greater \n        # than the max number of tokens, then add the chunk to the list of chunks and reset\n        # the chunk and tokens so far\n        if tokens_so_far + token &gt; max_tokens:\n            chunks.append(\". \".join(chunk) + \".\")\n            chunk = []\n            tokens_so_far = 0\n\n        # If the number of tokens in the current sentence is greater than the max number of \n        # tokens, go to the next sentence\n        if token &gt; max_tokens:\n            continue\n\n        # Otherwise, add the sentence to the chunk and add the number of tokens to the total\n        chunk.append(sentence)\n        tokens_so_far += token + 1\n        \n    # Add the last chunk to the list of chunks\n    if chunk:\n        chunks.append(\". \".join(chunk) + \".\")\n\n    return chunks\n    \n\nshortened = []\n\n# Loop through the dataframe\nfor row in df.iterrows():\n\n    # If the text is None, go to the next row\n    if row[1]['text'] is None:\n        continue\n\n    # If the number of tokens is greater than the max number of tokens, split the text into chunks\n    if row[1]['n_tokens'] &gt; max_tokens:\n        shortened += split_into_many(row[1]['text'])\n    \n    # Otherwise, add the text to the list of shortened texts\n    else:\n        shortened.append( row[1]['text'] )\n</pre> max_tokens = 500  # Function to split the text into chunks of a maximum number of tokens def split_into_many(text, max_tokens = max_tokens):      # Split the text into sentences     sentences = text.split('. ')      # Get the number of tokens for each sentence     n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]          chunks = []     tokens_so_far = 0     chunk = []      # Loop through the sentences and tokens joined together in a tuple     for sentence, token in zip(sentences, n_tokens):          # If the number of tokens so far plus the number of tokens in the current sentence is greater          # than the max number of tokens, then add the chunk to the list of chunks and reset         # the chunk and tokens so far         if tokens_so_far + token &gt; max_tokens:             chunks.append(\". \".join(chunk) + \".\")             chunk = []             tokens_so_far = 0          # If the number of tokens in the current sentence is greater than the max number of          # tokens, go to the next sentence         if token &gt; max_tokens:             continue          # Otherwise, add the sentence to the chunk and add the number of tokens to the total         chunk.append(sentence)         tokens_so_far += token + 1              # Add the last chunk to the list of chunks     if chunk:         chunks.append(\". \".join(chunk) + \".\")      return chunks       shortened = []  # Loop through the dataframe for row in df.iterrows():      # If the text is None, go to the next row     if row[1]['text'] is None:         continue      # If the number of tokens is greater than the max number of tokens, split the text into chunks     if row[1]['n_tokens'] &gt; max_tokens:         shortened += split_into_many(row[1]['text'])          # Otherwise, add the text to the list of shortened texts     else:         shortened.append( row[1]['text'] ) In\u00a0[\u00a0]: Copied! <pre>df = pd.DataFrame(shortened, columns = ['text'])\ndf['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\ndf.n_tokens.hist()\n</pre> df = pd.DataFrame(shortened, columns = ['text']) df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x))) df.n_tokens.hist() In\u00a0[\u00a0]: Copied! <pre># Note that you may run into rate limit issues depending on how many files you try to embed\n# Please check out our rate limit guide to learn more on how to handle this: https://platform.openai.com/docs/guides/rate-limits\n\ndf['embeddings'] = df.text.apply(lambda x: openai.Embedding.create(input=x, engine='text-embedding-ada-002')['data'][0]['embedding'])\ndf.to_csv('processed/embeddings.csv')\ndf.head()\n</pre> # Note that you may run into rate limit issues depending on how many files you try to embed # Please check out our rate limit guide to learn more on how to handle this: https://platform.openai.com/docs/guides/rate-limits  df['embeddings'] = df.text.apply(lambda x: openai.Embedding.create(input=x, engine='text-embedding-ada-002')['data'][0]['embedding']) df.to_csv('processed/embeddings.csv') df.head() In\u00a0[\u00a0]: Copied! <pre>df=pd.read_csv('processed/embeddings.csv', index_col=0)\ndf['embeddings'] = df['embeddings'].apply(eval).apply(np.array)\n\ndf.head()\n</pre> df=pd.read_csv('processed/embeddings.csv', index_col=0) df['embeddings'] = df['embeddings'].apply(eval).apply(np.array)  df.head() In\u00a0[\u00a0]: Copied! <pre>def create_context(\n    question, df, max_len=1800, size=\"ada\"\n):\n\"\"\"\n    Create a context for a question by finding the most similar context from the dataframe\n    \"\"\"\n\n    # Get the embeddings for the question\n    q_embeddings = openai.Embedding.create(input=question, engine='text-embedding-ada-002')['data'][0]['embedding']\n\n    # Get the distances from the embeddings\n    df['distances'] = distances_from_embeddings(q_embeddings, df['embeddings'].values, distance_metric='cosine')\n\n\n    returns = []\n    cur_len = 0\n\n    # Sort by distance and add the text to the context until the context is too long\n    for i, row in df.sort_values('distances', ascending=True).iterrows():\n        \n        # Add the length of the text to the current length\n        cur_len += row['n_tokens'] + 4\n        \n        # If the context is too long, break\n        if cur_len &gt; max_len:\n            break\n        \n        # Else add it to the text that is being returned\n        returns.append(row[\"text\"])\n\n    # Return the context\n    return \"\\n\\n###\\n\\n\".join(returns)\n\ndef answer_question(\n    df,\n    model=\"text-davinci-003\",\n    question=\"Am I allowed to publish model outputs to Twitter, without a human review?\",\n    max_len=1800,\n    size=\"ada\",\n    debug=False,\n    max_tokens=150,\n    stop_sequence=None\n):\n\"\"\"\n    Answer a question based on the most similar context from the dataframe texts\n    \"\"\"\n    context = create_context(\n        question,\n        df,\n        max_len=max_len,\n        size=size,\n    )\n    # If debug, print the raw model response\n    if debug:\n        print(\"Context:\\n\" + context)\n        print(\"\\n\\n\")\n\n    try:\n        # Create a completions using the questin and context\n        response = openai.Completion.create(\n            prompt=f\"Answer the question based on the context below, and if the question can't be answered based on the context, say \\\"I don't know\\\"\\n\\nContext: {context}\\n\\n---\\n\\nQuestion: {question}\\nAnswer:\",\n            temperature=0,\n            max_tokens=max_tokens,\n            top_p=1,\n            frequency_penalty=0,\n            presence_penalty=0,\n            stop=stop_sequence,\n            model=model,\n        )\n        return response[\"choices\"][0][\"text\"].strip()\n    except Exception as e:\n        print(e)\n        return \"\"\n</pre> def create_context(     question, df, max_len=1800, size=\"ada\" ):     \"\"\"     Create a context for a question by finding the most similar context from the dataframe     \"\"\"      # Get the embeddings for the question     q_embeddings = openai.Embedding.create(input=question, engine='text-embedding-ada-002')['data'][0]['embedding']      # Get the distances from the embeddings     df['distances'] = distances_from_embeddings(q_embeddings, df['embeddings'].values, distance_metric='cosine')       returns = []     cur_len = 0      # Sort by distance and add the text to the context until the context is too long     for i, row in df.sort_values('distances', ascending=True).iterrows():                  # Add the length of the text to the current length         cur_len += row['n_tokens'] + 4                  # If the context is too long, break         if cur_len &gt; max_len:             break                  # Else add it to the text that is being returned         returns.append(row[\"text\"])      # Return the context     return \"\\n\\n###\\n\\n\".join(returns)  def answer_question(     df,     model=\"text-davinci-003\",     question=\"Am I allowed to publish model outputs to Twitter, without a human review?\",     max_len=1800,     size=\"ada\",     debug=False,     max_tokens=150,     stop_sequence=None ):     \"\"\"     Answer a question based on the most similar context from the dataframe texts     \"\"\"     context = create_context(         question,         df,         max_len=max_len,         size=size,     )     # If debug, print the raw model response     if debug:         print(\"Context:\\n\" + context)         print(\"\\n\\n\")      try:         # Create a completions using the questin and context         response = openai.Completion.create(             prompt=f\"Answer the question based on the context below, and if the question can't be answered based on the context, say \\\"I don't know\\\"\\n\\nContext: {context}\\n\\n---\\n\\nQuestion: {question}\\nAnswer:\",             temperature=0,             max_tokens=max_tokens,             top_p=1,             frequency_penalty=0,             presence_penalty=0,             stop=stop_sequence,             model=model,         )         return response[\"choices\"][0][\"text\"].strip()     except Exception as e:         print(e)         return \"\" In\u00a0[\u00a0]: Copied! <pre>print(answer_question(df, question=\"What day is it?\", debug=False))\n\nprint(answer_question(df, question=\"What is our newest embeddings model?\"))\n</pre> print(answer_question(df, question=\"What day is it?\", debug=False))  print(answer_question(df, question=\"What is our newest embeddings model?\"))"},{"location":"apps/web-crawl-q-and-a/web-qa/#Step-1","title":"Step 1\u00b6","text":"<p>###############################################################################</p>"},{"location":"apps/web-crawl-q-and-a/web-qa/#Step-2","title":"Step 2\u00b6","text":"<p>###############################################################################</p>"},{"location":"apps/web-crawl-q-and-a/web-qa/#Step-3","title":"Step 3\u00b6","text":"<p>###############################################################################</p>"},{"location":"apps/web-crawl-q-and-a/web-qa/#Step-4","title":"Step 4\u00b6","text":"<p>###############################################################################</p>"},{"location":"apps/web-crawl-q-and-a/web-qa/#Step-5","title":"Step 5\u00b6","text":"<p>###############################################################################</p>"},{"location":"apps/web-crawl-q-and-a/web-qa/#Step-6","title":"Step 6\u00b6","text":"<p>###############################################################################</p>"},{"location":"apps/web-crawl-q-and-a/web-qa/#Step-7","title":"Step 7\u00b6","text":"<p>###############################################################################</p>"},{"location":"apps/web-crawl-q-and-a/web-qa/#Step-8","title":"Step 8\u00b6","text":"<p>###############################################################################</p>"},{"location":"apps/web-crawl-q-and-a/web-qa/#Step-9","title":"Step 9\u00b6","text":"<p>###############################################################################</p>"},{"location":"apps/web-crawl-q-and-a/web-qa/#Step-10","title":"Step 10\u00b6","text":"<p>###############################################################################</p>"},{"location":"apps/web-crawl-q-and-a/web-qa/#Step-11","title":"Step 11\u00b6","text":"<p>###############################################################################</p>"},{"location":"apps/web-crawl-q-and-a/web-qa/#Step-12","title":"Step 12\u00b6","text":"<p>###############################################################################</p>"},{"location":"apps/web-crawl-q-and-a/web-qa/#Step-13","title":"Step 13\u00b6","text":"<p>###############################################################################</p>"},{"location":"examples/Backtranslation_of_SQL_queries/","title":"Backtranslation of SQL queries","text":"In\u00a0[\u00a0]: Copied! <pre>from typing import List, Union\n</pre> from typing import List, Union In\u00a0[\u00a0]: Copied! <pre>from smokey import Smokey\n</pre> from smokey import Smokey In\u00a0[\u00a0]: Copied! <pre>import openai\n</pre> import openai In\u00a0[\u00a0]: Copied! <pre>def get_candidates(\n    prompt: str,\n    stop: List[str],\n    temperature: float,\n    priming_prefix: str,\n    engine: str,\n    n: int = 5,\n) -&gt; List[str]:\n\"\"\"\n    Generate N candidate completions based on the prompt, generated with a specific temperature.\n\n    :param prompt: The prompt to start the conversation with.\n    :param stop: A list of tokens that indicate the end of the generation.\n    :param temperature: The temperature of the generation.\n    :param priming_prefix: The prefix to use for the priming.\n    :param engine: The engine to use for the generation.\n    :param n: The number of completions to generate.\n    :return: A list of completions.\n    \"\"\"\n    response = openai.Completion.create(\n        engine=engine,\n        prompt=prompt,\n        temperature=temperature,\n        max_tokens=150,\n        top_p=1,\n        frequency_penalty=0,\n        presence_penalty=0,\n        stop=stop,\n        n=n,\n    )\n    responses = [priming_prefix + choice.text for choice in response.choices]\n    return responses\n</pre> def get_candidates(     prompt: str,     stop: List[str],     temperature: float,     priming_prefix: str,     engine: str,     n: int = 5, ) -&gt; List[str]:     \"\"\"     Generate N candidate completions based on the prompt, generated with a specific temperature.      :param prompt: The prompt to start the conversation with.     :param stop: A list of tokens that indicate the end of the generation.     :param temperature: The temperature of the generation.     :param priming_prefix: The prefix to use for the priming.     :param engine: The engine to use for the generation.     :param n: The number of completions to generate.     :return: A list of completions.     \"\"\"     response = openai.Completion.create(         engine=engine,         prompt=prompt,         temperature=temperature,         max_tokens=150,         top_p=1,         frequency_penalty=0,         presence_penalty=0,         stop=stop,         n=n,     )     responses = [priming_prefix + choice.text for choice in response.choices]     return responses In\u00a0[\u00a0]: Copied! <pre>def rindex(lst: List, value: str) -&gt; int:\n\"\"\"\n    Return the index of the last occurence of a value in a list.\n\n    :param lst: The list to search in.\n    :param value: The value to search for.\n    :return: The index of the last occurence of the value.\n    \"\"\"\n    try:\n        return len(lst) - lst[::-1].index(value) - 1\n    except ValueError:\n        raise ValueError(f\"Answer start token `{value}` not found in the eval template\")\n</pre> def rindex(lst: List, value: str) -&gt; int:     \"\"\"     Return the index of the last occurence of a value in a list.      :param lst: The list to search in.     :param value: The value to search for.     :return: The index of the last occurence of the value.     \"\"\"     try:         return len(lst) - lst[::-1].index(value) - 1     except ValueError:         raise ValueError(f\"Answer start token `{value}` not found in the eval template\") In\u00a0[\u00a0]: Copied! <pre>def eval_candidate(\n    candidate_answer: str,\n    original_instruction: str,\n    eval_template: str,\n    answer_start_token: str,\n    engine: str,\n) -&gt; float:\n\"\"\"\n    Evaluate a candidate answer by calculating the average log probability\n    of the original instruction, given the candidate answer with a specific\n    evaluation template, aimed at reconstructing the original instruction.\n\n    :param candidate_answer: The candidate answer to evaluate.\n    :param original_instruction: The original instruction.\n    :param eval_template: The template to use for the evaluation.\n    :param answer_start_token: The token to use to indicate the start of the answer.\n    :param engine: The engine to use for the evaluation.\n    :return: The evaluation of the candidate answer.\n    \"\"\"\n    response = openai.Completion.create(\n        engine=engine,\n        prompt=eval_template.format(candidate_answer, original_instruction),\n        temperature=0,\n        max_tokens=0,\n        top_p=1,\n        frequency_penalty=0,\n        presence_penalty=0,\n        logprobs=1,\n        echo=True,\n    )\n\n    answer_start = rindex(\n        response[\"choices\"][0][\"logprobs\"][\"tokens\"], answer_start_token\n    )\n    logprobs = response[\"choices\"][0][\"logprobs\"][\"token_logprobs\"][answer_start + 1 :]\n    return sum(logprobs) / len(logprobs)\n</pre> def eval_candidate(     candidate_answer: str,     original_instruction: str,     eval_template: str,     answer_start_token: str,     engine: str, ) -&gt; float:     \"\"\"     Evaluate a candidate answer by calculating the average log probability     of the original instruction, given the candidate answer with a specific     evaluation template, aimed at reconstructing the original instruction.      :param candidate_answer: The candidate answer to evaluate.     :param original_instruction: The original instruction.     :param eval_template: The template to use for the evaluation.     :param answer_start_token: The token to use to indicate the start of the answer.     :param engine: The engine to use for the evaluation.     :return: The evaluation of the candidate answer.     \"\"\"     response = openai.Completion.create(         engine=engine,         prompt=eval_template.format(candidate_answer, original_instruction),         temperature=0,         max_tokens=0,         top_p=1,         frequency_penalty=0,         presence_penalty=0,         logprobs=1,         echo=True,     )      answer_start = rindex(         response[\"choices\"][0][\"logprobs\"][\"tokens\"], answer_start_token     )     logprobs = response[\"choices\"][0][\"logprobs\"][\"token_logprobs\"][answer_start + 1 :]     return sum(logprobs) / len(logprobs) In\u00a0[\u00a0]: Copied! <pre>def backtranslation(\n    prompt_template: str,\n    additional_info: str,\n    instruction: str,\n    eval_template: str,\n    priming_prefix: str = \"SELECT\",\n    stop1: List[str] = [\"#\", \";\"],\n    answer_start_token: str = \"--\",\n    n: int = 5,\n    temperature: float = 0.5,\n    return_all_results: bool = False,\n    engine: str = \"davinci-codex\",\n) -&gt; Union[str, List[str, float]]:\n\"\"\"\n    Generate a number of SQL queries given a natural language instruction,\n    and pick the best one based on the average log probability of explaining the\n    candidate SQL query with the exact original instruction, when prompted for\n    a natural language explanation of the candidate SQL query.\n\n    :param prompt_template: The template to use for the prompt to generate SQL.\n    :param additional_info: Additional information to include in the prompt\n                            (SQL Tables, and their properties).\n    :param instruction: The instruction in natural language.\n    :param eval_template: The template to use for the evaluation.\n    :param priming_prefix: The prefix to use for the priming of the SQL query.\n    :param stop1: A list of tokens that indicate the end of the generation.\n    :param answer_start_token: The token to use to indicate the start of the\n                               natural answer.\n    :param n: The number of candidates to generate.\n    :param temperature: The temperature of the generation.\n    :param return_all_results: Whether to return all results or just the best one.\n    :param engine: The engine to use for the generation and evaluation.\n    :return: The best SQL query, or a list of all scored generated SQL queries.\n    \"\"\"\n    prompt_template = prompt_template.format(\n        additional_info, instruction, priming_prefix\n    )\n\n    candidates = []\n    responses = get_candidates(\n        prompt_template, stop1, temperature, priming_prefix, engine=engine, n=n\n    )\n    for i in range(n):\n        quality = eval_candidate(\n            responses[i],\n            instruction,\n            eval_template,\n            answer_start_token,\n            engine=engine,\n        )\n        candidates.append((responses[i], quality))\n\n    candidates.sort(key=lambda x: x[1], reverse=True)\n    if return_all_results:\n        return candidates\n    return candidates[0][0]\n</pre> def backtranslation(     prompt_template: str,     additional_info: str,     instruction: str,     eval_template: str,     priming_prefix: str = \"SELECT\",     stop1: List[str] = [\"#\", \";\"],     answer_start_token: str = \"--\",     n: int = 5,     temperature: float = 0.5,     return_all_results: bool = False,     engine: str = \"davinci-codex\", ) -&gt; Union[str, List[str, float]]:     \"\"\"     Generate a number of SQL queries given a natural language instruction,     and pick the best one based on the average log probability of explaining the     candidate SQL query with the exact original instruction, when prompted for     a natural language explanation of the candidate SQL query.      :param prompt_template: The template to use for the prompt to generate SQL.     :param additional_info: Additional information to include in the prompt                             (SQL Tables, and their properties).     :param instruction: The instruction in natural language.     :param eval_template: The template to use for the evaluation.     :param priming_prefix: The prefix to use for the priming of the SQL query.     :param stop1: A list of tokens that indicate the end of the generation.     :param answer_start_token: The token to use to indicate the start of the                                natural answer.     :param n: The number of candidates to generate.     :param temperature: The temperature of the generation.     :param return_all_results: Whether to return all results or just the best one.     :param engine: The engine to use for the generation and evaluation.     :return: The best SQL query, or a list of all scored generated SQL queries.     \"\"\"     prompt_template = prompt_template.format(         additional_info, instruction, priming_prefix     )      candidates = []     responses = get_candidates(         prompt_template, stop1, temperature, priming_prefix, engine=engine, n=n     )     for i in range(n):         quality = eval_candidate(             responses[i],             instruction,             eval_template,             answer_start_token,             engine=engine,         )         candidates.append((responses[i], quality))      candidates.sort(key=lambda x: x[1], reverse=True)     if return_all_results:         return candidates     return candidates[0][0] In\u00a0[\u00a0]: Copied! <pre>def main(\n    nl_query: str = \"Return the name of each department that had more than 10 employees in June 2021\",\n    eval_template: str = \"{};\\n-- Explanation of the above query in human readable format\\n-- {}\",\n    table_definitions: str = \"# Employee(id, name, department_id)\\n# Department(id, name, address)\\n# Salary_Payments(id, employee_id, amount, date)\\n\",\n    prompt_template: str = \"### Postgres SQL tables, with their properties:\\n#\\n{}#\\n### {}\\n{}\",\n    n: int = 3,\n    temperature: float = 0.3,\n    engine: str = \"davinci-codex\",\n):\n\"\"\"\n    Generate a number of SQL queries given a natural language instruction,\n    and pick the best one based on the highest backtranslation score.\n\n    :param nl_query: The natural language query.\n    :param eval_template: The template to use for the evaluation.\n    :param table_definitions: The definitions of the tables used in the query.\n    :param prompt_template: The template to use for the prompt to generate SQL.\n    :param n: The number of candidates to generate.\n    :param temperature: The temperature of the generation.\n    :param engine: The engine to use for the generation and evaluation.\n    :return: The best SQL query, or a list of all scored generated SQL queries.\n    \"\"\"\n\n    result = backtranslation(\n        prompt_template,\n        table_definitions,\n        nl_query,\n        eval_template,\n        priming_prefix=\"SELECT\",\n        temperature=temperature,\n        n=n,\n        engine=engine,\n    )\n    print(result)\n</pre> def main(     nl_query: str = \"Return the name of each department that had more than 10 employees in June 2021\",     eval_template: str = \"{};\\n-- Explanation of the above query in human readable format\\n-- {}\",     table_definitions: str = \"# Employee(id, name, department_id)\\n# Department(id, name, address)\\n# Salary_Payments(id, employee_id, amount, date)\\n\",     prompt_template: str = \"### Postgres SQL tables, with their properties:\\n#\\n{}#\\n### {}\\n{}\",     n: int = 3,     temperature: float = 0.3,     engine: str = \"davinci-codex\", ):     \"\"\"     Generate a number of SQL queries given a natural language instruction,     and pick the best one based on the highest backtranslation score.      :param nl_query: The natural language query.     :param eval_template: The template to use for the evaluation.     :param table_definitions: The definitions of the tables used in the query.     :param prompt_template: The template to use for the prompt to generate SQL.     :param n: The number of candidates to generate.     :param temperature: The temperature of the generation.     :param engine: The engine to use for the generation and evaluation.     :return: The best SQL query, or a list of all scored generated SQL queries.     \"\"\"      result = backtranslation(         prompt_template,         table_definitions,         nl_query,         eval_template,         priming_prefix=\"SELECT\",         temperature=temperature,         n=n,         engine=engine,     )     print(result) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    Smokey(main)\n</pre> if __name__ == \"__main__\":     Smokey(main)"},{"location":"examples/Classification_using_embeddings/","title":"Classification using embeddings","text":"In\u00a0[1]: Copied! <pre># imports\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# load data\ndatafile_path = \"data/fine_food_reviews_with_embeddings_1k.csv\"\n\ndf = pd.read_csv(datafile_path)\ndf[\"embedding\"] = df.embedding.apply(eval).apply(np.array)  # convert string to array\n\n# split data into train and test\nX_train, X_test, y_train, y_test = train_test_split(\n    list(df.embedding.values), df.Score, test_size=0.2, random_state=42\n)\n\n# train random forest classifier\nclf = RandomForestClassifier(n_estimators=100)\nclf.fit(X_train, y_train)\npreds = clf.predict(X_test)\nprobas = clf.predict_proba(X_test)\n\nreport = classification_report(y_test, preds)\nprint(report)\n</pre> # imports import pandas as pd import numpy as np  from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report, accuracy_score  # load data datafile_path = \"data/fine_food_reviews_with_embeddings_1k.csv\"  df = pd.read_csv(datafile_path) df[\"embedding\"] = df.embedding.apply(eval).apply(np.array)  # convert string to array  # split data into train and test X_train, X_test, y_train, y_test = train_test_split(     list(df.embedding.values), df.Score, test_size=0.2, random_state=42 )  # train random forest classifier clf = RandomForestClassifier(n_estimators=100) clf.fit(X_train, y_train) preds = clf.predict(X_test) probas = clf.predict_proba(X_test)  report = classification_report(y_test, preds) print(report)  <pre>              precision    recall  f1-score   support\n\n           1       0.88      0.35      0.50        20\n           2       1.00      0.38      0.55         8\n           3       1.00      0.18      0.31        11\n           4       1.00      0.26      0.41        27\n           5       0.74      1.00      0.85       134\n\n    accuracy                           0.77       200\n   macro avg       0.92      0.43      0.52       200\nweighted avg       0.82      0.77      0.72       200\n\n</pre> <p>We can see that the model has learnt to distinguish between the categories decently. 5-star reviews show the best performance overall, and this is not too surprising, since they are the most common in the dataset.</p> In\u00a0[2]: Copied! <pre>from openai.embeddings_utils import plot_multiclass_precision_recall\n\nplot_multiclass_precision_recall(probas, y_test, [1, 2, 3, 4, 5], clf)\n</pre> from openai.embeddings_utils import plot_multiclass_precision_recall  plot_multiclass_precision_recall(probas, y_test, [1, 2, 3, 4, 5], clf)  <pre>RandomForestClassifier() - Average precision score over all classes: 0.87\n</pre> <p>Unsurprisingly 5-star and 1-star reviews seem to be easier to predict. Perhaps with more data, the nuances between 2-4 stars could be better predicted, but there's also probably more subjectivity in how people use the inbetween scores.</p>"},{"location":"examples/Classification_using_embeddings/#Classification-using-embeddings","title":"Classification using embeddings\u00b6","text":"<p>There are many ways to classify text. This notebook shares an example of text classification using embeddings. For many text classification tasks, we've seen fine-tuned models do better than embeddings. See an example of fine-tuned models for classification in Fine-tuned_classification.ipynb. We also recommend having more examples than embedding dimensions, which we don't quite achieve here.</p> <p>In this text classification task, we predict the score of a food review (1 to 5) based on the embedding of the review's text. We split the dataset into a training and a testing set for all the following tasks, so we can realistically evaluate performance on unseen data. The dataset is created in the Obtain_dataset Notebook.</p>"},{"location":"examples/Clustering/","title":"Clustering","text":"In\u00a0[1]: Copied! <pre># imports\nimport numpy as np\nimport pandas as pd\n\n# load data\ndatafile_path = \"./data/fine_food_reviews_with_embeddings_1k.csv\"\n\ndf = pd.read_csv(datafile_path)\ndf[\"embedding\"] = df.embedding.apply(eval).apply(np.array)  # convert string to numpy array\nmatrix = np.vstack(df.embedding.values)\nmatrix.shape\n</pre> # imports import numpy as np import pandas as pd  # load data datafile_path = \"./data/fine_food_reviews_with_embeddings_1k.csv\"  df = pd.read_csv(datafile_path) df[\"embedding\"] = df.embedding.apply(eval).apply(np.array)  # convert string to numpy array matrix = np.vstack(df.embedding.values) matrix.shape  Out[1]: <pre>(1000, 1536)</pre> <p>We show the simplest use of K-means. You can pick the number of clusters that fits your use case best.</p> In\u00a0[2]: Copied! <pre>from sklearn.cluster import KMeans\n\nn_clusters = 4\n\nkmeans = KMeans(n_clusters=n_clusters, init=\"k-means++\", random_state=42)\nkmeans.fit(matrix)\nlabels = kmeans.labels_\ndf[\"Cluster\"] = labels\n\ndf.groupby(\"Cluster\").Score.mean().sort_values()\n</pre> from sklearn.cluster import KMeans  n_clusters = 4  kmeans = KMeans(n_clusters=n_clusters, init=\"k-means++\", random_state=42) kmeans.fit(matrix) labels = kmeans.labels_ df[\"Cluster\"] = labels  df.groupby(\"Cluster\").Score.mean().sort_values()  <pre>/Users/ted/.virtualenvs/openai/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n</pre> Out[2]: <pre>Cluster\n0    4.105691\n1    4.191176\n2    4.215613\n3    4.306590\nName: Score, dtype: float64</pre> In\u00a0[3]: Copied! <pre>from sklearn.manifold import TSNE\nimport matplotlib\nimport matplotlib.pyplot as plt\n\ntsne = TSNE(n_components=2, perplexity=15, random_state=42, init=\"random\", learning_rate=200)\nvis_dims2 = tsne.fit_transform(matrix)\n\nx = [x for x, y in vis_dims2]\ny = [y for x, y in vis_dims2]\n\nfor category, color in enumerate([\"purple\", \"green\", \"red\", \"blue\"]):\n    xs = np.array(x)[df.Cluster == category]\n    ys = np.array(y)[df.Cluster == category]\n    plt.scatter(xs, ys, color=color, alpha=0.3)\n\n    avg_x = xs.mean()\n    avg_y = ys.mean()\n\n    plt.scatter(avg_x, avg_y, marker=\"x\", color=color, s=100)\nplt.title(\"Clusters identified visualized in language 2d using t-SNE\")\n</pre> from sklearn.manifold import TSNE import matplotlib import matplotlib.pyplot as plt  tsne = TSNE(n_components=2, perplexity=15, random_state=42, init=\"random\", learning_rate=200) vis_dims2 = tsne.fit_transform(matrix)  x = [x for x, y in vis_dims2] y = [y for x, y in vis_dims2]  for category, color in enumerate([\"purple\", \"green\", \"red\", \"blue\"]):     xs = np.array(x)[df.Cluster == category]     ys = np.array(y)[df.Cluster == category]     plt.scatter(xs, ys, color=color, alpha=0.3)      avg_x = xs.mean()     avg_y = ys.mean()      plt.scatter(avg_x, avg_y, marker=\"x\", color=color, s=100) plt.title(\"Clusters identified visualized in language 2d using t-SNE\")  Out[3]: <pre>Text(0.5, 1.0, 'Clusters identified visualized in language 2d using t-SNE')</pre> <p>Visualization of clusters in a 2d projection. In this run, the green cluster (#1) seems quite different from the others. Let's see a few samples from each cluster.</p> In\u00a0[4]: Copied! <pre>import openai\n\n# Reading a review which belong to each group.\nrev_per_cluster = 5\n\nfor i in range(n_clusters):\n    print(f\"Cluster {i} Theme:\", end=\" \")\n\n    reviews = \"\\n\".join(\n        df[df.Cluster == i]\n        .combined.str.replace(\"Title: \", \"\")\n        .str.replace(\"\\n\\nContent: \", \":  \")\n        .sample(rev_per_cluster, random_state=42)\n        .values\n    )\n    response = openai.Completion.create(\n        engine=\"text-davinci-003\",\n        prompt=f'What do the following customer reviews have in common?\\n\\nCustomer reviews:\\n\"\"\"\\n{reviews}\\n\"\"\"\\n\\nTheme:',\n        temperature=0,\n        max_tokens=64,\n        top_p=1,\n        frequency_penalty=0,\n        presence_penalty=0,\n    )\n    print(response[\"choices\"][0][\"text\"].replace(\"\\n\", \"\"))\n\n    sample_cluster_rows = df[df.Cluster == i].sample(rev_per_cluster, random_state=42)\n    for j in range(rev_per_cluster):\n        print(sample_cluster_rows.Score.values[j], end=\", \")\n        print(sample_cluster_rows.Summary.values[j], end=\":   \")\n        print(sample_cluster_rows.Text.str[:70].values[j])\n\n    print(\"-\" * 100)\n</pre> import openai  # Reading a review which belong to each group. rev_per_cluster = 5  for i in range(n_clusters):     print(f\"Cluster {i} Theme:\", end=\" \")      reviews = \"\\n\".join(         df[df.Cluster == i]         .combined.str.replace(\"Title: \", \"\")         .str.replace(\"\\n\\nContent: \", \":  \")         .sample(rev_per_cluster, random_state=42)         .values     )     response = openai.Completion.create(         engine=\"text-davinci-003\",         prompt=f'What do the following customer reviews have in common?\\n\\nCustomer reviews:\\n\"\"\"\\n{reviews}\\n\"\"\"\\n\\nTheme:',         temperature=0,         max_tokens=64,         top_p=1,         frequency_penalty=0,         presence_penalty=0,     )     print(response[\"choices\"][0][\"text\"].replace(\"\\n\", \"\"))      sample_cluster_rows = df[df.Cluster == i].sample(rev_per_cluster, random_state=42)     for j in range(rev_per_cluster):         print(sample_cluster_rows.Score.values[j], end=\", \")         print(sample_cluster_rows.Summary.values[j], end=\":   \")         print(sample_cluster_rows.Text.str[:70].values[j])      print(\"-\" * 100)  <pre>Cluster 0 Theme:  All of the reviews are positive and the customers are satisfied with the product they purchased.\n5, Loved these gluten free healthy bars, saved $$ ordering on Amazon:   These Kind Bars are so good and healthy &amp; gluten free.  My daughter ca\n1, Should advertise coconut as an ingredient more prominently:   First, these should be called Mac - Coconut bars, as Coconut is the #2\n5, very good!!:   just like the runts&lt;br /&gt;great flavor, def worth getting&lt;br /&gt;I even o\n5, Excellent product:   After scouring every store in town for orange peels and not finding an\n5, delicious:   Gummi Frogs have been my favourite candy that I have ever tried. of co\n----------------------------------------------------------------------------------------------------\nCluster 1 Theme:  All of the reviews are about pet food.\n2, Messy and apparently undelicious:   My cat is not a huge fan. Sure, she'll lap up the gravy, but leaves th\n4, The cats like it:   My 7 cats like this food but it is a little yucky for the human. Piece\n5, cant get enough of it!!!:   Our lil shih tzu puppy cannot get enough of it. Everytime she sees the\n1, Food Caused Illness:   I switched my cats over from the Blue Buffalo Wildnerness Food to this\n5, My furbabies LOVE these!:   Shake the container and they come running. Even my boy cat, who isn't \n----------------------------------------------------------------------------------------------------\nCluster 2 Theme:  All of the reviews are positive and express satisfaction with the product.\n5, Fog Chaser Coffee:   This coffee has a full body and a rich taste. The price is far below t\n5, Excellent taste:   This is to me a great coffee, once you try it you will enjoy it, this \n4, Good, but not Wolfgang Puck good:   Honestly, I have to admit that I expected a little better. That's not \n5, Just My Kind of Coffee:   Coffee Masters Hazelnut coffee used to be carried in a local coffee/pa\n5, Rodeo Drive is Crazy Good Coffee!:   Rodeo Drive is my absolute favorite and I'm ready to order more!  That\n----------------------------------------------------------------------------------------------------\nCluster 3 Theme:  All of the reviews are about food or drink products.\n5, Wonderful alternative to soda pop:   This is a wonderful alternative to soda pop.  It's carbonated for thos\n5, So convenient, for so little!:   I needed two vanilla beans for the Love Goddess cake that my husbands \n2, bot very cheesy:   Got this about a month ago.first of all it smells horrible...it tastes\n5, Delicious!:   I am not a huge beer lover.  I do enjoy an occasional Blue Moon (all o\n3, Just ok:   I bought this brand because it was all they had at Ranch 99 near us. I\n----------------------------------------------------------------------------------------------------\n</pre> <p>It's important to note that clusters will not necessarily match what you intend to use them for. A larger amount of clusters will focus on more specific patterns, whereas a small number of clusters will usually focus on largest discrepencies in the data.</p>"},{"location":"examples/Clustering/#Clustering","title":"Clustering\u00b6","text":"<p>We use a simple k-means algorithm to demonstrate how clustering can be done. Clustering can help discover valuable, hidden groupings within the data. The dataset is created in the Obtain_dataset Notebook.</p>"},{"location":"examples/Clustering/#1.-Find-the-clusters-using-K-means","title":"1. Find the clusters using K-means\u00b6","text":""},{"location":"examples/Clustering/#2.-Text-samples-in-the-clusters-&-naming-the-clusters","title":"2. Text samples in the clusters &amp; naming the clusters\u00b6","text":"<p>Let's show random samples from each cluster. We'll use text-davinci-003 to name the clusters, based on a random sample of 5 reviews from that cluster.</p>"},{"location":"examples/Clustering_for_transaction_classification/","title":"Clustering for Transaction Classification","text":"In\u00a0[1]: Copied! <pre># imports\nimport openai\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.manifold import TSNE\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport os\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\nCOMPLETIONS_MODEL = \"text-davinci-003\"\n\n# This path leads to a file with data and precomputed embeddings\nembedding_path = \"data/library_transactions_with_embeddings_359.csv\"\n</pre> # imports import openai import pandas as pd import numpy as np from sklearn.cluster import KMeans from sklearn.manifold import TSNE import matplotlib import matplotlib.pyplot as plt import os  openai.api_key = os.getenv(\"OPENAI_API_KEY\") COMPLETIONS_MODEL = \"text-davinci-003\"  # This path leads to a file with data and precomputed embeddings embedding_path = \"data/library_transactions_with_embeddings_359.csv\"  In\u00a0[2]: Copied! <pre>df = pd.read_csv(embedding_path)\n</pre> df = pd.read_csv(embedding_path) In\u00a0[3]: Copied! <pre>embedding_df = pd.read_csv(embedding_path)\nembedding_df[\"embedding\"] = embedding_df.embedding.apply(eval).apply(np.array)\nmatrix = np.vstack(embedding_df.embedding.values)\nmatrix.shape\n</pre> embedding_df = pd.read_csv(embedding_path) embedding_df[\"embedding\"] = embedding_df.embedding.apply(eval).apply(np.array) matrix = np.vstack(embedding_df.embedding.values) matrix.shape Out[3]: <pre>(359, 1536)</pre> In\u00a0[4]: Copied! <pre>n_clusters = 5\n\nkmeans = KMeans(n_clusters=n_clusters, init=\"k-means++\", random_state=42)\nkmeans.fit(matrix)\nlabels = kmeans.labels_\nembedding_df[\"Cluster\"] = labels\n</pre> n_clusters = 5  kmeans = KMeans(n_clusters=n_clusters, init=\"k-means++\", random_state=42) kmeans.fit(matrix) labels = kmeans.labels_ embedding_df[\"Cluster\"] = labels <pre>/Users/ted/.virtualenvs/openai/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n</pre> In\u00a0[5]: Copied! <pre>tsne = TSNE(\n    n_components=2, perplexity=15, random_state=42, init=\"random\", learning_rate=200\n)\nvis_dims2 = tsne.fit_transform(matrix)\n\nx = [x for x, y in vis_dims2]\ny = [y for x, y in vis_dims2]\n\nfor category, color in enumerate([\"purple\", \"green\", \"red\", \"blue\",\"yellow\"]):\n    xs = np.array(x)[embedding_df.Cluster == category]\n    ys = np.array(y)[embedding_df.Cluster == category]\n    plt.scatter(xs, ys, color=color, alpha=0.3)\n\n    avg_x = xs.mean()\n    avg_y = ys.mean()\n\n    plt.scatter(avg_x, avg_y, marker=\"x\", color=color, s=100)\nplt.title(\"Clusters identified visualized in language 2d using t-SNE\")\n</pre> tsne = TSNE(     n_components=2, perplexity=15, random_state=42, init=\"random\", learning_rate=200 ) vis_dims2 = tsne.fit_transform(matrix)  x = [x for x, y in vis_dims2] y = [y for x, y in vis_dims2]  for category, color in enumerate([\"purple\", \"green\", \"red\", \"blue\",\"yellow\"]):     xs = np.array(x)[embedding_df.Cluster == category]     ys = np.array(y)[embedding_df.Cluster == category]     plt.scatter(xs, ys, color=color, alpha=0.3)      avg_x = xs.mean()     avg_y = ys.mean()      plt.scatter(avg_x, avg_y, marker=\"x\", color=color, s=100) plt.title(\"Clusters identified visualized in language 2d using t-SNE\")  Out[5]: <pre>Text(0.5, 1.0, 'Clusters identified visualized in language 2d using t-SNE')</pre> In\u00a0[6]: Copied! <pre># We'll read 10 transactions per cluster as we're expecting some variation\ntransactions_per_cluster = 10\n\nfor i in range(n_clusters):\n    print(f\"Cluster {i} Theme:\", end=\" \")\n\n    transactions = \"\\n\".join(\n        embedding_df[embedding_df.Cluster == i]\n        .combined.str.replace(\"Supplier: \", \"\")\n        .str.replace(\"Description: \", \":  \")\n        .str.replace(\"Value: \", \":  \")\n        .sample(transactions_per_cluster, random_state=42)\n        .values\n    )\n    response = openai.Completion.create(\n        model=COMPLETIONS_MODEL,\n        # We'll include a prompt to instruct the model what sort of description we're looking for\n        prompt=f'''We want to group these transactions into meaningful clusters so we can target the areas we are spending the most money. \n                What do the following transactions have in common?\\n\\nTransactions:\\n\"\"\"\\n{transactions}\\n\"\"\"\\n\\nTheme:''',\n        temperature=0,\n        max_tokens=100,\n        top_p=1,\n        frequency_penalty=0,\n        presence_penalty=0,\n    )\n    print(response[\"choices\"][0][\"text\"].replace(\"\\n\", \"\"))\n\n    sample_cluster_rows = embedding_df[embedding_df.Cluster == i].sample(transactions_per_cluster, random_state=42)\n    for j in range(transactions_per_cluster):\n        print(sample_cluster_rows.Supplier.values[j], end=\", \")\n        print(sample_cluster_rows.Description.values[j], end=\"\\n   \")\n        #print(str(sample_cluster_rows.Value).values[j])\n\n    print(\"-\" * 100)\n</pre> # We'll read 10 transactions per cluster as we're expecting some variation transactions_per_cluster = 10  for i in range(n_clusters):     print(f\"Cluster {i} Theme:\", end=\" \")      transactions = \"\\n\".join(         embedding_df[embedding_df.Cluster == i]         .combined.str.replace(\"Supplier: \", \"\")         .str.replace(\"Description: \", \":  \")         .str.replace(\"Value: \", \":  \")         .sample(transactions_per_cluster, random_state=42)         .values     )     response = openai.Completion.create(         model=COMPLETIONS_MODEL,         # We'll include a prompt to instruct the model what sort of description we're looking for         prompt=f'''We want to group these transactions into meaningful clusters so we can target the areas we are spending the most money.                  What do the following transactions have in common?\\n\\nTransactions:\\n\"\"\"\\n{transactions}\\n\"\"\"\\n\\nTheme:''',         temperature=0,         max_tokens=100,         top_p=1,         frequency_penalty=0,         presence_penalty=0,     )     print(response[\"choices\"][0][\"text\"].replace(\"\\n\", \"\"))      sample_cluster_rows = embedding_df[embedding_df.Cluster == i].sample(transactions_per_cluster, random_state=42)     for j in range(transactions_per_cluster):         print(sample_cluster_rows.Supplier.values[j], end=\", \")         print(sample_cluster_rows.Description.values[j], end=\"\\n   \")         #print(str(sample_cluster_rows.Value).values[j])      print(\"-\" * 100)  <pre>Cluster 0 Theme:  All of the transactions have to do with legal deposit services from the Agency for Legal Deposit Libraries (ALDL).\nALDL, ALDL Charges\n   Agency for Legal Deposit Libaries, Agency services\n   ALDL, Legal Deposit Services\n   ALDL, ALDL Charges\n   ALDL, Legal deposit services\n   British Library, Legal Deposit Services\n   ALDL, Legal deposit materials April to July 22\n   ALDL, Jan20-Mar 20 charge from Agency for Legal Deposit Libraries\n   ALDL, ALDL Charges\n   ALDL, Legal Deposit Services\n   ----------------------------------------------------------------------------------------------------\nCluster 1 Theme:  These transactions all involve the purchase of goods or services from various vendors.\nGARDINER AND THEOBALD LLP, GB Feasibility Study\n   BSI, Annual subscription\n   EX LIBRIS, IT equipment\n   Private Sale, Literary &amp; Archival Items\n   XMA SCOTLAND LTD, 2 invoices one for the replacement of obsolete hardware and the other for a new laptop\n   JISC SERVICES LTD SUBSCRIPTION ACCOUNT, Annual Subscription\n   EDF Energy, Utilities\n   JISC Services Ltd, Managed router service charge annual subscription 01/04/22 to 31/03/23\n   RICK GEKOSKI, Papers 1970's to 2019 Alisdair Gray\n   Trustmarque, Miscrosoft Software Licenses\n   ----------------------------------------------------------------------------------------------------\nCluster 2 Theme:  All of these transactions are related to ECG Facilities Service, and involve payments for facility management fees, inspections and maintenance, maintenance contracts, electrical and mechanical works, boiler replacements, USP batteries, gutter works, and cleaning of pigeon fouling.\nECG FACILITIES SERVICE, This payment covers multiple invoices for facility management fees\n   ECG FACILITIES SERVICE, Facilities Management Charge\n   ECG FACILITIES SERVICE, Inspection and Maintenance of all Library properties\n   ECG Facilities Service, Facilities Management Charge\n   ECG FACILITIES SERVICE, Maintenance contract - October\n   ECG FACILITIES SERVICE, Electrical and mechanical works\n   ECG FACILITIES SERVICE, This payment covers multiple invoices for facility management fees\n   ECG FACILITIES SERVICE, CB Bolier Replacement (1),USP Batteries,Gutter Works &amp; Cleaning of pigeon fouling\n   ECG Facilities Service, Facilities Management Charge\n   ECG Facilities Service, Facilities Management Charge\n   ----------------------------------------------------------------------------------------------------\nCluster 3 Theme:  All of these transactions are related to Kelvin Hall and involve spending money on services or goods.\nCBRE, Kelvin Hall\n   GLASGOW CITY COUNCIL, Kelvin Hall\n   University Of Glasgow, Kelvin Hall\n   GLASGOW LIFE, Oct 20 to Dec 20 service charge - Kelvin Hall\n   Computacenter Uk, Kelvin Hall\n   XMA Scotland Ltd, Kelvin Hall\n   GLASGOW LIFE, Service Charges Kelvin Hall 01/07/19-30/09/19\n   Glasgow Life, Kelvin Hall Service Charges\n   Glasgow City Council, Kelvin Hall\n   GLASGOW LIFE, Quarterly service charge KH\n   ----------------------------------------------------------------------------------------------------\nCluster 4 Theme:  These transactions all involve construction and refurbishment work.\nM &amp; J Ballantyne Ltd, George IV Bridge Work\n   John Graham Construction Ltd, Causewayside Refurbishment\n   John Graham Construction Ltd, Causewayside Refurbishment\n   John Graham Construction Ltd, Causewayside Refurbishment\n   John Graham Construction Ltd, Causewayside Refurbishment\n   ARTHUR MCKAY BUILDING SERVICES, Causewayside Work\n   John Graham Construction Ltd, Causewayside Refurbishment\n   Morris &amp; Spottiswood Ltd, George IV Bridge Work\n   ECG FACILITIES SERVICE, Causewayside IT Work\n   John Graham Construction Ltd, Causewayside Refurbishment\n   ----------------------------------------------------------------------------------------------------\n</pre>"},{"location":"examples/Clustering_for_transaction_classification/#Clustering-for-Transaction-Classification","title":"Clustering for Transaction Classification\u00b6","text":"<p>This notebook covers use cases where your data is unlabelled but has features that can be used to cluster them into meaningful categories. The challenge with clustering is making the features that make those clusters stand out human-readable, and that is where we'll look to use GPT-3 to generate meaningful cluster descriptions for us. We can then use these to apply labels to a previously unlabelled dataset.</p> <p>To feed the model we use embeddings created using the approach displayed in the notebook Multiclass classification for transactions Notebook, applied to the full 359 transactions in the dataset to give us a bigger pool for learning</p>"},{"location":"examples/Clustering_for_transaction_classification/#Setup","title":"Setup\u00b6","text":""},{"location":"examples/Clustering_for_transaction_classification/#Clustering","title":"Clustering\u00b6","text":"<p>We'll reuse the approach from the Clustering Notebook, using K-Means to cluster our dataset using the feature embeddings we created previously. We'll then use the Completions endpoint to generate cluster descriptions for us and judge their effectiveness</p>"},{"location":"examples/Clustering_for_transaction_classification/#Conclusion","title":"Conclusion\u00b6","text":"<p>We now have five new clusters that we can use to describe our data. Looking at the visualisation some of our clusters have some overlap and we'll need some tuning to get to the right place, but already we can see that GPT-3 has made some effective inferences. In particular, it picked up that items including legal deposits were related to literature archival, which is true but the model was given no clues on. Very cool, and with some tuning we can create a base set of clusters that we can then use with a multiclass classifier to generalise to other transactional datasets we might use.</p>"},{"location":"examples/Code_search/","title":"Code search","text":"In\u00a0[1]: Copied! <pre>import os\nfrom glob import glob\nimport pandas as pd\n\ndef get_function_name(code):\n\"\"\"\n    Extract function name from a line beginning with \"def \"\n    \"\"\"\n    assert code.startswith(\"def \")\n    return code[len(\"def \"): code.index(\"(\")]\n\ndef get_until_no_space(all_lines, i) -&gt; str:\n\"\"\"\n    Get all lines until a line outside the function definition is found.\n    \"\"\"\n    ret = [all_lines[i]]\n    for j in range(i + 1, i + 10000):\n        if j &lt; len(all_lines):\n            if len(all_lines[j]) == 0 or all_lines[j][0] in [\" \", \"\\t\", \")\"]:\n                ret.append(all_lines[j])\n            else:\n                break\n    return \"\\n\".join(ret)\n\ndef get_functions(filepath):\n\"\"\"\n    Get all functions in a Python file.\n    \"\"\"\n    whole_code = open(filepath).read().replace(\"\\r\", \"\\n\")\n    all_lines = whole_code.split(\"\\n\")\n    for i, l in enumerate(all_lines):\n        if l.startswith(\"def \"):\n            code = get_until_no_space(all_lines, i)\n            function_name = get_function_name(code)\n            yield {\"code\": code, \"function_name\": function_name, \"filepath\": filepath}\n\n\n# get user root directory\nroot_dir = os.path.expanduser(\"~\")\n# note: for this code to work, the openai-python repo must be downloaded and placed in your root directory\n\n# path to code repository directory\ncode_root = root_dir + \"/openai-python\"\n\ncode_files = [y for x in os.walk(code_root) for y in glob(os.path.join(x[0], '*.py'))]\nprint(\"Total number of py files:\", len(code_files))\n\nif len(code_files) == 0:\n    print(\"Double check that you have downloaded the openai-python repo and set the code_root variable correctly.\")\n\nall_funcs = []\nfor code_file in code_files:\n    funcs = list(get_functions(code_file))\n    for func in funcs:\n        all_funcs.append(func)\n\nprint(\"Total number of functions extracted:\", len(all_funcs))\n</pre> import os from glob import glob import pandas as pd  def get_function_name(code):     \"\"\"     Extract function name from a line beginning with \"def \"     \"\"\"     assert code.startswith(\"def \")     return code[len(\"def \"): code.index(\"(\")]  def get_until_no_space(all_lines, i) -&gt; str:     \"\"\"     Get all lines until a line outside the function definition is found.     \"\"\"     ret = [all_lines[i]]     for j in range(i + 1, i + 10000):         if j &lt; len(all_lines):             if len(all_lines[j]) == 0 or all_lines[j][0] in [\" \", \"\\t\", \")\"]:                 ret.append(all_lines[j])             else:                 break     return \"\\n\".join(ret)  def get_functions(filepath):     \"\"\"     Get all functions in a Python file.     \"\"\"     whole_code = open(filepath).read().replace(\"\\r\", \"\\n\")     all_lines = whole_code.split(\"\\n\")     for i, l in enumerate(all_lines):         if l.startswith(\"def \"):             code = get_until_no_space(all_lines, i)             function_name = get_function_name(code)             yield {\"code\": code, \"function_name\": function_name, \"filepath\": filepath}   # get user root directory root_dir = os.path.expanduser(\"~\") # note: for this code to work, the openai-python repo must be downloaded and placed in your root directory  # path to code repository directory code_root = root_dir + \"/openai-python\"  code_files = [y for x in os.walk(code_root) for y in glob(os.path.join(x[0], '*.py'))] print(\"Total number of py files:\", len(code_files))  if len(code_files) == 0:     print(\"Double check that you have downloaded the openai-python repo and set the code_root variable correctly.\")  all_funcs = [] for code_file in code_files:     funcs = list(get_functions(code_file))     for func in funcs:         all_funcs.append(func)  print(\"Total number of functions extracted:\", len(all_funcs)) <pre>Total number of py files: 51\nTotal number of functions extracted: 97\n</pre> In\u00a0[2]: Copied! <pre>from openai.embeddings_utils import get_embedding\n\ndf = pd.DataFrame(all_funcs)\ndf['code_embedding'] = df['code'].apply(lambda x: get_embedding(x, engine='text-embedding-ada-002'))\ndf['filepath'] = df['filepath'].apply(lambda x: x.replace(code_root, \"\"))\ndf.to_csv(\"data/code_search_openai-python.csv\", index=False)\ndf.head()\n</pre> from openai.embeddings_utils import get_embedding  df = pd.DataFrame(all_funcs) df['code_embedding'] = df['code'].apply(lambda x: get_embedding(x, engine='text-embedding-ada-002')) df['filepath'] = df['filepath'].apply(lambda x: x.replace(code_root, \"\")) df.to_csv(\"data/code_search_openai-python.csv\", index=False) df.head() Out[2]: code function_name filepath code_embedding 0 def _console_log_level():\\n    if openai.log i... _console_log_level /openai/util.py [0.03389773145318031, -0.004390408284962177, 0... 1 def log_debug(message, **params):\\n    msg = l... log_debug /openai/util.py [-0.004034275189042091, 0.004895383026450872, ... 2 def log_info(message, **params):\\n    msg = lo... log_info /openai/util.py [0.004882764536887407, 0.0033515947870910168, ... 3 def log_warn(message, **params):\\n    msg = lo... log_warn /openai/util.py [0.002535992069169879, -0.010829543694853783, ... 4 def logfmt(props):\\n    def fmt(key, val):\\n  ... logfmt /openai/util.py [0.016732551157474518, 0.017367802560329437, 0... In\u00a0[3]: Copied! <pre>from openai.embeddings_utils import cosine_similarity\n\ndef search_functions(df, code_query, n=3, pprint=True, n_lines=7):\n    embedding = get_embedding(code_query, engine='text-embedding-ada-002')\n    df['similarities'] = df.code_embedding.apply(lambda x: cosine_similarity(x, embedding))\n\n    res = df.sort_values('similarities', ascending=False).head(n)\n    if pprint:\n        for r in res.iterrows():\n            print(r[1].filepath+\":\"+r[1].function_name + \"  score=\" + str(round(r[1].similarities, 3)))\n            print(\"\\n\".join(r[1].code.split(\"\\n\")[:n_lines]))\n            print('-'*70)\n    return res\n\nres = search_functions(df, 'Completions API tests', n=3)\n</pre> from openai.embeddings_utils import cosine_similarity  def search_functions(df, code_query, n=3, pprint=True, n_lines=7):     embedding = get_embedding(code_query, engine='text-embedding-ada-002')     df['similarities'] = df.code_embedding.apply(lambda x: cosine_similarity(x, embedding))      res = df.sort_values('similarities', ascending=False).head(n)     if pprint:         for r in res.iterrows():             print(r[1].filepath+\":\"+r[1].function_name + \"  score=\" + str(round(r[1].similarities, 3)))             print(\"\\n\".join(r[1].code.split(\"\\n\")[:n_lines]))             print('-'*70)     return res  res = search_functions(df, 'Completions API tests', n=3) <pre>/openai/tests/test_endpoints.py:test_completions  score=0.826\ndef test_completions():\n    result = openai.Completion.create(prompt=\"This was a test\", n=5, engine=\"ada\")\n    assert len(result.choices) == 5\n\n\n----------------------------------------------------------------------\n/openai/tests/test_endpoints.py:test_completions_model  score=0.811\ndef test_completions_model():\n    result = openai.Completion.create(prompt=\"This was a test\", n=5, model=\"ada\")\n    assert len(result.choices) == 5\n    assert result.model.startswith(\"ada\")\n\n\n----------------------------------------------------------------------\n/openai/tests/test_endpoints.py:test_completions_multiple_prompts  score=0.808\ndef test_completions_multiple_prompts():\n    result = openai.Completion.create(\n        prompt=[\"This was a test\", \"This was another test\"], n=5, engine=\"ada\"\n    )\n    assert len(result.choices) == 10\n\n\n----------------------------------------------------------------------\n</pre> In\u00a0[4]: Copied! <pre>res = search_functions(df, 'fine-tuning input data validation logic', n=3)\n</pre> res = search_functions(df, 'fine-tuning input data validation logic', n=3) <pre>/openai/validators.py:format_inferrer_validator  score=0.751\ndef format_inferrer_validator(df):\n    \"\"\"\n    This validator will infer the likely fine-tuning format of the data, and display it to the user if it is classification.\n    It will also suggest to use ada and explain train/validation split benefits.\n    \"\"\"\n    ft_type = infer_task_type(df)\n    immediate_msg = None\n----------------------------------------------------------------------\n/openai/validators.py:get_validators  score=0.748\ndef get_validators():\n    return [\n        num_examples_validator,\n        lambda x: necessary_column_validator(x, \"prompt\"),\n        lambda x: necessary_column_validator(x, \"completion\"),\n        additional_column_validator,\n        non_empty_field_validator,\n----------------------------------------------------------------------\n/openai/validators.py:infer_task_type  score=0.738\ndef infer_task_type(df):\n    \"\"\"\n    Infer the likely fine-tuning task type from the data\n    \"\"\"\n    CLASSIFICATION_THRESHOLD = 3  # min_average instances of each class\n    if sum(df.prompt.str.len()) == 0:\n        return \"open-ended generation\"\n----------------------------------------------------------------------\n</pre> In\u00a0[5]: Copied! <pre>res = search_functions(df, 'find common suffix', n=2, n_lines=10)\n</pre> res = search_functions(df, 'find common suffix', n=2, n_lines=10) <pre>/openai/validators.py:get_common_xfix  score=0.793\ndef get_common_xfix(series, xfix=\"suffix\"):\n    \"\"\"\n    Finds the longest common suffix or prefix of all the values in a series\n    \"\"\"\n    common_xfix = \"\"\n    while True:\n        common_xfixes = (\n            series.str[-(len(common_xfix) + 1) :]\n            if xfix == \"suffix\"\n            else series.str[: len(common_xfix) + 1]\n----------------------------------------------------------------------\n/openai/validators.py:common_completion_suffix_validator  score=0.778\ndef common_completion_suffix_validator(df):\n    \"\"\"\n    This validator will suggest to add a common suffix to the completion if one doesn't already exist in case of classification or conditional generation.\n    \"\"\"\n    error_msg = None\n    immediate_msg = None\n    optional_msg = None\n    optional_fn = None\n\n    ft_type = infer_task_type(df)\n----------------------------------------------------------------------\n</pre> In\u00a0[6]: Copied! <pre>res = search_functions(df, 'Command line interface for fine-tuning', n=1, n_lines=20)\n</pre> res = search_functions(df, 'Command line interface for fine-tuning', n=1, n_lines=20) <pre>/openai/cli.py:tools_register  score=0.773\ndef tools_register(parser):\n    subparsers = parser.add_subparsers(\n        title=\"Tools\", help=\"Convenience client side tools\"\n    )\n\n    def help(args):\n        parser.print_help()\n\n    parser.set_defaults(func=help)\n\n    sub = subparsers.add_parser(\"fine_tunes.prepare_data\")\n    sub.add_argument(\n        \"-f\",\n        \"--file\",\n        required=True,\n        help=\"JSONL, JSON, CSV, TSV, TXT or XLSX file containing prompt-completion examples to be analyzed.\"\n        \"This should be the local file path.\",\n    )\n    sub.add_argument(\n        \"-q\",\n----------------------------------------------------------------------\n</pre>"},{"location":"examples/Code_search/#Code-search","title":"Code search\u00b6","text":"<p>We index our own openai-python code repository, and show how it can be searched. We implement a simple version of file parsing and extracting of functions from python files.</p>"},{"location":"examples/Customizing_embeddings/","title":"Customizing embeddings","text":"In\u00a0[1]: Copied! <pre># imports\nfrom typing import List, Tuple  # for type hints\n\nimport numpy as np  # for manipulating arrays\nimport pandas as pd  # for manipulating data in dataframes\nimport pickle  # for saving the embeddings cache\nimport plotly.express as px  # for plots\nimport random  # for generating run IDs\nfrom sklearn.model_selection import train_test_split  # for splitting train &amp; test data\nimport torch  # for matrix optimization\n\nfrom openai.embeddings_utils import get_embedding, cosine_similarity  # for embeddings\n</pre> # imports from typing import List, Tuple  # for type hints  import numpy as np  # for manipulating arrays import pandas as pd  # for manipulating data in dataframes import pickle  # for saving the embeddings cache import plotly.express as px  # for plots import random  # for generating run IDs from sklearn.model_selection import train_test_split  # for splitting train &amp; test data import torch  # for matrix optimization  from openai.embeddings_utils import get_embedding, cosine_similarity  # for embeddings  In\u00a0[2]: Copied! <pre># input parameters\nembedding_cache_path = \"data/snli_embedding_cache.pkl\"  # embeddings will be saved/loaded here\ndefault_embedding_engine = \"babbage-similarity\"  # text-embedding-ada-002 is recommended\nnum_pairs_to_embed = 1000  # 1000 is arbitrary\nlocal_dataset_path = \"data/snli_1.0_train_2k.csv\"  # download from: https://nlp.stanford.edu/projects/snli/\n\n\ndef process_input_data(df: pd.DataFrame) -&gt; pd.DataFrame:\n    # you can customize this to preprocess your own dataset\n    # output should be a dataframe with 3 columns: text_1, text_2, label (1 for similar, -1 for dissimilar)\n    df[\"label\"] = df[\"gold_label\"]\n    df = df[df[\"label\"].isin([\"entailment\"])]\n    df[\"label\"] = df[\"label\"].apply(lambda x: {\"entailment\": 1, \"contradiction\": -1}[x])\n    df = df.rename(columns={\"sentence1\": \"text_1\", \"sentence2\": \"text_2\"})\n    df = df[[\"text_1\", \"text_2\", \"label\"]]\n    df = df.head(num_pairs_to_embed)\n    return df\n</pre> # input parameters embedding_cache_path = \"data/snli_embedding_cache.pkl\"  # embeddings will be saved/loaded here default_embedding_engine = \"babbage-similarity\"  # text-embedding-ada-002 is recommended num_pairs_to_embed = 1000  # 1000 is arbitrary local_dataset_path = \"data/snli_1.0_train_2k.csv\"  # download from: https://nlp.stanford.edu/projects/snli/   def process_input_data(df: pd.DataFrame) -&gt; pd.DataFrame:     # you can customize this to preprocess your own dataset     # output should be a dataframe with 3 columns: text_1, text_2, label (1 for similar, -1 for dissimilar)     df[\"label\"] = df[\"gold_label\"]     df = df[df[\"label\"].isin([\"entailment\"])]     df[\"label\"] = df[\"label\"].apply(lambda x: {\"entailment\": 1, \"contradiction\": -1}[x])     df = df.rename(columns={\"sentence1\": \"text_1\", \"sentence2\": \"text_2\"})     df = df[[\"text_1\", \"text_2\", \"label\"]]     df = df.head(num_pairs_to_embed)     return df  In\u00a0[3]: Copied! <pre># load data\ndf = pd.read_csv(local_dataset_path)\n\n# process input data\ndf = process_input_data(df)  # this demonstrates training data containing only positives\n\n# view data\ndf.head()\n</pre> # load data df = pd.read_csv(local_dataset_path)  # process input data df = process_input_data(df)  # this demonstrates training data containing only positives  # view data df.head()  <pre>/var/folders/r4/x3kdvs816995fnnph2gdpwp40000gn/T/ipykernel_17509/1977422881.py:13: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[\"label\"] = df[\"label\"].apply(lambda x: {\"entailment\": 1, \"contradiction\": -1}[x])\n</pre> Out[3]: text_1 text_2 label 2 A person on a horse jumps over a broken down a... A person is outdoors, on a horse. 1 4 Children smiling and waving at camera There are children present 1 7 A boy is jumping on skateboard in the middle o... The boy does a skateboarding trick. 1 14 Two blond women are hugging one another. There are women showing affection. 1 17 A few people in a restaurant setting, one of t... The diners are at a restaurant. 1 In\u00a0[4]: Copied! <pre># split data into train and test sets\ntest_fraction = 0.5  # 0.5 is fairly arbitrary\nrandom_seed = 123  # random seed is arbitrary, but is helpful in reproducibility\ntrain_df, test_df = train_test_split(\n    df, test_size=test_fraction, stratify=df[\"label\"], random_state=random_seed\n)\ntrain_df.loc[:, \"dataset\"] = \"train\"\ntest_df.loc[:, \"dataset\"] = \"test\"\n</pre> # split data into train and test sets test_fraction = 0.5  # 0.5 is fairly arbitrary random_seed = 123  # random seed is arbitrary, but is helpful in reproducibility train_df, test_df = train_test_split(     df, test_size=test_fraction, stratify=df[\"label\"], random_state=random_seed ) train_df.loc[:, \"dataset\"] = \"train\" test_df.loc[:, \"dataset\"] = \"test\"  In\u00a0[5]: Copied! <pre># generate negatives\ndef dataframe_of_negatives(dataframe_of_positives: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Return dataframe of negative pairs made by combining elements of positive pairs.\"\"\"\n    texts = set(dataframe_of_positives[\"text_1\"].values) | set(\n        dataframe_of_positives[\"text_2\"].values\n    )\n    all_pairs = {(t1, t2) for t1 in texts for t2 in texts if t1 &lt; t2}\n    positive_pairs = set(\n        tuple(text_pair)\n        for text_pair in dataframe_of_positives[[\"text_1\", \"text_2\"]].values\n    )\n    negative_pairs = all_pairs - positive_pairs\n    df_of_negatives = pd.DataFrame(list(negative_pairs), columns=[\"text_1\", \"text_2\"])\n    df_of_negatives[\"label\"] = -1\n    return df_of_negatives\n</pre> # generate negatives def dataframe_of_negatives(dataframe_of_positives: pd.DataFrame) -&gt; pd.DataFrame:     \"\"\"Return dataframe of negative pairs made by combining elements of positive pairs.\"\"\"     texts = set(dataframe_of_positives[\"text_1\"].values) | set(         dataframe_of_positives[\"text_2\"].values     )     all_pairs = {(t1, t2) for t1 in texts for t2 in texts if t1 &lt; t2}     positive_pairs = set(         tuple(text_pair)         for text_pair in dataframe_of_positives[[\"text_1\", \"text_2\"]].values     )     negative_pairs = all_pairs - positive_pairs     df_of_negatives = pd.DataFrame(list(negative_pairs), columns=[\"text_1\", \"text_2\"])     df_of_negatives[\"label\"] = -1     return df_of_negatives  In\u00a0[6]: Copied! <pre>negatives_per_positive = (\n    1  # it will work at higher values too, but more data will be slower\n)\n# generate negatives for training dataset\ntrain_df_negatives = dataframe_of_negatives(train_df)\ntrain_df_negatives[\"dataset\"] = \"train\"\n# generate negatives for test dataset\ntest_df_negatives = dataframe_of_negatives(test_df)\ntest_df_negatives[\"dataset\"] = \"test\"\n# sample negatives and combine with positives\ntrain_df = pd.concat(\n    [\n        train_df,\n        train_df_negatives.sample(\n            n=len(train_df) * negatives_per_positive, random_state=random_seed\n        ),\n    ]\n)\ntest_df = pd.concat(\n    [\n        test_df,\n        test_df_negatives.sample(\n            n=len(test_df) * negatives_per_positive, random_state=random_seed\n        ),\n    ]\n)\n\ndf = pd.concat([train_df, test_df])\n</pre> negatives_per_positive = (     1  # it will work at higher values too, but more data will be slower ) # generate negatives for training dataset train_df_negatives = dataframe_of_negatives(train_df) train_df_negatives[\"dataset\"] = \"train\" # generate negatives for test dataset test_df_negatives = dataframe_of_negatives(test_df) test_df_negatives[\"dataset\"] = \"test\" # sample negatives and combine with positives train_df = pd.concat(     [         train_df,         train_df_negatives.sample(             n=len(train_df) * negatives_per_positive, random_state=random_seed         ),     ] ) test_df = pd.concat(     [         test_df,         test_df_negatives.sample(             n=len(test_df) * negatives_per_positive, random_state=random_seed         ),     ] )  df = pd.concat([train_df, test_df])  In\u00a0[7]: Copied! <pre># establish a cache of embeddings to avoid recomputing\n# cache is a dict of tuples (text, engine) -&gt; embedding\ntry:\n    with open(embedding_cache_path, \"rb\") as f:\n        embedding_cache = pickle.load(f)\nexcept FileNotFoundError:\n    precomputed_embedding_cache_path = \"https://cdn.openai.com/API/examples/data/snli_embedding_cache.pkl\"\n    embedding_cache = pd.read_pickle(precomputed_embedding_cache_path)\n\n\n# this function will get embeddings from the cache and save them there afterward\ndef get_embedding_with_cache(\n    text: str,\n    engine: str = default_embedding_engine,\n    embedding_cache: dict = embedding_cache,\n    embedding_cache_path: str = embedding_cache_path,\n) -&gt; list:\n    print(f\"Getting embedding for {text}\")\n    if (text, engine) not in embedding_cache.keys():\n        # if not in cache, call API to get embedding\n        embedding_cache[(text, engine)] = get_embedding(text, engine)\n        # save embeddings cache to disk after each update\n        with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n            pickle.dump(embedding_cache, embedding_cache_file)\n    return embedding_cache[(text, engine)]\n\n\n# create column of embeddings\nfor column in [\"text_1\", \"text_2\"]:\n    df[f\"{column}_embedding\"] = df[column].apply(get_embedding_with_cache)\n\n# create column of cosine similarity between embeddings\ndf[\"cosine_similarity\"] = df.apply(\n    lambda row: cosine_similarity(row[\"text_1_embedding\"], row[\"text_2_embedding\"]),\n    axis=1,\n)\n</pre> # establish a cache of embeddings to avoid recomputing # cache is a dict of tuples (text, engine) -&gt; embedding try:     with open(embedding_cache_path, \"rb\") as f:         embedding_cache = pickle.load(f) except FileNotFoundError:     precomputed_embedding_cache_path = \"https://cdn.openai.com/API/examples/data/snli_embedding_cache.pkl\"     embedding_cache = pd.read_pickle(precomputed_embedding_cache_path)   # this function will get embeddings from the cache and save them there afterward def get_embedding_with_cache(     text: str,     engine: str = default_embedding_engine,     embedding_cache: dict = embedding_cache,     embedding_cache_path: str = embedding_cache_path, ) -&gt; list:     print(f\"Getting embedding for {text}\")     if (text, engine) not in embedding_cache.keys():         # if not in cache, call API to get embedding         embedding_cache[(text, engine)] = get_embedding(text, engine)         # save embeddings cache to disk after each update         with open(embedding_cache_path, \"wb\") as embedding_cache_file:             pickle.dump(embedding_cache, embedding_cache_file)     return embedding_cache[(text, engine)]   # create column of embeddings for column in [\"text_1\", \"text_2\"]:     df[f\"{column}_embedding\"] = df[column].apply(get_embedding_with_cache)  # create column of cosine similarity between embeddings df[\"cosine_similarity\"] = df.apply(     lambda row: cosine_similarity(row[\"text_1_embedding\"], row[\"text_2_embedding\"]),     axis=1, )  <pre>Getting embedding for Three wheelchair basketball players wearing team uniforms are attempting to reach the descending basketball with other players in the background.\nGetting embedding for A man with a beard, wearing a red shirt with gray sleeves and work gloves, pulling on a rope.\nGetting embedding for A person in a red coat and a running black and brown dog.\nGetting embedding for Five men, one wearing a white shirt standing on something, hanging up a picture of a child.\nGetting embedding for A busy city that looks like New York City.\nGetting embedding for People on bicycles waiting at an intersection.\nGetting embedding for Emergency personnel looking into the back of a car.\nGetting embedding for Outside by the trees, a woman wearing jeans and red jacket throws something for a German shepherd to chase.\nGetting embedding for A man with a red shirt is watching another man who is standing on top of a attached cart filled to the top.\nGetting embedding for High fashion ladies wait outside a tram beside a crowd of people in the city.\nGetting embedding for A young man and young lady dancing on a carpeted floor with a picture from the movie Toy Story on a big screen in the background.\nGetting embedding for Two tan and white dogs and one tan dog racing down the beach near the water.\nGetting embedding for Two women, each embracing a little girl, catch up at a small family gathering.\nGetting embedding for A person is hanging up pictures of women with a few onlookers watching surrounded by bikes.\nGetting embedding for A man wearing a white shire shirt and hate is riding a bucking horse in a rodeo.\nGetting embedding for A man and a woman are holding hands as they walk along a city sidewalk.\nGetting embedding for A young woman packs belongings into a black suitcase.\nGetting embedding for A man with a red shirt is watching another man who is standing on top of a attached cart filled to the top.\nGetting embedding for A main street scene of a small town with an overhead welcome sign that says \"Welcome to Golden\".\nGetting embedding for Little girl walking along a dirt, rock, and grass path wearing a blue hat many steps behind two people wearing backpacks and holding umbrellas.\nGetting embedding for A woman is walking across the street eating a banana, while a man is following with his briefcase.\nGetting embedding for A couple playing with a little boy on the beach.\nGetting embedding for Child in red and blue shirt painting a log.\nGetting embedding for A man in a gold skirt sits in front of the computer.\nGetting embedding for Soccer teams play on a field as the sun sets behind a line of trees.\nGetting embedding for two little girls, one in a green jacket and one in a pink jacket, and a little boy in a green jacket holding an apple sitting on a rock.\nGetting embedding for A man in an elf hat holding a white umbrella is standing on the sidewalk with two other men.\nGetting embedding for Mothers with children talking at a gathering.\nGetting embedding for Workers are sitting next to a work zone eating food.\nGetting embedding for Three young children consisting of two girls and a boy who is holding an apple with a bite out of it, are posing on a scenic mountain view background.\nGetting embedding for A small group of church-goers watch a choir practice.\nGetting embedding for A family between a van and fence\nGetting embedding for Two men with heads down signing a paper.\nGetting embedding for A woman wearing orange looking upward.\nGetting embedding for A woman with dark hair is wearing a green sweater.\nGetting embedding for A brown dog running with two white and brown dogs on the seashore with crashing waves behind them.\nGetting embedding for A soccer match between a team with white jerseys, and a team with yellow jerseys.\nGetting embedding for A mountain biker jumping a slope outdoors in a forest area.\nGetting embedding for The Arsenal football club warms-up on the soccer field as a few fans watch.\nGetting embedding for A man in blue shorts and without a shirt is jogging down the road while listening to his iPod.\nGetting embedding for Three girls on an amusement ride enjoying themselves.\nGetting embedding for A woman wearing a green headscarf laughs while a woman in the background paddles a boat.\nGetting embedding for A soccer game where the team in yellow is attempting to advance past the team in white towards the goalie wearing a black top and blue shorts.\nGetting embedding for Group of people dancing\nGetting embedding for A man wearing a blue shirt is sitting on a brick planter next to some paintings.\nGetting embedding for People listening to a choir in a Catholic church.\nGetting embedding for A driver is racing his Ford vehicle on a gravel track.\nGetting embedding for three bikers stop in town.\nGetting embedding for A skier slides along a metal rail.\nGetting embedding for A good-looking firefighter sets up \"Do Not Cross\" tape in the city.\nGetting embedding for Three young children consisting of two girls and a boy who is holding an apple with a bite out of it, are posing on a scenic mountain view background.\nGetting embedding for Closeup image of a dog swimming.\nGetting embedding for An english farmer with a horse pulled wagon.\nGetting embedding for A man in a white shirt hangs a painting in a run down store while other men watch.\nGetting embedding for two female medical personnel read their charts.\nGetting embedding for A person is hanging up pictures of women with a few onlookers watching surrounded by bikes.\nGetting embedding for The dogs run and play with a red ball.\nGetting embedding for Two men in wheelchairs are reaching in the air for a basketball.\nGetting embedding for Men fish on a concrete slab.\nGetting embedding for Three guys and a girl are all jumping in a pool together.\nGetting embedding for Busy ChinaTown street corner where people are walking past an open front store.\nGetting embedding for Young lady dressed in black shorts and light blue shirt sitting outside at a public table looking at a picture on her camera with her left hand on her face.\nGetting embedding for The skier is wearing a yellow jumpsuit and sliding across a yellow rail.\nGetting embedding for A man on a street in a bright t-shirt holds some sort of tablet towards a woman in a pink t-shirt and shades.\nGetting embedding for Toddler with milk around his mouth.\nGetting embedding for man sitting down playing a game of chess alone\nGetting embedding for Four teenage boys are doing cannonballs into a swimming pool.\nGetting embedding for Little girl in a blue and yellow plaid outfit and blue hat is running along the trail.\nGetting embedding for Men in uniform work together.\nGetting embedding for A blond child is pulling a wagon with a little blond boy in it.\nGetting embedding for Busy Japanese intersection like maybe Tokyo.\nGetting embedding for A model posing to look as if she's a real female soccer player.\nGetting embedding for 1 little boy wearing a pirate costume following closely behind a little girl wearing a blue dress carrying a orange pumpkin bucket and walking down the sidewalk.\nGetting embedding for People in orange vests and blue pants with a yellow stripe at the bottom await the beginning of a race.\nGetting embedding for A bunch of people playing soccer.\nGetting embedding for A woman talks to two other women and a man with notepads in an office building with large windows.\nGetting embedding for Five men, one wearing a white shirt standing on something, hanging up a picture of a child.\nGetting embedding for People sit and relax next to a pool in a plaza.\nGetting embedding for A young boy paddles across the water in a makeshift boat.\nGetting embedding for A woman is waiting with children as she is checked out at Walmart.\nGetting embedding for There is a woman holding a baby, along with a man with a save the children bag.\nGetting embedding for A soccer game occurring at sunset.\nGetting embedding for A groom and a bride are standing on the grass with his hand on her waist.\nGetting embedding for a motorcyclist does a nose wheelie.\nGetting embedding for A man with a red shirt is watching another man who is standing on top of a attached cart filled to the top.\nGetting embedding for Small blond-haired girl drinking a glass of juice.\nGetting embedding for Five men, one wearing a white shirt standing on something, hanging up a picture of a child.\nGetting embedding for A woman stands behind an outdoor grill with a blue basket of food in her hands.\nGetting embedding for People in a truck-full of sacks in a field full of sheep.\nGetting embedding for An expectant woman happily lets another listen to the baby inside of her.\nGetting embedding for A man wearing blue jeans and red bowling shoes stands in a bowling alley lane with a green ball in his hand.\nGetting embedding for Two women, each with a child, look at each other.\nGetting embedding for A young girl dancing in her socks on a wooden floor strewn with pink balloons.\nGetting embedding for A climber is making his way up a snowy mountainside.\nGetting embedding for Five men, one wearing a white shirt standing on something, hanging up a picture of a child.\nGetting embedding for A woman in costume is marching with a large drum.\nGetting embedding for Two men cook together with a metal bowl, near a hanging plant.\nGetting embedding for A man and a woman are standing next to sculptures, talking while another man looks at other sculptures.\nGetting embedding for A fuzzy white lap dog runs along a rocky beach.\nGetting embedding for Three men are smiling and posing behind a truck loaded with various construction supplies.\nGetting embedding for A dog drops a red disc on a beach.\nGetting embedding for A girl wearing a dress is blowing bubbles at a dock.\nGetting embedding for A little boy underwater in a pool, holding a plastic dinosaur.\nGetting embedding for A group of people sitting around a picnic table.\nGetting embedding for A person on a horse jumps over a broken down airplane.\nGetting embedding for A man and two women in black jackets holding umbrellas sit on a long wooden bench.\nGetting embedding for Two men stand around a mixing bowl.\nGetting embedding for the three boys are all holding onto a flotation device in the water.\nGetting embedding for Children going home from school.\nGetting embedding for Several people are dancing together in sync.\nGetting embedding for A group of people sitting at a table outside talking.\nGetting embedding for A boy in a blue, yellow, and orange shirt holding his arms out from his sides.\nGetting embedding for People waiting at a light on bikes.\nGetting embedding for A sumo wrestler with a brown belt is pushing another wrestler in a bout.\nGetting embedding for Young lady dressed in black shorts and light blue shirt sitting outside at a public table looking at a picture on her camera with her left hand on her face.\nGetting embedding for Woman in white in foreground and a man slightly behind walking with a sign for John's Pizza and Gyro in the background.\nGetting embedding for Two people wearing blue clothing are making hand gestures next to one another.\nGetting embedding for A white bike is leaning against a post.\nGetting embedding for Two adults, one female in white, with shades and one male, gray clothes, walking across a street, away from a eatery with a blurred image of a dark colored red shirted person in the foreground.\nGetting embedding for An older man is drinking orange juice at a restaurant.\nGetting embedding for Skydivers in formation.\nGetting embedding for A black dog swimming in water near rocks.\nGetting embedding for A view of buildings and people walking across the streets in Times Square, New York City.\nGetting embedding for People in orange vests and blue pants with a yellow stripe at the bottom await the beginning of a race.\nGetting embedding for A woman is walking across the street eating a banana, while a man is following with his briefcase.\nGetting embedding for a woman wearing a Chinese straw hat operating some sort of stainless steel machine in what appears to be a park.\nGetting embedding for Two men share a laugh while in the kitchen.\nGetting embedding for A little kid enjoying some sledding on a winter day.\nGetting embedding for A child with a brightly colored shirt plays outside.\nGetting embedding for People waiting at a light on bikes.\nGetting embedding for A woman and a child holding on to the railing while on trolley.\nGetting embedding for A gentleman in a purple scarf and hat is looking at money while holding an accordion.\nGetting embedding for Toddler in striped sweatshirt plays on rope on playground.\nGetting embedding for A boy in a red and blue shirt painting a log.\nGetting embedding for An elderly couple dance in front of a juke box while a guy in shorts sleeps at a nearby table\nGetting embedding for Two women and one man sit on a bench.\nGetting embedding for A man in a red and black jacket, blue shirt, lots of silver necklaces, and his blue jeans falling down, checks out a woman wearing a black leather jacket, yellow bra, pink fingerless gloves, and sunglasses.\nGetting embedding for A young man in a red quilted vest displays an assortment of silver pendants around his neck as he watches a woman in a yellow bikini top, a black jacket, and bright pink fingerless gloves go by.\nGetting embedding for three bikers stop in town.\nGetting embedding for A woman in a green jacket and hood over her head looking towards a valley.\nGetting embedding for A baseball player is about to throw a baseball.\nGetting embedding for Two elderly men having a conversation, snow covered grass in the background.\nGetting embedding for A man is using his computer while seated at a desk.\nGetting embedding for Firemen emerge from a subway station.\nGetting embedding for A female violinist surrounded by other violinists.\nGetting embedding for Soccer players on a field from a distance.\nGetting embedding for A man sitting on a scooter on the curb.\nGetting embedding for Motorcyclist performing while two men watch.\nGetting embedding for Indian couple holding child near riverbank.\nGetting embedding for Four people near a body of water, one sitting and three standing, while two people walk on a nearby sidewalk.\nGetting embedding for A woman sitting in a laundromat looking at the camera.\nGetting embedding for Two men are on scaffolding as they paint above a storefront while a man on the sidewalk stands next to them talking on the phone.\nGetting embedding for A man is sitting with his head facing down, while other people are looking in his direction.\nGetting embedding for A little boy swimming underwater with a toy in his hand.\nGetting embedding for Bicyclists waiting at an intersection.\nGetting embedding for A man wearing black is playing an electric guitar at a concert.\nGetting embedding for Two adults, one female in white, with shades and one male, gray clothes, walking across a street, away from a eatery with a blurred image of a dark colored red shirted person in the foreground.\nGetting embedding for A young man doing a trick on a skateboard down the stairs while being photographed.\nGetting embedding for Asian city scene of people in street with bright lights and glass buildings behind.\nGetting embedding for A woman in a green jacket and black sunglasses outside in a crowd.\nGetting embedding for man in red canada shirt standing with three men in army uniform\nGetting embedding for Two women are walking down a dirt path carrying loads on their heads.\nGetting embedding for Young woman in a cafe checking her cellphone.\nGetting embedding for Four people near a body of water, one sitting and three standing, while two people walk on a nearby sidewalk.\nGetting embedding for A mother with her four children.\nGetting embedding for A woman talks to two other women and a man with notepads in an office building with large windows.\nGetting embedding for A man on a street in a bright t-shirt holds some sort of tablet towards a woman in a pink t-shirt and shades.\nGetting embedding for Older couple posing for a picture in front of a fountain.\nGetting embedding for The boy locked the cycle and went away.\nGetting embedding for People waiting to get on a train or just getting off.\nGetting embedding for A man on a street in a bright t-shirt holds some sort of tablet towards a woman in a pink t-shirt and shades.\nGetting embedding for A crowded street, in an Asian country, where the buildings are dominated by the Seiko building.\nGetting embedding for A boat worker securing line.\nGetting embedding for A man wearing a red sweater is sitting on a car bumper watching another person work.\nGetting embedding for A man walking along side a clean up crew.\nGetting embedding for A blond woman with her hair up is taking off a white sweatshirt.\nGetting embedding for A man with a bright green shirt is talking to a woman in a pink shirt.\nGetting embedding for A large group, wearing pink shirts, waves to onlookers.\nGetting embedding for A woman in a white dress with a tiara sings in a chorus, which has a row of men in sailor hats.\nGetting embedding for A little boy in brown pants is playing on ropes at a park.\nGetting embedding for Woman in white in foreground and a man slightly behind walking with a sign for John's Pizza and Gyro in the background.\nGetting embedding for A person on skis on a rail at night.\nGetting embedding for A man with a gray shirt holds a young infant in his hands.\nGetting embedding for A young man in his mid twenties is kicking his left foot about two feet off the leaf covered ground, with paved asphalt and green plants and trees in the background.\nGetting embedding for A man is singing into a microphone.\nGetting embedding for A man in ruffles pushes a stroller through a park.\nGetting embedding for Outside by the trees, a woman wearing jeans and red jacket throws something for a German shepherd to chase.\nGetting embedding for A man sitting in a barber shop.\nGetting embedding for The surfer catches a big wave but stays on his board.\nGetting embedding for A little boy swims underwater.\nGetting embedding for Oddly dressed man walking down the street.\nGetting embedding for Cars are passing through a town.\nGetting embedding for Child with pink strings on head dancing surrounded by confetti, balloons.\nGetting embedding for A gray-haired woman in a blue dress coat with snowflakes on it balances something on her head.\nGetting embedding for A man walking proudly down the street.\nGetting embedding for An average looking man is playing the guitar.\nGetting embedding for A young woman tries to stick her foot in a fountain.\nGetting embedding for Young lady dressed in black shorts and light blue shirt sitting outside at a public table looking at a picture on her camera with her left hand on her face.\nGetting embedding for A woman is walking across the street eating a banana, while a man is following with his briefcase.\nGetting embedding for A spotted black and white dog splashes in the water.\nGetting embedding for A boy in a blue, yellow, and orange shirt holding his arms out from his sides.\nGetting embedding for A cowboy Roping a calf in a rodeo.\nGetting embedding for A little boy drinks milk and gets milk all over his face and table.\nGetting embedding for Woman with green sweater and sunglasses smiling\nGetting embedding for A dog jumps to catch a toy in the snow.\nGetting embedding for A woman on the side of a street is making food on her cart.\nGetting embedding for A young woman tries to stick her foot in a fountain.\nGetting embedding for A smiling man cooks something delicious.\nGetting embedding for A woman in colorful native attire featuring a blue shirt with a colorful design displays her dark hair braided with red ribbons.\nGetting embedding for Two older men are talking.\nGetting embedding for Three dogs in different shades of brown and white biting and licking each other.\nGetting embedding for A man and a woman being intimate, with their legs in the water.\nGetting embedding for A dog is chasing a ball in a backyard.\nGetting embedding for A male painting a scene in front of him.\nGetting embedding for Two young girls are playing outside in a non-urban environment.\nGetting embedding for a couple are holding hands behind their backs while walking down a street, and the man has his arm around her shoulder.\nGetting embedding for Cheerleaders are on the field cheering.\nGetting embedding for A young woman is playing the violin.\nGetting embedding for A man in costume is ringing a bell.\nGetting embedding for Tourists waiting at a train stop.\nGetting embedding for two small girls walk along the leaves.\nGetting embedding for Two pre-teen girls listening to music on an MP3 player with headphones.\nGetting embedding for The furry brown dog is swimming in the ocean.\nGetting embedding for A man on a street in a bright t-shirt holds some sort of tablet towards a woman in a pink t-shirt and shades.\nGetting embedding for Little Girl in brown shirt and blue jean skirt dances on wood floor.\nGetting embedding for A man holding a green bowling ball stands by the ball return machine in a bowling alley.\nGetting embedding for A Little League team tries to catch a runner sliding into a base in an afternoon game.\nGetting embedding for A row of legs and black boots with a boy sitting at the end of the row.\nGetting embedding for Two people loading brush on a trailer attached to a truck.\nGetting embedding for A woman is walking across the street eating a banana, while a man is following with his briefcase.\nGetting embedding for Three men standing on grass by the water looking at something on a table.\nGetting embedding for An oddly dressed man pushing a stroller down a sidewalk in a park.\nGetting embedding for Various people hanging around outside of a building.\nGetting embedding for A man is painting a portrait of an outside scene that includes a street sign with a bicycle chained to it.\nGetting embedding for a skateboarder skates in the pool.\nGetting embedding for A big brown dog swims towards the camera.\nGetting embedding for A shot-on-goal action photo of soccer players in red and black uniforms.\nGetting embedding for Three people are sitting on a bench.\nGetting embedding for Two young men drink beer, leaning on a graffitied wall.\nGetting embedding for A man in a tan suit is using a pay phone to make a call.\nGetting embedding for Cheerleaders are on the field cheering.\nGetting embedding for A blond woman with two children is checking out at a Walmart register.\nGetting embedding for Women exercising one woman has a green mat and black outfit on.\nGetting embedding for People on bicycles waiting at an intersection.\nGetting embedding for A man in a striped polo shirt is pointing and smiling.\nGetting embedding for A mountain biker jumping a slope outdoors in a forest area.\nGetting embedding for A small white dog running on a pebble covered beach.\nGetting embedding for A man and a woman are standing next to sculptures, talking while another man looks at other sculptures.\nGetting embedding for A young Asian man sits behind a set chessboard waiting for the other player to arrive.\nGetting embedding for Cheerleaders are on the field cheering.\nGetting embedding for Two adults, one female in white, with shades and one male, gray clothes, walking across a street, away from a eatery with a blurred image of a dark colored red shirted person in the foreground.\nGetting embedding for Two men are standing outside and snow is on the ground.\nGetting embedding for Two people standing in front of a large statue of a woman, other statues and busts visible in the background.\nGetting embedding for One biker is running with their bike while another is riding around them.\nGetting embedding for A woman in a blue shirt and green hat looks up at the camera.\nGetting embedding for a young man wearing a backpack and sunglasses is walking towards a shopping area.\nGetting embedding for A man and woman watching two kids while the man holds a balloon.\nGetting embedding for A child stoops to pick up a watermelon from a large pile of them.\nGetting embedding for A woman is sitting at an outdoor dining table.\nGetting embedding for Asian city scene of people in street with bright lights and glass buildings behind.\nGetting embedding for A man, wearing revolutionary period clothes, is ringing a bell.\nGetting embedding for A man carrying a load of fresh direct boxes on car with wheels in the city streets, as a woman walks towards him.\nGetting embedding for A man parasails in the choppy water.\nGetting embedding for A crowded street, in an Asian country, where the buildings are dominated by the Seiko building.\nGetting embedding for A foreign family is walking along a dirt path next to the water.\nGetting embedding for Two girls, each in a dress walking together.\nGetting embedding for A black-and-white dog carries a stick in his mouth as he swims in the clear water.\nGetting embedding for Stacks of neatly folded clothing cover most of this floor while a woman with a beige shirt and jeans busily fills a suitcase.\nGetting embedding for Woman at Walmart check-out having her groceries bagged by an employee.\nGetting embedding for A doctor checks the stomach of a toddler.\nGetting embedding for A couple, wearing black, burgundy, and white, dance.\nGetting embedding for The young man is waiting with others on the sidewalk.\nGetting embedding for A street vendor in Asia tries to bring in more customers.\nGetting embedding for A man being airlifted to safety after being in danger.\nGetting embedding for A man and a woman are walking on a street at the top of a hill.\nGetting embedding for Wet brown dog swims towards camera.\nGetting embedding for Five men, one wearing a white shirt standing on something, hanging up a picture of a child.\nGetting embedding for A man riding a dirt bike\nGetting embedding for Two Asian people sit at a blue table in a food court.\nGetting embedding for A white dog runs along a rocky shoreline.\nGetting embedding for A woman talks to two other women and a man with notepads in an office building with large windows.\nGetting embedding for A young man is performing a jump on a skateboard while another young man photographs his stunt.\nGetting embedding for A couple pose in front of a fountain.\nGetting embedding for Bicyclists waiting at an intersection.\nGetting embedding for A man riding a dirt bike\nGetting embedding for There are cars driving down a street with a sign displaying Welcome to Golden.\nGetting embedding for BMX biker jumps over a ravine.\nGetting embedding for A couple, who appear to be Indian or Pakistani, walk on a path beside a body of water, the mother carrying a child in a diaper, the father wrapped in a blanket with the logo of the humanitarian organization \"Save the Children.\"\nGetting embedding for A track event held by J.P. Morgan Chase with security.\nGetting embedding for A man in a gold foils skirt, sitting at a computer desk, looks at the camera with his hands raised to his face.\nGetting embedding for Two little girls lie on the carpet next to an O made of wooden blocks.\nGetting embedding for A young boy wearing a light blue jacket walks across the brick patio.\nGetting embedding for A man in a bright green shirt shows a woman in a bright pink shirt something on a clipboard.\nGetting embedding for A dog in the water carries a stick in his mouth.\nGetting embedding for A barber waiting for customers.\nGetting embedding for a girl wearing a blue and pink swimsuit is throwing stones into a lake.\nGetting embedding for A woman is running a marathon in a park.\nGetting embedding for A child with a yellow cup and milk all over his face.\nGetting embedding for An excited, smiling woman stands at a red railing as she holds a boombox to one side.\nGetting embedding for There are four Eastern guys working on and hanging pictures of humans.\nGetting embedding for Two men are cooking in the kitchen using rice milk.\nGetting embedding for A meeting of young people sitting at a conference table.\nGetting embedding for Indian lady and a guy in a blue suit dancing in the sunlight.\nGetting embedding for A dog is preparing to run away from a person interacting with it.\nGetting embedding for Two women holding children talking to one another.\nGetting embedding for A man is putting up a poster in front of a shop.\nGetting embedding for A soccer game played by a blue team and a red team on a perfectly manicured field at dusk.\nGetting embedding for Two men are painting a building while a third is walking past on his cellphone.\nGetting embedding for Young woman running as two guys in the back try to catch up to her.\nGetting embedding for Asian city scene of people in street with bright lights and glass buildings behind.\nGetting embedding for A baseball player is putting all his might in to throwing a ball.\nGetting embedding for Group of young women in dresses strolling on the sidewalk.\nGetting embedding for A woman talks to two other women and a man with notepads in an office building with large windows.\nGetting embedding for a young girl in a flowery dress surrounded by watermelons\nGetting embedding for People going for a long walk to the mountains.\nGetting embedding for A group of young girls playing jump rope in the street.\nGetting embedding for A woman is making a clay pot.\nGetting embedding for A soccer game where the team in yellow is attempting to advance past the team in white towards the goalie wearing a black top and blue shorts.\nGetting embedding for A man and a woman are standing next to sculptures, talking while another man looks at other sculptures.\nGetting embedding for A mountain biker jumping a slope outdoors in a forest area.\nGetting embedding for An elderly couple dances next to a table where a boy is sitting with his head down.\nGetting embedding for People in a meeting setting paying attention to a speaker in an orange shirt.\nGetting embedding for A lady is kneeling wearing a blue shirt.\nGetting embedding for A couple holding hands walks down a street.\nGetting embedding for Three small puppies bite and play together in the grass.\nGetting embedding for Four people are acting behind a woman in a yellow shirt is lying on the ground.\nGetting embedding for a woman in a red jacket watches as a black and brown dog runs away from her in woodland clearing.\nGetting embedding for A man dances with a fire baton at night.\nGetting embedding for A man stare at a passing couple while walking down the block.\nGetting embedding for Two older men in winter coats talking outside of a shop with a grassy lawn covered in a light coat of snow in front of it.\nGetting embedding for A man and woman are walking down the street holding hands.\nGetting embedding for A man is putting up a poster in front of a shop.\nGetting embedding for Woman in white in foreground and a man slightly behind walking with a sign for John's Pizza and Gyro in the background.\nGetting embedding for Hispanic woman wearing a red plaid shirt works on sewing an article of clothing.\nGetting embedding for A man is playing an instrument.\nGetting embedding for A couple carrying a child are walking along water.\nGetting embedding for Small blond-haired girl drinking a glass of juice.\nGetting embedding for People going for a long walk to the mountains.\nGetting embedding for A man stares at a passing couple.\nGetting embedding for Two men are cooking in the kitchen using rice milk.\nGetting embedding for two female medical personnel read their charts.\nGetting embedding for City people in street\nGetting embedding for BMX biker jumps over a ravine.\nGetting embedding for A group watches a practice.\nGetting embedding for A child with a yellow cup and milk all over his face.\nGetting embedding for A spotted black and white dog splashes in the water.\nGetting embedding for The three men are outside.\nGetting embedding for A man and a woman are outside.\nGetting embedding for Cheerleaders cheering.\nGetting embedding for People in a meeting setting paying attention to a speaker in an orange shirt.\nGetting embedding for A child swimming.\nGetting embedding for A large group, wearing pink shirts, waves to onlookers.\nGetting embedding for A dog is preparing to run away from a person interacting with it.\nGetting embedding for A couple of people are loading brush onto a trailer that is attached to a truck.\nGetting embedding for A man hangs a poster in front of a shop.\nGetting embedding for A group of young girls playing jump rope in the street.\nGetting embedding for The skier is wearing a yellow jumpsuit and sliding across a yellow rail.\nGetting embedding for A male painting a scene in front of him.\nGetting embedding for A meeting of young people sitting at a conference table.\nGetting embedding for A person on a horse jumps over a broken down airplane.\nGetting embedding for There are two people in this picture\nGetting embedding for A man is hanging a picture of a child.\nGetting embedding for Three small puppies bite and play together in the grass.\nGetting embedding for A man with a bright green shirt is talking to a woman in a pink shirt.\nGetting embedding for A child with a yellow cup and milk all over his face.\nGetting embedding for A violinist among other string musicians\nGetting embedding for There are cars driving down a street with a sign displaying Welcome to Golden.\nGetting embedding for The couple danced.\nGetting embedding for A couple playing with a little boy on the beach.\nGetting embedding for A human is riding a vehicle.\nGetting embedding for A man sitting in a barber shop.\nGetting embedding for A couple of people are loading brush onto a trailer that is attached to a truck.\nGetting embedding for Five men, one wearing a white shirt standing on something, hanging up a picture of a child.\nGetting embedding for A couple is holding a child\nGetting embedding for A dog on the beach.\nGetting embedding for An oddly dressed man pushing a stroller down a sidewalk in a park.\nGetting embedding for Three wheelchair basketball players wearing team uniforms are attempting to reach the descending basketball with other players in the background.\nGetting embedding for Boy in costume followed by a girl in costume.\nGetting embedding for A man carrying a load of fresh direct boxes on car with wheels in the city streets, as a woman walks towards him.\nGetting embedding for A violinist among other string musicians\nGetting embedding for Two people loading brush on a trailer attached to a truck.\nGetting embedding for Asian city scene of people in street with bright lights and glass buildings behind.\nGetting embedding for A white bike is leaning against a post.\nGetting embedding for Three men standing on grass by the water looking at something on a table.\nGetting embedding for The woman is outdoors with a machine.\nGetting embedding for There are soccer players on the field.\nGetting embedding for A big brown dog swims towards the camera.\nGetting embedding for A man carrying a load of fresh direct boxes on car with wheels in the city streets, as a woman walks towards him.\nGetting embedding for The dogs are playing with a ball.\nGetting embedding for A groom and bride are standing outside.\nGetting embedding for A pregnant lady shares the sounds of her pregnancy.\nGetting embedding for A black-and-white dog carries a stick in his mouth as he swims in the clear water.\nGetting embedding for A boy painting a log.\nGetting embedding for A person is sitting down.\nGetting embedding for A girl wearing a dress is blowing bubbles at a dock.\nGetting embedding for People are looking at sculptures at a museum.\nGetting embedding for A male painting a scene in front of him.\nGetting embedding for Soccer teams are competing outdoors.\nGetting embedding for A man sitting on a scooter on the curb.\nGetting embedding for A man is singing into a microphone.\nGetting embedding for A person on a horse jumps over a broken down airplane.\nGetting embedding for A woman is lying down.\nGetting embedding for A man wearing a red sweater is sitting on a car bumper watching another person work.\nGetting embedding for A couple is holding a child\nGetting embedding for Someone is making music.\nGetting embedding for A child has milk on their face.\nGetting embedding for An old couple dance in by a juke box while a dude wearing shorts sleeps near a table\nGetting embedding for The school children head home.\nGetting embedding for A boy sits at peoples feet.\nGetting embedding for A couple playing with a little boy on the beach.\nGetting embedding for A dog carries a stick in his mouth.\nGetting embedding for A group watches a practice.\nGetting embedding for A child with a yellow cup and milk all over his face.\nGetting embedding for Two girls, each in a dress walking together.\nGetting embedding for A blond woman with two children is checking out at a Walmart register.\nGetting embedding for A woman gazes skyward\nGetting embedding for People going for a long walk to the mountains.\nGetting embedding for A boat worker securing line.\nGetting embedding for Someone is toting packages in an urban setting.\nGetting embedding for A woman is there.\nGetting embedding for A mother with her four children.\nGetting embedding for A girl is running on the trail.\nGetting embedding for The man is running.\nGetting embedding for A small white dog running on a pebble covered beach.\nGetting embedding for A man is being moved.\nGetting embedding for There is a woman in a room.\nGetting embedding for A group of men are hanging a picture on a wall.\nGetting embedding for A child stoops to pick up a watermelon from a large pile of them.\nGetting embedding for A track event held by J.P. Morgan Chase with security.\nGetting embedding for A young lady is looking at her camera.\nGetting embedding for A woman standing behind a grill outside with a blue basket of food in her hands.\nGetting embedding for An average looking man is playing the guitar.\nGetting embedding for A barber waiting for customers.\nGetting embedding for A boy is in a boat.\nGetting embedding for A group sits outside while talking.\nGetting embedding for A man holding a green bowling ball stands by the ball return machine in a bowling alley.\nGetting embedding for Boy in costume followed by a girl in costume.\nGetting embedding for A little boy swimming underwater with a toy in his hand.\nGetting embedding for Men fish on a concrete slab.\nGetting embedding for There are lots of cars on the street.\nGetting embedding for A child plays on a playground.\nGetting embedding for Busy Japanese intersection like maybe Tokyo.\nGetting embedding for A young woman packs belongings into a black luggage carrier.\nGetting embedding for A dog swims towards the camera.\nGetting embedding for An animal is jumping in a place that is not hot.\nGetting embedding for A child with a yellow cup and milk all over his face.\nGetting embedding for A couple is talking.\nGetting embedding for A dog is outside playing in the water.\nGetting embedding for Two guys cook using some rice milk.\nGetting embedding for Soccer players on a field from a distance.\nGetting embedding for A man is putting up a poster in front of a shop.\nGetting embedding for A group of people sitting at a table outside talking.\nGetting embedding for A dog swims towards the camera.\nGetting embedding for A man in a gold skirt sits in front of the computer.\nGetting embedding for A man parasails in the choppy water.\nGetting embedding for A group of people are playing a game of soccer.\nGetting embedding for Four people are acting behind a woman in a yellow shirt is lying on the ground.\nGetting embedding for Choir singing in mass.\nGetting embedding for A woman throws something for a dog.\nGetting embedding for Pitcher is winding up a throw\nGetting embedding for A couple are playing with a young child outside.\nGetting embedding for Woman in white in foreground and a man slightly behind walking with a sign for John's Pizza and Gyro in the background.\nGetting embedding for Two pre-teen girls listening to music on an MP3 player with headphones.\nGetting embedding for A child is dancing.\nGetting embedding for A soccer game played by a blue team and a red team on a perfectly manicured field at dusk.\nGetting embedding for Mothers with children talking at a gathering.\nGetting embedding for An oddly dressed man pushing a stroller down a sidewalk in a park.\nGetting embedding for The bicycles are on a road.\nGetting embedding for A woman laughs while another paddles a boat.\nGetting embedding for A man is sitting down.\nGetting embedding for A barber waiting for customers.\nGetting embedding for A child swimming.\nGetting embedding for A group of people are playing a game of soccer.\nGetting embedding for A woman talks to two other women and a man with notepads in an office building with large windows.\nGetting embedding for A man is playing the guitar.\nGetting embedding for A woman talks to two other women and a man with notepads in an office building with large windows.\nGetting embedding for There are people in an urban area.\nGetting embedding for A woman in a white dress with a tiara sings in a chorus, which has a row of men in sailor hats.\nGetting embedding for A man is using his computer while seated at a desk.\nGetting embedding for Woman at Walmart check-out having her groceries bagged by an employee.\nGetting embedding for A boat worker securing line.\nGetting embedding for The man knows how to play guitar.\nGetting embedding for A woman sitting in a laundromat looking at the camera.\nGetting embedding for Firemen emerge from a subway station.\nGetting embedding for A boy painting a log.\nGetting embedding for A dog swims towards the camera.\nGetting embedding for A lady at a table takes pictures.\nGetting embedding for A man is decorating a shop.\nGetting embedding for A man and woman walk on a street.\nGetting embedding for Three people are sitting on a bench.\nGetting embedding for A man in a tan suit is using a pay phone to make a call.\nGetting embedding for A gentleman in a purple scarf and hat is looking at money while holding an accordion.\nGetting embedding for A soccer game is happening.\nGetting embedding for A Little League team tries to catch a runner sliding into a base in an afternoon game.\nGetting embedding for A good-looking firefighter sets up \"Do Not Cross\" tape in the city.\nGetting embedding for A woman is in Walmart\nGetting embedding for A man with a gray shirt holds a young infant in his hands.\nGetting embedding for A man is outside, near the street.\nGetting embedding for A man is wearing many silver necklaces.\nGetting embedding for A street vendor in Asia tries to bring in more customers.\nGetting embedding for People waiting at a light on bikes.\nGetting embedding for A woman with a green sweater has a happy expression.\nGetting embedding for The man is pushing a stroller.\nGetting embedding for Bikers stop and wait for traffic at the intersection.\nGetting embedding for A man hangs a poster in front of a shop.\nGetting embedding for A group of people are playing a game of soccer.\nGetting embedding for The furry brown dog is swimming in the ocean.\nGetting embedding for A blond child is pulling a wagon with a little blond boy in it.\nGetting embedding for A group of tourist waiting for a train at a train station.\nGetting embedding for A person on skis on a rail at night.\nGetting embedding for A model posing to look as if she's a real female soccer player.\nGetting embedding for A couple holding hands walks down a street.\nGetting embedding for A person rides a bike outdoors.\nGetting embedding for A man and two women in black jackets holding umbrellas sit on a long wooden bench.\nGetting embedding for A brown dog running with two white and brown dogs on the seashore with crashing waves behind them.\nGetting embedding for The cart is filled to the top.\nGetting embedding for A dog is in the water.\nGetting embedding for A boat worker securing line.\nGetting embedding for A dog is outside\nGetting embedding for A group of people are playing a game of soccer.\nGetting embedding for People standing near sculptures\nGetting embedding for A person eating.\nGetting embedding for A mother with her four children.\nGetting embedding for A foreign family is walking along a dirt path next to the water.\nGetting embedding for An oddly dressed man pushing a stroller down a sidewalk in a park.\nGetting embedding for A man with a gray shirt holds a young infant in his hands.\nGetting embedding for A brown dog running with two white and brown dogs on the seashore with crashing waves behind them.\nGetting embedding for Indian lady and a guy in a blue suit dancing in the sunlight.\nGetting embedding for People in a meeting setting paying attention to a speaker in an orange shirt.\nGetting embedding for A Ford is being driven on a track.\nGetting embedding for A man walks near a store\nGetting embedding for A competition is happening.\nGetting embedding for A climber is making his way up a snowy mountainside.\nGetting embedding for A little boy swimming underwater with a toy in his hand.\nGetting embedding for A soccer game played by a blue team and a red team on a perfectly manicured field at dusk.\nGetting embedding for A white dog runs along a rocky shoreline.\nGetting embedding for A woman throws something for a dog.\nGetting embedding for A man hangs a poster in front of a shop.\nGetting embedding for A crowded street, in an Asian country, where the buildings are dominated by the Seiko building.\nGetting embedding for A couple playing with a little boy on the beach.\nGetting embedding for 2 women are carrying little girls.\nGetting embedding for A person is painting.\nGetting embedding for A man is wearing a bright green shirt\nGetting embedding for Two Asian people sit at a blue table in a food court.\nGetting embedding for A woman is sitting at an outdoor dining table.\nGetting embedding for A child has milk on their face.\nGetting embedding for A man wearing a blue shirt is sitting on a brick planter next to some paintings.\nGetting embedding for A dog in the water carries a stick in his mouth.\nGetting embedding for A man wearing black is playing an electric guitar at a concert.\nGetting embedding for 2 women are carrying little girls.\nGetting embedding for A woman wearing orange looking upward.\nGetting embedding for A bike it outside\nGetting embedding for A woman is filling a suitcase.\nGetting embedding for A man is advertising for a restaurant.\nGetting embedding for A man wearing a red sweater is sitting on a car bumper watching another person work.\nGetting embedding for A meeting of young people sitting at a conference table.\nGetting embedding for A bunch of people playing soccer.\nGetting embedding for A couple is talking.\nGetting embedding for A man is wearing many silver necklaces.\nGetting embedding for People are paying attention to the person hanging pictures\nGetting embedding for A little boy is underwater.\nGetting embedding for A small white dog running on a pebble covered beach.\nGetting embedding for A soccer game is happening.\nGetting embedding for A man with a bright green shirt is talking to a woman in a pink shirt.\nGetting embedding for A group of people are possing for an add.\nGetting embedding for A woman is lying down.\nGetting embedding for A man wearing blue jeans and red bowling shoes stands in a bowling alley lane with a green ball in his hand.\nGetting embedding for A man with a beard, wearing a red shirt with gray sleeves and work gloves, pulling on a rope.\nGetting embedding for A person is performing.\nGetting embedding for A man being photographed while he does a trick on his skateboard down the stairs.\nGetting embedding for The people are near the table.\nGetting embedding for A young lady is looking at her camera.\nGetting embedding for A man and woman walk on a street.\nGetting embedding for A woman talks to others indoors.\nGetting embedding for A girl is running on the trail.\nGetting embedding for A gray-haired woman in a blue dress coat with snowflakes on it balances something on her head.\nGetting embedding for A girl is dancing in a brown shirt and blue jean skirt on a wooden floor\nGetting embedding for A climber is ascending\nGetting embedding for A woman gazes skyward\nGetting embedding for A man wearing a white shire shirt and hate is riding a bucking horse in a rodeo.\nGetting embedding for A woman in a green jacket and hood over her head looking towards a valley.\nGetting embedding for A dog runs across the beach.\nGetting embedding for A couple, who appear to be Indian or Pakistani, walk on a path beside a body of water, the mother carrying a child in a diaper, the father wrapped in a blanket with the logo of the humanitarian organization \"Save the Children.\"\nGetting embedding for A human is riding a vehicle.\nGetting embedding for Group of people dancing\nGetting embedding for A woman talks to others indoors.\nGetting embedding for A man with a bright green shirt is talking to a woman in a pink shirt.\nGetting embedding for A young boy paddles across the water in a makeshift boat.\nGetting embedding for A biker is doing jumps by trees.\nGetting embedding for A guy playing a board game by himself.\nGetting embedding for Someone is browsing jewelry.\nGetting embedding for A Little League team tries to catch a runner sliding into a base in an afternoon game.\nGetting embedding for Woman wearing a shirt sewing.\nGetting embedding for A man is drinking juice.\nGetting embedding for A little boy drinks milk and gets milk all over his face and table.\nGetting embedding for The furry brown dog is swimming in the ocean.\nGetting embedding for The dogs run and play with a red ball.\nGetting embedding for A little kid enjoying some sledding on a winter day.\nGetting embedding for There are four Eastern guys working on and hanging pictures of humans.\nGetting embedding for Busy Japanese intersection like maybe Tokyo.\nGetting embedding for The people are all jumping into a body of water.\nGetting embedding for The man is holding a balloon.\nGetting embedding for A Ford is being driven on a track.\nGetting embedding for A groom and a bride are standing on the grass with his hand on her waist.\nGetting embedding for J.P. Morgan Chase held a track event.\nGetting embedding for A man in a red and black jacket, blue shirt, lots of silver necklaces, and his blue jeans falling down, checks out a woman wearing a black leather jacket, yellow bra, pink fingerless gloves, and sunglasses.\nGetting embedding for A woman is wearing a red jacket\nGetting embedding for Older couple posing for a picture in front of a fountain.\nGetting embedding for A couple is talking.\nGetting embedding for A woman wearing orange looking upward.\nGetting embedding for A violinist among other string musicians\nGetting embedding for A woman is sitting at an outdoor dining table.\nGetting embedding for The women work in the health field.\nGetting embedding for A man is being moved.\nGetting embedding for A young Asian man sits behind a set chessboard waiting for the other player to arrive.\nGetting embedding for A woman with a green sweater has a happy expression.\nGetting embedding for Busy ChinaTown street corner where people are walking past an open front store.\nGetting embedding for Toddler in striped sweatshirt plays on rope on playground.\nGetting embedding for Soccer teams are competing outdoors.\nGetting embedding for A man is being moved.\nGetting embedding for A man sits at a desk.\nGetting embedding for A woman on the side of a street is making food on her cart.\nGetting embedding for A man sitting in a barber shop.\nGetting embedding for Cheerleaders are on the field cheering.\nGetting embedding for Three men are outside.\nGetting embedding for A man is sitting down.\nGetting embedding for A man riding a dirt bike\nGetting embedding for A child plays on a playground.\nGetting embedding for A big brown dog swims towards the camera.\nGetting embedding for A big brown dog swims towards the camera.\nGetting embedding for A couple carrying a child are walking along water.\nGetting embedding for A lady is close to the floor.\nGetting embedding for A human standing.\nGetting embedding for Asian city scene of people in street with bright lights and glass buildings behind.\nGetting embedding for A man sitting on a scooter on the curb.\nGetting embedding for Somebody is hanging up pictures while people watch.\nGetting embedding for People are on their bikes.\nGetting embedding for A dog jumps to catch a toy in the snow.\nGetting embedding for A woman on the side of a street is making food on her cart.\nGetting embedding for A bearded man is pulling on a rope.\nGetting embedding for A bunch of people playing soccer.\nGetting embedding for A cowboy Roping a calf in a rodeo.\nGetting embedding for A gray-haired woman in a blue dress coat with snowflakes on it balances something on her head.\nGetting embedding for He is wearing a multicolored shirt\nGetting embedding for People are looking at sculptures at a museum.\nGetting embedding for A man is being moved.\nGetting embedding for A boy sits at peoples feet.\nGetting embedding for A man wearing a white shire shirt and hate is riding a bucking horse in a rodeo.\nGetting embedding for A female violinist surrounded by other violinists.\nGetting embedding for A woman is there.\nGetting embedding for A man stands in a bowling alley lane.\nGetting embedding for The Arsenal football club warms-up on the soccer field as a few fans watch.\nGetting embedding for A couple of people are loading brush onto a trailer that is attached to a truck.\nGetting embedding for A boat worker securing line.\nGetting embedding for A man being airlifted to safety after being in danger.\nGetting embedding for A little boy swims underwater.\nGetting embedding for A male painting a scene in front of him.\nGetting embedding for A guy playing a board game by himself.\nGetting embedding for A person on skis on a rail at night.\nGetting embedding for Asian city scene of people in street with bright lights and glass buildings behind.\nGetting embedding for A baseball player is about to throw a baseball.\nGetting embedding for A group of tourist waiting for a train at a train station.\nGetting embedding for A track event held by J.P. Morgan Chase with security.\nGetting embedding for A woman in costume is marching with a large drum.\nGetting embedding for An english farmer with a horse pulled wagon.\nGetting embedding for A family by a van.\nGetting embedding for A girl is dancing in a brown shirt and blue jean skirt on a wooden floor\nGetting embedding for A man sitting on a scooter on the curb.\nGetting embedding for Children playing a game in a field.\nGetting embedding for Bicyclists waiting at an intersection.\nGetting embedding for A man and a woman are standing next to sculptures, talking while another man looks at other sculptures.\nGetting embedding for A boy in a blue, yellow, and orange shirt plays outside.\nGetting embedding for Workers are taking a break during midday.\nGetting embedding for A woman in colorful garb with her back to the camera and cloth on her hear.\nGetting embedding for A man in a bright green shirt shows a woman in a bright pink shirt something on a clipboard.\nGetting embedding for A man in a green shirt holds out a clipboard for a woman in pink's attention.\nGetting embedding for A boy in a blue, yellow, and orange shirt holding his arms out from his sides.\nGetting embedding for Child in red and blue shirt painting a log.\nGetting embedding for Two people with bicycles, one in front running with a bike and one in back riding.\nGetting embedding for Two barefoot men are playing on a green lawn outside a building with other people in the background.\nGetting embedding for A soccer game where the team in yellow is attempting to advance past the team in white towards the goalie wearing a black top and blue shorts.\nGetting embedding for A foreign family is walking along a dirt path next to the water.\nGetting embedding for A man in a bright green shirt shows a woman in a bright pink shirt something on a clipboard.\nGetting embedding for Four people near a body of water, one sitting and three standing, while two people walk on a nearby sidewalk.\nGetting embedding for a man wearing a multicolored striped shirt playing the guitar on the street\nGetting embedding for Man wearing black t-shirt sitting at a computer desk.\nGetting embedding for A person is hanging up pictures of women with a few onlookers watching surrounded by bikes.\nGetting embedding for A man riding a dirt bike\nGetting embedding for A white and brown dog is leaping through the air.\nGetting embedding for A group of people are sitting around a table under a blue sunshade.\nGetting embedding for Woman balancing on edge of fountain while sticking her toe in the water.\nGetting embedding for Overlooking a street with a sign above shops that states Welcome To Golden.\nGetting embedding for A blond man is drinking from a public fountain.\nGetting embedding for Two women who just had lunch hugging and saying goodbye.\nGetting embedding for Toddler with milk around his mouth.\nGetting embedding for A man with a red shirt is watching another man who is standing on top of a attached cart filled to the top.\nGetting embedding for Child in red and blue shirt painting a log.\nGetting embedding for Asian city scene of people in street with bright lights and glass buildings behind.\nGetting embedding for The boy in the blue and yellow top is standing with arms outstretched.\nGetting embedding for A person in an orange shirt is laying on the ground while others are standing around her, smiling.\nGetting embedding for A man windsurfs in a wetsuit.\nGetting embedding for A young boy with a blue coat makes a funny face as he walks towards the grass.\nGetting embedding for A woman sitting at a table, taking a picture.\nGetting embedding for A motorcycle racer is in action at the track.\nGetting embedding for The red team knocked the ball toward the goal and the black team tried to block it.\nGetting embedding for A guy performing a bicycle jump trick for an audience.\nGetting embedding for Three firefighters, the nearest firefighter is holding a helmet in his left hand.\nGetting embedding for A small girl stands among many large watermelons.\nGetting embedding for A woman checks her purse while at a outside cafe.\nGetting embedding for Six soccer players on field with player in red uniform in the air and ball airborne.\nGetting embedding for Cheerleaders are doing a cheer at a football field.\nGetting embedding for People sitting down to eat.\nGetting embedding for People are on a stage performing.\nGetting embedding for A woman holding a boombox.\nGetting embedding for Small laughing child with blond-hair sitting at a table holding a green sippy cup.\nGetting embedding for One soccer team is playing against another.\nGetting embedding for An elderly woman places carrots into a casserole.\nGetting embedding for A man is putting up a poster in front of a shop.\nGetting embedding for Two dogs playfully bite a third dog, which has its tongue sticking out.\nGetting embedding for A large golden dog sniffing the butt of a white dog\nGetting embedding for Woman in white in foreground and a man slightly behind walking with a sign for John's Pizza and Gyro in the background.\nGetting embedding for A Land Rover makes its way through a deep pond.\nGetting embedding for People are stretching on yoga mats.\nGetting embedding for A small boy has gotten into the cabinet and gotten flour and crisco all over himself.\nGetting embedding for a man with a white covering is walking up a flight of stairs.\nGetting embedding for A couple of people working around a pile of rocks.\nGetting embedding for Two children are running down a sidewalk dressed in costumes.\nGetting embedding for A man and two women sitting on a bench.\nGetting embedding for Young lady dressed in black shorts and light blue shirt sitting outside at a public table looking at a picture on her camera with her left hand on her face.\nGetting embedding for Three men, two wearing yellow suits, are looking in the back of a car.\nGetting embedding for Children's soccer game being played while the sun sets in the background.\nGetting embedding for An old man wearing khaki pants and a brown shirt standing on the sidewalk in front of a building.\nGetting embedding for A white horse is pulling a cart while a man stands and watches.\nGetting embedding for A group of people are doing yoga.\nGetting embedding for A woman in a floral dress talks to children in front of a van.\nGetting embedding for Three young children consisting of two girls and a boy who is holding an apple with a bite out of it, are posing on a scenic mountain view background.\nGetting embedding for A man wearing a colorful and striped sweater plays music in the street.\nGetting embedding for Two children play outside in a field.\nGetting embedding for A man in a blue shirt sits outside alone with a chessboard laid out in front of him.\nGetting embedding for a motorcyclist does a nose wheelie.\nGetting embedding for People are fishing and walking next to the water.\nGetting embedding for A man squatting in the foreground of a photograph while taking a photograph of his own of a man doing a skateboarding kick flip in midair above a short flight of stairs outdoors.\nGetting embedding for A group of adults is having a discussion at a table under a tent.\nGetting embedding for A boy in a blue, yellow, and orange shirt holding his arms out from his sides.\nGetting embedding for A woman with a black jacket walks past an outdoor movie poster.\nGetting embedding for People walking around in a big city.\nGetting embedding for People in line for plates of rice.\nGetting embedding for Three men are grouped around the back of a car with its tailgate out, two of the men clothed in yellow uniforms and one in blue.\nGetting embedding for The cowboy waves to the rodeo crowd.\nGetting embedding for two men serving preparing food.\nGetting embedding for A dog zips along the beach.\nGetting embedding for a woman on a yellow shirt is on the floor.\nGetting embedding for many people relax in the yard.\nGetting embedding for A group of people gathers on the grass in a backyard with tents, tables, and chairs set up.\nGetting embedding for A man dressed in snow-gear takes a leap into a snow-covered ravine.\nGetting embedding for A cowboy is riding a bucking bull in a rodeo arena.\nGetting embedding for A man with wild hair rocks a show playing a guitar center stage.\nGetting embedding for Three construction workers posing with construction materials.\nGetting embedding for three bikers stop in town.\nGetting embedding for A woman wearing all white and eating, walks next to a man holding a briefcase.\nGetting embedding for Two adults, one female in white, with shades and one male, gray clothes, walking across a street, away from a eatery with a blurred image of a dark colored red shirted person in the foreground.\nGetting embedding for Five people on stage performing and acting while girl lay's on belly.\nGetting embedding for Two people are next to a fountain with a red bottom and arches of water.\nGetting embedding for a woman with a straw hat working on a strange machine with coconuts at her side.\nGetting embedding for Asian school children sitting on each others shoulders.\nGetting embedding for A crowded street, in an Asian country, where the buildings are dominated by the Seiko building.\nGetting embedding for Asian city scene of people in street with bright lights and glass buildings behind.\nGetting embedding for Students practicing yoga in a class setting.\nGetting embedding for Two soccer teams are competing on a soccer field.\nGetting embedding for People on bicycles waiting at an intersection.\nGetting embedding for An Asian woman in a blue top and green headscarf smiling widely as another woman rows a boat in the background.\nGetting embedding for Two guys playing football on a campus green.\nGetting embedding for A good-looking firefighter sets up \"Do Not Cross\" tape in the city.\nGetting embedding for Girl is blowing to a butterfly.\nGetting embedding for a child is pushing another kid in a wheeler dressed in a red top and wearing a cap.\nGetting embedding for A classroom of students discussing lecture.\nGetting embedding for A group of people point forwards while performing some kind of act.\nGetting embedding for A white bike is tied to a street sign.\nGetting embedding for A guy stands with a green bowling ball in his hand, and looks down the bowling lane.\nGetting embedding for People in orange vests and blue pants with a yellow stripe at the bottom await the beginning of a race.\nGetting embedding for A crowded city during daytime.\nGetting embedding for A fireman protects an area by setting up a boundary while others watch.\nGetting embedding for A lady with sunglasses on her head and a green sweatshirt is looking off-camera.\nGetting embedding for A man is running behind a dogsled being pulled by four dogs.\nGetting embedding for A child in formal clothing is walking along the edge of a stony area that is littered in places.\nGetting embedding for Some children are playing jump rope.\nGetting embedding for An older man dressed in blue historical clothing is ringing a bell in his right hand.\nGetting embedding for A young girl sitting at a table with a bowl on her head\nGetting embedding for Brown dog treads through water.\nGetting embedding for Four people near a body of water, one sitting and three standing, while two people walk on a nearby sidewalk.\nGetting embedding for A man in a bright green shirt shows a woman in a bright pink shirt something on a clipboard.\nGetting embedding for The silhouette of three people in front of a wall.\nGetting embedding for Three people stand proudly by a truck stocked with building supplies in the street.\nGetting embedding for A farmer fertilizing his garden with manure with a horse and wagon.\nGetting embedding for The school is having a special event in order to show the american culture on how other cultures are dealt with in parties.\nGetting embedding for Young people playing with a long jump rope in the street.\nGetting embedding for Young lady dressed in black shorts and light blue shirt sitting outside at a public table looking at a picture on her camera with her left hand on her face.\nGetting embedding for A saddle bronc rider gets lifted out of the saddle, but keeps his grip during his ride.\nGetting embedding for A boy is jumping on skateboard in the middle of a red bridge.\nGetting embedding for A man doing tricks in the snow.\nGetting embedding for A family with a baby, the father is wearing a save the children sign.\nGetting embedding for A woman preparing to glaze a bowl.\nGetting embedding for The man wearing lots of medals is watching the girl in the yellow bikini top.\nGetting embedding for Three working men smile in front of a truck while holding construction equipment.\nGetting embedding for A boy is drinking out of a water fountain shaped like a woman.\nGetting embedding for A group of people stand on a grassy field.\nGetting embedding for Outside by the trees, a woman wearing jeans and red jacket throws something for a German shepherd to chase.\nGetting embedding for A lady wearing a blue print shirt and green head cover smiles for the camera.\nGetting embedding for A seated woman with short hair and a camera throws a Frisbee to a brown dog.\nGetting embedding for A man in a blue shirt is looking up.\nGetting embedding for Three young children consisting of two girls and a boy who is holding an apple with a bite out of it, are posing on a scenic mountain view background.\nGetting embedding for Four guys in wheelchairs on a basketball court two are trying to grab a basketball in midair.\nGetting embedding for A man wearing a tan coat signs papers for another man wearing a blue coat.\nGetting embedding for A black and white dog with a stick in its mouth is swimming.\nGetting embedding for Two adults, one female in white, with shades and one male, gray clothes, walking across a street, away from a eatery with a blurred image of a dark colored red shirted person in the foreground.\nGetting embedding for A foreign family is walking along a dirt path next to the water.\nGetting embedding for A pirate is chasing a princess down the sidewalk.\nGetting embedding for A doctor in blue scrubs is performing an operation assisted by two men and a woman.\nGetting embedding for A man in shorts and a white garment stands at the base of stairs framed by black railing.\nGetting embedding for A soccer game where the team in yellow is attempting to advance past the team in white towards the goalie wearing a black top and blue shorts.\nGetting embedding for A man wearing a rice hat is shucking corn using a corn shucker and is surrounded by trees.\nGetting embedding for A person is a red hat and winter jacket is looking into the distance.\nGetting embedding for A man is sleeping on the grass.\nGetting embedding for Street performer in colorful shirt performing with small guitar.\nGetting embedding for Two men trying to build something together, while having fun.\nGetting embedding for A woman and a girl are playing in a field of leaves\nGetting embedding for A man and a woman are standing next to sculptures, talking while another man looks at other sculptures.\nGetting embedding for A lady is on the floor packing a suitcase.\nGetting embedding for A woman in a blue shirt is sitting at a table and looking at her cellphone.\nGetting embedding for Bicyclists waiting at an intersection.\nGetting embedding for Two people pose for the camera.\nGetting embedding for The parents of the younger male are posing for a picture in front of a water fountain.\nGetting embedding for Toddler with milk around his mouth.\nGetting embedding for A couple play in the tide with their young son.\nGetting embedding for A foreign family is walking along a dirt path next to the water.\nGetting embedding for A woman in capri jeans crouches on the edge of a fountain with her left foot kicked out to touch the falling water.\nGetting embedding for People waiting to get on a train or just getting off.\nGetting embedding for A couple walk through a white brick town.\nGetting embedding for A car sinking in water.\nGetting embedding for Man on the sidewalk sitting on a motorcycle.\nGetting embedding for Gray dog running down pavement toward laundry line in courtyard.\nGetting embedding for Two people enjoying a water fountain display.\nGetting embedding for Toddler with milk around his mouth.\nGetting embedding for A young woman frolicking on the lawn in front of the us capitol building.\nGetting embedding for An older man stands on the sidewalk painting the view.\nGetting embedding for Woman in white in foreground and a man slightly behind walking with a sign for John's Pizza and Gyro in the background.\nGetting embedding for A young toddler wearing pink sandals is walking on hopscotch numbers.\nGetting embedding for Two children in hats play in an open, rocky field.\nGetting embedding for Two large dogs greet other while their owners watch.\nGetting embedding for Outside by the trees, a woman wearing jeans and red jacket throws something for a German shepherd to chase.\nGetting embedding for A woman talks to two other women and a man with notepads in an office building with large windows.\nGetting embedding for A woman wearing a green and pink dress is dancing with someone wearing a blue top with white pants.\nGetting embedding for Two older men in coats are standing outside.\nGetting embedding for A man wearing a blue shirt screaming or yelling with his arms raised up in the air.\nGetting embedding for A man and a woman cross the street in front of a pizza and gyro restaurant.\nGetting embedding for People in orange vests and blue pants with a yellow stripe at the bottom await the beginning of a race.\nGetting embedding for A man with a red shirt is watching another man who is standing on top of a attached cart filled to the top.\nGetting embedding for Several men sit outside on brick ledges built around tall trees.\nGetting embedding for A soccer player jumping in air during a game.\nGetting embedding for A man with blond-hair, and a brown shirt drinking out of a public water fountain.\nGetting embedding for A mountain biker jumping a slope outdoors in a forest area.\nGetting embedding for Man in gold pants looking at the camera.\nGetting embedding for A man and a woman having an intimate conversation in front a statue.\nGetting embedding for A man squatting in the foreground of a photograph while taking a photograph of his own of a man doing a skateboarding kick flip in midair above a short flight of stairs outdoors.\nGetting embedding for A man with a beard, wearing a red shirt with gray sleeves and work gloves, pulling on a rope.\nGetting embedding for Four people near a body of water, one sitting and three standing, while two people walk on a nearby sidewalk.\nGetting embedding for A man squatting in the foreground of a photograph while taking a photograph of his own of a man doing a skateboarding kick flip in midair above a short flight of stairs outdoors.\nGetting embedding for Exhausted looking firemen are walking.\nGetting embedding for A man dressed in warm clothing sleds behind four dogs in the snow.\nGetting embedding for The blond girl is dancing inside a house.\nGetting embedding for A woman wearing an apron inspects a large pot on a table filled with cups, bowls, pots and baskets of assorted size.\nGetting embedding for People waiting at a light on bikes.\nGetting embedding for Workers are eating a meal while one man sits on a pile of plywood.\nGetting embedding for Black dog jumping into the air to catch a toy in the snow.\nGetting embedding for Three young children consisting of two girls and a boy who is holding an apple with a bite out of it, are posing on a scenic mountain view background.\nGetting embedding for New sport is being played to show appreciation to the kids who can not walk.\nGetting embedding for A few people in a restaurant setting, one of them is drinking orange juice.\nGetting embedding for A lady in a black and white striped shirt and holding a bouquet of flowers, looks seriously at two gentlemen talking on the steps.\nGetting embedding for A foreign family is walking along a dirt path next to the water.\nGetting embedding for A black dog in snow is jumping off the ground to catch a stick.\nGetting embedding for A little girl picking up a watermelon from a pile.\nGetting embedding for A man wearing black with a gray hat, holding a pitchfork, directs a horse-drawn cart.\nGetting embedding for People on bicycles waiting at an intersection.\nGetting embedding for People relax around a large community fountain in a park.\nGetting embedding for One man sits inside and plays the banjo, there are trees behind him outside.\nGetting embedding for A child using a woodworking tool\nGetting embedding for a lone person jumping through the air from one snowy mountain to another.\nGetting embedding for A little boy in a pirate costume is running behind a little girl in a princess costume carrying an orange pumpkin along the sidewalk.\nGetting embedding for Bicyclists waiting at an intersection.\nGetting embedding for A man dressed in blue shirt and shorts sits at a table while playing black in chess.\nGetting embedding for Cheerleaders are on the field cheering.\nGetting embedding for A windsurfer is balancing on choppy water.\nGetting embedding for A soccer game where the team in yellow is attempting to advance past the team in white towards the goalie wearing a black top and blue shorts.\nGetting embedding for Child in red and blue shirt painting a log.\nGetting embedding for A man holds a clipboard and a pen as a woman looks at them.\nGetting embedding for a man wearing blue plays soccer.\nGetting embedding for A man with a beard, wearing a red shirt with gray sleeves and work gloves, pulling on a rope.\nGetting embedding for A woman playing the violin with sunglasses on her head.\nGetting embedding for Biker riding dirt bike on dirt track\nGetting embedding for A little girl follows two guys with umbrellas down a path.\nGetting embedding for A boy in a blue, yellow, and orange shirt holding his arms out from his sides.\nGetting embedding for A man is sitting on a motorcycle on the sidewalk.\nGetting embedding for A woman talking to four little children outside.\nGetting embedding for A man riding a dirt bike\nGetting embedding for A person is hanging up pictures of women with a few onlookers watching surrounded by bikes.\nGetting embedding for A kite surfer begins to fall in the ocean.\nGetting embedding for woman and child on trolley car labeled Powell and market and bay and taylor\nGetting embedding for three bikers stop in town.\nGetting embedding for A man in a kitchen is frying breaded food in a cast iron pan.\nGetting embedding for A man wearing a gray cap is looking down.\nGetting embedding for A woman in blue jeans and a dark jacket walks in front of a building.\nGetting embedding for People in orange vests and blue pants with a yellow stripe at the bottom await the beginning of a race.\nGetting embedding for Two people dancing, wearing dance costumes.\nGetting embedding for A man on a street in a bright t-shirt holds some sort of tablet towards a woman in a pink t-shirt and shades.\nGetting embedding for Two blond women are hugging one another.\nGetting embedding for A view of a marketplace full of people in an asian country.\nGetting embedding for A person is hanging up pictures of women with a few onlookers watching surrounded by bikes.\nGetting embedding for An elderly man is drinking orange juice at a cafe.\nGetting embedding for A woman in a black and orange jacket throws a stick for a brown and black dog to fetch.\nGetting embedding for A young man in blue sunglasses walking in front of a red brick building.\nGetting embedding for Two women, holding food carryout containers, hug.\nGetting embedding for Lady wearing a yellow top is sitting on a chair\nGetting embedding for A man riding a dirt bike\nGetting embedding for A woman is walking across the street eating a banana, while a man is following with his briefcase.\nGetting embedding for A man with a beard, wearing a red shirt with gray sleeves and work gloves, pulling on a rope.\nGetting embedding for A group of people are playing soccer and two players from opposing teams are battling for the ball.\nGetting embedding for Cheerleaders are on the field cheering.\nGetting embedding for Two children, in colorful outfits, playing in a field with a big rock in the middle.\nGetting embedding for three bikers stop in town.\nGetting embedding for A woman in a red shirt is speaking at a table in a room where three other people are listening to her.\nGetting embedding for A crowded street, in an Asian country, where the buildings are dominated by the Seiko building.\nGetting embedding for A blue excavator digging a large hold in cement.\nGetting embedding for Two women are talking while children are sitting on their laps.\nGetting embedding for A middle-aged oriental woman in a green headscarf and blue shirt is flashing a giant smile.\nGetting embedding for An adult couple enjoys time in a hot tub.\nGetting embedding for A small girl with a necklace is swimming.\nGetting embedding for A boy looks down and spreads his arms wide\nGetting embedding for The blond woman is searching for medical supplies in a suitcase.\nGetting embedding for A man wearing a striped top and jeans does a skateboard trick on some steps while a man who is hunched over photographs him.\nGetting embedding for Soccer players warm up by kicking the soccer ball around while the crowd waits.\nGetting embedding for A man squatting in the foreground of a photograph while taking a photograph of his own of a man doing a skateboarding kick flip in midair above a short flight of stairs outdoors.\nGetting embedding for A young man in a blue blazer and shorts sits alone in front of table with a chess game set up.\nGetting embedding for A man squatting in the foreground of a photograph while taking a photograph of his own of a man doing a skateboarding kick flip in midair above a short flight of stairs outdoors.\nGetting embedding for An old man wearing khaki pants and a brown shirt standing on the sidewalk in front of a building.\nGetting embedding for A crowded street, in an Asian country, where the buildings are dominated by the Seiko building.\nGetting embedding for People on bicycles waiting at an intersection.\nGetting embedding for A young man wearing a backpack and dark glasses approaches the brick building where there is a bit of graffiti on the wall.\nGetting embedding for A couple strolls arm and arm and hand in hand down a city sidewalk.\nGetting embedding for A white dog running in the backyard.\nGetting embedding for Child in red and blue shirt painting a log.\nGetting embedding for A man wearing a gray sweater walking through a pile of leaves.\nGetting embedding for A man wearing a multi-color coat is playing the guitar on the street.\nGetting embedding for Men are playing soccer, the one in front is about to kick the ball.\nGetting embedding for A man is leading a Clydesdale up a hay road, within a Old Country.\nGetting embedding for A camera crew is filming two women in formal dresses sitting on a blanket in the middle of a park.\nGetting embedding for A man in a blue jacket screaming.\nGetting embedding for In a bowling alley, a man holding a green bowling ball looks ahead at the pins that he must knock down.\nGetting embedding for Young blond woman putting her foot into a water fountain\nGetting embedding for A man wakeboards on choppy water.\nGetting embedding for A man is putting up a poster in front of a shop.\nGetting embedding for Some firefighters check a vehicle.\nGetting embedding for Two dogs biting another dog in a field.\nGetting embedding for A small girl dressed in a yellow dress with flowers on it bends over near a large pile of watermelons.\nGetting embedding for A group of people sitting at some sort of gathering.\nGetting embedding for A man is putting up a poster in front of a shop.\nGetting embedding for Two female medical professionals, one african american&amp; one white, looking over paperwork in a hospital.\nGetting embedding for A picture of a city with a sign welcoming travelers on a busy street.\nGetting embedding for A man with khaki shorts on is holding a little girls hand while she walks in the water of a creek.\nGetting embedding for A man in blue lies on a mostly-barren patch of grass while small groups of people congregate in the distance.\nGetting embedding for A little boy with a blue jacket is making a sour face at the camera.\nGetting embedding for Children smiling and waving at camera\nGetting embedding for A girl wearing a blue shirt, shorts, and sneakers is seated on a stool at a round table, looking at her phone.\nGetting embedding for Outside by the trees, a woman wearing jeans and red jacket throws something for a German shepherd to chase.\nGetting embedding for A person is looking at water jets.\nGetting embedding for Toddler with milk around his mouth.\nGetting embedding for A bearded man in a black t-shirt sits in front of a desk holding a computer.\nGetting embedding for A baby is playing with a strand of beads.\nGetting embedding for Two little kids showing their American pride in their star spangled wagon.\nGetting embedding for People waiting at a light on bikes.\nGetting embedding for White small child wearing a brown and gray striped hoodie plays at park.\nGetting embedding for A smiling lady in a green jacket at a public gathering.\nGetting embedding for A dog is fetching a stick out of very clear water.\nGetting embedding for A man with facial hair and a red and gray shirt tugging on a piece of rope.\nGetting embedding for An elderly couple, both wearing white shirts, dancing and a young male sitting at a table.\nGetting embedding for An older couple posing in front of a fountain for a picture\nGetting embedding for A man, woman, and child enjoying themselves on a beach.\nGetting embedding for A man stopping on the sidewalk with his bike to have a smoke.\nGetting embedding for A man is standing up holding a green bowling ball with his right hand.\nGetting embedding for A man with a beard, wearing a red shirt with gray sleeves and work gloves, pulling on a rope.\nGetting embedding for A man in a Tour De Force shirt is working on a bicycle.\nGetting embedding for A group of children play jump rope in the streets while others watch in the background.\nGetting embedding for A woman and a few children in an alleyway in between a vehicle and a fence.\nGetting embedding for A man resting on a street.\nGetting embedding for A mountain biker jumping a slope outdoors in a forest area.\nGetting embedding for A man in the distance is walking past a brick wall painted with words and graffiti.\nGetting embedding for A skier in electric green on the edge of a ramp made of metal bars.\nGetting embedding for A yellow uniformed skier is performing a trick across a railed object.\nGetting embedding for A man in a bright green shirt shows a woman in a bright pink shirt something on a clipboard.\nGetting embedding for A man in a gray vehicle feeding sheep.\nGetting embedding for A group of men and women are having a discussion in a restaurant.\nGetting embedding for People waiting at a light on bikes.\nGetting embedding for Man smokes while sitting on a parked scooter.\nGetting embedding for Two ladies are reading through binders.\nGetting embedding for A couple is in a hot tub.\nGetting embedding for A little boy with a blue jacket is making a sour face at the camera.\nGetting embedding for A man and a woman are standing next to sculptures, talking while another man looks at other sculptures.\nGetting embedding for A lady is on the floor packing a suitcase.\nGetting embedding for A man in a colorful shirt is playing an instrument.\nGetting embedding for Asian school children sitting on each others shoulders.\nGetting embedding for A person looks up.\nGetting embedding for A man sits in front of a set up chess game.\nGetting embedding for A guy performing a bicycle jump trick for an audience.\nGetting embedding for Bikers stop in towns\nGetting embedding for An elderly woman places carrots into a casserole.\nGetting embedding for A dog is outside.\nGetting embedding for A man is working on a bike.\nGetting embedding for A person throwing something for her dog.\nGetting embedding for A man with blond-hair, and a brown shirt drinking out of a public water fountain.\nGetting embedding for A middle aged oriental woman in a green headscarf and blue shirt is flashing a giant smile\nGetting embedding for A lady with a serious face is standing with two guys in front of steps outside.\nGetting embedding for A group of adults is having a discussion at a table under a tent.\nGetting embedding for A man is pulling on a rope.\nGetting embedding for A group of people gathers on the grass in a backyard with tents, tables, and chairs set up.\nGetting embedding for A dog is outside.\nGetting embedding for The buildings are tall.\nGetting embedding for A man wearing a gray cap is looking down.\nGetting embedding for A bearded man in a black t-shirt sits in front of a desk holding a computer.\nGetting embedding for A young man wearing a backpack and dark glasses approaches the brick building where there is a bit of graffiti on the wall.\nGetting embedding for A little girl follows two guys with umbrellas down a path.\nGetting embedding for A child plays at a park.\nGetting embedding for A woman is wearing a green sweatshirt.\nGetting embedding for A man is outdoors.\nGetting embedding for A blond man drinking water from a fountain.\nGetting embedding for There are some people outside.\nGetting embedding for The couple is dancing together.\nGetting embedding for An old man wearing khaki pants and a brown shirt standing on the sidewalk in front of a building.\nGetting embedding for People on bicycles waiting at an intersection.\nGetting embedding for A guy stands on stage with his guitar.\nGetting embedding for A woman talks to two other women and a man with notepads in an office building with large windows.\nGetting embedding for A few people in a restaurant setting, one of them is drinking orange juice.\nGetting embedding for A farmer fertilizing his garden with manure with a horse and wagon.\nGetting embedding for A young toddler wearing pink sandals is walking on hopscotch numbers.\nGetting embedding for People are working.\nGetting embedding for A man is on a dirt bike.\nGetting embedding for Toddler with milk around his mouth.\nGetting embedding for A woman is wearing an apron.\nGetting embedding for Bicyclists waiting their turn to cross.\nGetting embedding for A man is playing a game\nGetting embedding for A picture of a city is on a street\nGetting embedding for A woman playing the violin with sunglasses on her head.\nGetting embedding for A foreign family walks by a dirt trail along a body of water.\nGetting embedding for Five people on stage performing and acting while girl lay's on belly.\nGetting embedding for A motorcycle racer is in action at the track.\nGetting embedding for People are on a stage performing.\nGetting embedding for A young man in blue sunglasses walking in front of a red brick building.\nGetting embedding for A man, woman, and child enjoying themselves on a beach.\nGetting embedding for THe woman is sitting down\nGetting embedding for A kite surfer begins to fall in the ocean.\nGetting embedding for Someone is filming.\nGetting embedding for A man makes a ruckus.\nGetting embedding for A man on a street in a bright t-shirt holds some sort of tablet towards a woman in a pink t-shirt and shades.\nGetting embedding for A man waiting with his computer.\nGetting embedding for A boy in a shirt plays outside.\nGetting embedding for Children playing soccer while the sun sets.\nGetting embedding for A man in a bright green shirt shows a woman in a bright pink shirt something on a clipboard.\nGetting embedding for A couple walk through a white brick town.\nGetting embedding for Gray dog running down pavement toward laundry line in courtyard.\nGetting embedding for The man is outside.\nGetting embedding for A middle-aged oriental woman in a green headscarf and blue shirt is flashing a giant smile.\nGetting embedding for Four people near a body of water, one sitting and three standing, while two people walk on a nearby sidewalk.\nGetting embedding for A woman talking to four little children outside.\nGetting embedding for A guy is driving a dirt bike.\nGetting embedding for A man with blond-hair, and a brown shirt drinking out of a public water fountain.\nGetting embedding for A man in blue lies on a mostly-barren patch of grass while small groups of people congregate in the distance.\nGetting embedding for A small girl stands among many large watermelons.\nGetting embedding for A little boy with a blue jacket is making a sour face at the camera.\nGetting embedding for A boy looks down and spreads his arms wide\nGetting embedding for A couple taking a picture\nGetting embedding for A classroom of students discussing lecture.\nGetting embedding for A woman preparing to glaze a bowl.\nGetting embedding for Someone is on top of a cart full of items, while someone else observes.\nGetting embedding for A female adult is near some kids.\nGetting embedding for People are having a discussion.\nGetting embedding for A man is outside.\nGetting embedding for A little boy in a pirate costume is running behind a little girl in a princess costume carrying an orange pumpkin along the sidewalk.\nGetting embedding for A woman walked pasted the front of a building.\nGetting embedding for A doctor in blue scrubs is performing an operation assisted by two men and a woman.\nGetting embedding for A cart is full of items.\nGetting embedding for A family walks along a dirt path.\nGetting embedding for A man is bowling.\nGetting embedding for A man wearing a colorful and striped sweater plays music in the street.\nGetting embedding for A little girl follows two guys with umbrellas down a path.\nGetting embedding for A man pulls on a rope.\nGetting embedding for One man sits inside and plays the banjo, there are trees behind him outside.\nGetting embedding for Pirate on the sidewalk\nGetting embedding for A boy in a blue, yellow, and orange shirt holding his arms out from his sides.\nGetting embedding for A woman wearing an apron inspects a large pot on a table filled with cups, bowls, pots and baskets of assorted size.\nGetting embedding for A blonde woman looks for things in a suitcase.\nGetting embedding for People are working.\nGetting embedding for A group of people are playing soccer and two players from opposing teams are battling for the ball.\nGetting embedding for A couple are having a conversation\nGetting embedding for A man is making a loud noise.\nGetting embedding for A woman playing the violin with sunglasses on her head.\nGetting embedding for A boy is wearing a shirt\nGetting embedding for A Land Rover makes its way through a deep pond.\nGetting embedding for The diners are at a restaurant.\nGetting embedding for A man is standing up holding a green bowling ball with his right hand.\nGetting embedding for A few people in a restaurant setting, one of them is drinking orange juice.\nGetting embedding for A foreign family walks by a dirt trail along a body of water.\nGetting embedding for People with bikes.\nGetting embedding for A man is sitting on a motorcycle on the sidewalk.\nGetting embedding for A small boy has gotten into the cabinet and gotten flour and crisco all over himself.\nGetting embedding for A fireman is working hard to keep people safe.\nGetting embedding for Men and women outside on a street corner.\nGetting embedding for A doctor in blue scrubs is performing an operation assisted by two men and a woman.\nGetting embedding for A person is a red hat and winter jacket is looking into the distance.\nGetting embedding for People wait on traffic.\nGetting embedding for A little boy in a pirate costume is running behind a little girl in a princess costume carrying an orange pumpkin along the sidewalk.\nGetting embedding for A young girl has a bowl on her head\nGetting embedding for A child was making a mess with milk.\nGetting embedding for A man stopping on the sidewalk with his bike to have a smoke.\nGetting embedding for There are people next to sculptures.\nGetting embedding for Two women are talking while children are sitting on their laps.\nGetting embedding for A baby walks on the ground.\nGetting embedding for A child plays at a park.\nGetting embedding for A man windsurfs in a wetsuit.\nGetting embedding for Two barefoot men are playing on a green lawn outside a building with other people in the background.\nGetting embedding for A person looks in her purse at a restaurant.\nGetting embedding for The blond girl is dancing inside a house.\nGetting embedding for A lady with sunglasses on her head and a green sweatshirt is looking off-camera.\nGetting embedding for A view of a marketplace full of people in an asian country.\nGetting embedding for A guy is driving a dirt bike.\nGetting embedding for A person is indoors.\nGetting embedding for A group of people point forwards while performing some kind of act.\nGetting embedding for A man in a colorful shirt is playing an instrument.\nGetting embedding for A lady with sunglasses on her head and a green sweatshirt is looking off-camera.\nGetting embedding for A school is hosting an event.\nGetting embedding for A blond man is drinking from a public fountain.\nGetting embedding for A man plays bowling.\nGetting embedding for A person and their pet are outdoors\nGetting embedding for A group of men and women are having a discussion in a restaurant.\nGetting embedding for A man cooks.\nGetting embedding for A man wearing a blue shirt screaming or yelling with his arms raised up in the air.\nGetting embedding for A man wearing a multi-color coat is playing the guitar on the street.\nGetting embedding for A man wearing a tan coat signs papers for another man wearing a blue coat.\nGetting embedding for A woman talking to four little children outside.\nGetting embedding for A man is leading a Clydesdale up a hay road, within a Old Country.\nGetting embedding for A woman in a blue shirt is sitting at a table and looking at her cellphone.\nGetting embedding for A mountain biker jumping a slope outdoors in a forest area.\nGetting embedding for A woman with a black jacket walks past an outdoor movie poster.\nGetting embedding for Some firefighters check a vehicle.\nGetting embedding for The toddler has milk around the corners of his mouth.\nGetting embedding for  Three men are grouped around the back of a car.\nGetting embedding for A man, woman, and child enjoying themselves on a beach.\nGetting embedding for A man is putting up a poster in front of a shop.\nGetting embedding for Brown dog treads through water as he is soaked in water\nGetting embedding for A man in a bright green shirt shows a woman in a bright pink shirt something on a clipboard.\nGetting embedding for Two men trying to build something together, while having fun.\nGetting embedding for A man wearing a blue shirt screaming or yelling with his arms raised up in the air.\nGetting embedding for A group of people gathers on the grass in a backyard with tents, tables, and chairs set up.\nGetting embedding for A woman on top of her clothes.\nGetting embedding for A crowded street, in an Asian country, where the buildings are dominated by the Seiko building.\nGetting embedding for There is a soccer game.\nGetting embedding for A man wearing a blue shirt screaming or yelling with his arms raised up in the air.\nGetting embedding for Lady wearing a yellow top is sitting on a chair\nGetting embedding for A man has facial hair.\nGetting embedding for A woman is walking across the street eating a banana, while a man is following with his briefcase.\nGetting embedding for A small girl stands among many large watermelons.\nGetting embedding for A family of foreigners walks by the water.\nGetting embedding for A man in a green shirt holds out a clipboard for a woman in pink's attention.\nGetting embedding for A man wearing a tan coat signs papers for another man wearing a blue coat.\nGetting embedding for The baby is playing.\nGetting embedding for The brightly dressed skier slid down the race course.\nGetting embedding for Four guys in wheelchairs on a basketball court two are trying to grab a basketball in midair.\nGetting embedding for A foreign family walks by a dirt trail along a body of water.\nGetting embedding for There is a man and a woman.\nGetting embedding for A boy in a shirt plays outside.\nGetting embedding for An animal is walking outside.\nGetting embedding for A dog is running outdoors.\nGetting embedding for People are near water.\nGetting embedding for A woman in capri jeans crouches on the edge of a fountain with her left foot kicked out to touch the falling water.\nGetting embedding for A man is holding a girls hand and walking through a creek.\nGetting embedding for A blond man drinking water from a fountain.\nGetting embedding for A woman preparing to glaze\nGetting embedding for A man with wild hair rocks a show playing a guitar center stage.\nGetting embedding for A dog is fetching a stick out of very clear water.\nGetting embedding for A man and a woman having an intimate conversation in front a statue.\nGetting embedding for A little boy with a blue jacket is making a sour face at the camera.\nGetting embedding for A man doing a wheelie\nGetting embedding for Man wearing black t-shirt sitting at a computer desk.\nGetting embedding for A couple strolls arm and arm and hand in hand down a city sidewalk.\nGetting embedding for Cheerleaders cheer on a field for an activity.\nGetting embedding for A man is photographing another man.\nGetting embedding for An older man dressed in blue historical clothing is ringing a bell in his right hand.\nGetting embedding for A soccer game.\nGetting embedding for A man, woman, and child enjoying themselves on a beach.\nGetting embedding for A lady is on the floor packing a suitcase.\nGetting embedding for A man is wakeboarding.\nGetting embedding for A soccer game where the team in yellow is attempting to advance past the team in white towards the goalie wearing a black top and blue shorts.\nGetting embedding for Students practicing yoga in a class setting.\nGetting embedding for Two children are running down a sidewalk dressed in costumes.\nGetting embedding for A small girl dressed in a yellow dress with flowers on it bends over near a large pile of watermelons.\nGetting embedding for A man and a woman cross the street in front of a pizza and gyro restaurant.\nGetting embedding for Children are jumping rope.\nGetting embedding for A man with blond-hair, and a brown shirt drinking out of a public water fountain.\nGetting embedding for A lady in a black and white striped shirt and holding a bouquet of flowers, looks seriously at two gentlemen talking on the steps.\nGetting embedding for A man is outdoors.\nGetting embedding for A child was making a mess with milk.\nGetting embedding for A Land Rover makes its way through a deep pond.\nGetting embedding for Girl is blowing to a butterfly.\nGetting embedding for A person is hanging up pictures of women with a few onlookers watching surrounded by bikes.\nGetting embedding for A couple are having a conversation\nGetting embedding for A blonde woman looks for things in a suitcase.\nGetting embedding for Someone is wearing formal clothes.\nGetting embedding for A man is on a dirt bike.\nGetting embedding for A male sitting indoors.\nGetting embedding for The boy is wearing a shirt.\nGetting embedding for A woman is looking at a man's possessions\nGetting embedding for A lady with sunglasses on her head and a green sweatshirt is looking off-camera.\nGetting embedding for A farmer fertilizing his garden with manure with a horse and wagon.\nGetting embedding for People are outside.\nGetting embedding for Several men sit outside on brick ledges built around tall trees.\nGetting embedding for A man in a blue jacket screaming.\nGetting embedding for  Three men are grouped around the back of a car.\nGetting embedding for A small boy has gotten into the cabinet and gotten flour and crisco all over himself.\nGetting embedding for A woman wearing a green and pink dress is dancing with someone wearing a blue top with white pants.\nGetting embedding for People are about to eat.\nGetting embedding for A person is looking at water jets.\nGetting embedding for A boy in multi-colored shirt hold his arms out from his sides\nGetting embedding for A blond man drinking water from a fountain.\nGetting embedding for A man holds a clipboard and a pen as a woman looks at them.\nGetting embedding for An Asian woman is smiling at while another lady is rowing.\nGetting embedding for A few people in a restaurant setting, one of them is drinking orange juice.\nGetting embedding for Men and women outside on a street corner.\nGetting embedding for A boy in a blue, yellow, and orange shirt plays outside.\nGetting embedding for A soccer game where the team in yellow is attempting to advance past the team in white towards the goalie wearing a black top and blue shorts.\nGetting embedding for Two women hug each other.\nGetting embedding for A man wearing a colorful and striped sweater plays music in the street.\nGetting embedding for Human rides two wheeled vehicle.\nGetting embedding for A car is flooding.\nGetting embedding for A family of three is at the beach.\nGetting embedding for A man dressed in snow-gear takes a leap into a snow-covered ravine.\nGetting embedding for The man is outside.\nGetting embedding for A group of adults is having a discussion at a table under a tent.\nGetting embedding for A man wearing a colorful and striped sweater plays music in the street.\nGetting embedding for A man in blue lies on a mostly-barren patch of grass while small groups of people congregate in the distance.\nGetting embedding for A young man in blue sunglasses walking in front of a red brick building.\nGetting embedding for A couple walk through a white brick town.\nGetting embedding for The man has something to tell the woman.\nGetting embedding for A fireman is working hard to keep people safe.\nGetting embedding for A couple strolls arm and arm and hand in hand down a city sidewalk.\nGetting embedding for A woman is wearing a green sweatshirt.\nGetting embedding for A male is getting a drink of water.\nGetting embedding for A baby is playing with a strand of beads.\nGetting embedding for A classroom of students discussing lecture.\nGetting embedding for Gray dog running down pavement toward laundry line in courtyard.\nGetting embedding for A dog is outside.\nGetting embedding for Men and women outside on a street corner.\nGetting embedding for A group of people gathers on the grass in a backyard with tents, tables, and chairs set up.\nGetting embedding for A man with facial hair and a red and gray shirt tugging on a piece of rope.\nGetting embedding for A child was making a mess with milk.\nGetting embedding for A man is wearing something with writing on it.\nGetting embedding for A woman is at a machine.\nGetting embedding for Black dog jumping into the air to catch a toy in the snow.\nGetting embedding for A man is sitting on a motorcycle on the sidewalk.\nGetting embedding for Two kids wearing costumes are outside.\nGetting embedding for People are near water.\nGetting embedding for A child in formal clothing is walking along the edge of a stony area that is littered in places.\nGetting embedding for A group of children are posing.\nGetting embedding for A woman preparing to glaze\nGetting embedding for A person looks up.\nGetting embedding for A man shows a woman something.\nGetting embedding for Asian city scene of people in street with bright lights and glass buildings behind.\nGetting embedding for A group of men and women are having a discussion in a restaurant.\nGetting embedding for A man in a Tour De Force shirt is working on a bicycle.\nGetting embedding for A small boy has gotten into the cabinet and gotten flour and crisco all over himself.\nGetting embedding for A dog is fetching a stick out of very clear water.\nGetting embedding for A family of foreigners walks by the water.\nGetting embedding for A woman walking outside.\nGetting embedding for A girl is picking an item up.\nGetting embedding for Biker riding dirt bike on dirt track\nGetting embedding for A man is playing a game\nGetting embedding for Firemen are walking.\nGetting embedding for A child plays at a park.\nGetting embedding for The child was walking near the grass making a funny face.\nGetting embedding for A fireman protects an area by setting up a boundary while others watch.\nGetting embedding for People in orange vests and blue pants with a yellow stripe at the bottom await the beginning of a race.\nGetting embedding for Several children are jumping rope in the middle of a road while other kids watch\nGetting embedding for Bicyclists waiting at an intersection.\nGetting embedding for A boy in multi-colored shirt hold his arms out from his sides\nGetting embedding for A man sits in front of a set up chess game.\nGetting embedding for A camera crew is filming two women in formal dresses sitting on a blanket in the middle of a park.\nGetting embedding for A classroom of students discussing lecture.\nGetting embedding for A female adult is near some kids.\nGetting embedding for A little girl follows two guys with umbrellas down a path.\nGetting embedding for A dog is outside.\nGetting embedding for Bicyclists waiting at an intersection.\nGetting embedding for A dog is running outdoors.\nGetting embedding for A group of people are sitting around a table under a blue sunshade.\nGetting embedding for A man is taking the picture of a skateboarder who is performing a trick.\nGetting embedding for A man squatting in the foreground of a photograph while taking a photograph of his own of a man doing a skateboarding kick flip in midair above a short flight of stairs outdoors.\nGetting embedding for A couple are having a conversation\nGetting embedding for A black dog in snow is jumping off the ground to catch a stick.\nGetting embedding for A man in a kitchen is frying breaded food in a cast iron pan.\nGetting embedding for A child.\nGetting embedding for A man and a woman cross the street in front of a pizza and gyro restaurant.\nGetting embedding for There are people outdoors.\nGetting embedding for A child was making a mess with milk.\nGetting embedding for A classroom is discussing the topics of the day.\nGetting embedding for A couple is in a hot tub.\nGetting embedding for There are children present\nGetting embedding for A man with wild hair rocks a show playing a guitar center stage.\nGetting embedding for A city filled with people in the middle of the daytime.\nGetting embedding for Soccer players warm up by kicking the soccer ball around while the crowd waits.\nGetting embedding for a old man was talking\nGetting embedding for A man is windsurfing.\nGetting embedding for A man is outside.\nGetting embedding for An Asian woman is smiling at while another lady is rowing.\nGetting embedding for A man wearing a colorful and striped sweater plays music in the street.\nGetting embedding for a man wearing a multicolored striped shirt playing the guitar on the street\nGetting embedding for Children playing soccer while the sun sets.\nGetting embedding for a old man was talking\nGetting embedding for Man smokes while sitting on a parked scooter.\nGetting embedding for A camera crew is filming two women in formal dresses sitting on a blanket in the middle of a park.\nGetting embedding for A group of adults is having a discussion at a table under a tent.\nGetting embedding for A white bike is tied to a street sign.\nGetting embedding for A classroom of students discussing lecture.\nGetting embedding for A woman on top of her clothes.\nGetting embedding for People are in the street.\nGetting embedding for A boy in a blue, yellow, and orange shirt plays outside.\nGetting embedding for An old man is enjoying a beverage at a cafe.\nGetting embedding for A boy is wearing a shirt\nGetting embedding for A couple is in a hot tub.\nGetting embedding for A lady wearing a blue shirt.\nGetting embedding for A woman wearing all white and eating, walks next to a man holding a briefcase.\nGetting embedding for An older couple posing in front of a fountain for a picture\nGetting embedding for A boy looks down and spreads his arms wide\nGetting embedding for A seated woman with short hair and a camera throws a Frisbee to a brown dog.\nGetting embedding for Three wheelchair basketball players playing basketball in field.\nGetting embedding for A bearded man is pulling on a rope.\nGetting embedding for A person in a red dress is running behind a black animal.\nGetting embedding for A man is hanging up a picture of a child.\nGetting embedding for The city has a lot of people in it.\nGetting embedding for People are riding their bicycles.\nGetting embedding for People are looking in a car.\nGetting embedding for A woman and an animal are interacting outdoors.\nGetting embedding for The cart is filled to the top.\nGetting embedding for Women are waiting by a tram.\nGetting embedding for The two people are dancing.\nGetting embedding for three dogs are outside\nGetting embedding for 2 women are carrying little girls.\nGetting embedding for Somebody is hanging up pictures while people watch.\nGetting embedding for A man is being bucked on a horse.\nGetting embedding for A man and a woman are holding hands.\nGetting embedding for A young woman packs belongings into a black luggage carrier.\nGetting embedding for A man watches another man.\nGetting embedding for A sign reads \"Welcome to Golden\"\nGetting embedding for 3 people are walking along a path.\nGetting embedding for The woman is eating a banana.\nGetting embedding for A couple are playing with a young child outside.\nGetting embedding for A child in a multicolored shirt is painting a log\nGetting embedding for A man in a gold skirt sitting at his computer watching the computer screen.\nGetting embedding for Soccer teams are competing outdoors.\nGetting embedding for Three kids are sitting on a rock.\nGetting embedding for A man stands.\nGetting embedding for The mothers are having conversations.\nGetting embedding for the workers are waiting for next work\nGetting embedding for The three young childeren were hold an apple with a bite on it\nGetting embedding for A group watches a practice.\nGetting embedding for A family by a van.\nGetting embedding for The two men sign something.\nGetting embedding for A woman gazes skyward\nGetting embedding for A woman is there.\nGetting embedding for The dogs were outdoors running along the shore\nGetting embedding for A competition is happening.\nGetting embedding for There's a biker\nGetting embedding for A football/soccer club during warm ups.\nGetting embedding for The man is running.\nGetting embedding for There are three girls\nGetting embedding for A woman laughs while another paddles a boat.\nGetting embedding for A soccer game is happening.\nGetting embedding for people dance together\nGetting embedding for A man is sitting down.\nGetting embedding for Choir singing in mass.\nGetting embedding for A Ford is being driven on a track.\nGetting embedding for Bikers stop for gas\nGetting embedding for A skier is near the rail.\nGetting embedding for A firefighter sets up a do not cross\nGetting embedding for Three children pose for a picture.\nGetting embedding for A dog swims in a body of water.\nGetting embedding for There is a farmer with a horse wagon\nGetting embedding for Men are indoors.\nGetting embedding for The women work in the health field.\nGetting embedding for People are paying attention to the person hanging pictures\nGetting embedding for The dogs are playing with a ball.\nGetting embedding for Two people in wheelchairs are reaching in the air for a basketball.\nGetting embedding for Men are near the water.\nGetting embedding for The people are all jumping into a body of water.\nGetting embedding for There are people in ChinaTown.\nGetting embedding for A lady at a table takes pictures.\nGetting embedding for The skier is wearing a jumpsuit.\nGetting embedding for A woman is shown a tablet by a man standing on the street.\nGetting embedding for A child has milk on their face.\nGetting embedding for A guy playing a board game by himself.\nGetting embedding for A group of young men are splashing a lot of water.\nGetting embedding for A girl is running on the trail.\nGetting embedding for Uniformed men work.\nGetting embedding for A child is pulling a toy wagon.\nGetting embedding for There are lots of cars on the street.\nGetting embedding for A model is doing a shoot.\nGetting embedding for Boy in costume followed by a girl in costume.\nGetting embedding for People wait for a race to begin.\nGetting embedding for a bunch of people are playing soccer\nGetting embedding for Four people stand near a wall speaking to each other.\nGetting embedding for Men are hanging something on the wall.\nGetting embedding for People lounge about a pool.\nGetting embedding for A boy is in a boat.\nGetting embedding for A woman has children with her at the check out counter.\nGetting embedding for A group of people are possing for an add.\nGetting embedding for a sports game is being played\nGetting embedding for A groom and bride are standing outside.\nGetting embedding for A motorcyclist doing a wheelie\nGetting embedding for A man is standing on top of a cart.\nGetting embedding for A girl is holding a glass.\nGetting embedding for A group of men are hanging a picture on a wall.\nGetting embedding for A woman standing behind a grill outside with a blue basket of food in her hands.\nGetting embedding for Some humans in a truck\nGetting embedding for A pregnant lady shares the sounds of her pregnancy.\nGetting embedding for A man stands in a bowling alley lane.\nGetting embedding for Women are with their kids\nGetting embedding for A girl is enjoying herself.\nGetting embedding for A climber is ascending\nGetting embedding for A man is hanging a picture of a child.\nGetting embedding for She plays in a band.\nGetting embedding for Near a hanging plant, a metal bowl was used to cook by two mens\nGetting embedding for People are looking at sculptures at a museum.\nGetting embedding for A dog runs across the beach.\nGetting embedding for Three men are smiling\nGetting embedding for a dog drops a red disc\nGetting embedding for A human wearing a dress.\nGetting embedding for Small boy in pool holds toy.\nGetting embedding for The people are near the table.\nGetting embedding for A person is outdoors, on a horse.\nGetting embedding for People are on a bench\nGetting embedding for Two men are around a bowl.\nGetting embedding for Three boys are in a body of water.\nGetting embedding for The school children head home.\nGetting embedding for people are dancing.\nGetting embedding for A group sits outside while talking.\nGetting embedding for He is wearing a multicolored shirt\nGetting embedding for People are on their bikes.\nGetting embedding for Two sumo wrestlers compete in a match.\nGetting embedding for A young lady is looking at a picture.\nGetting embedding for The woman is wearing white.\nGetting embedding for Two people are next to each other.\nGetting embedding for A bike it outside\nGetting embedding for The adults are both male and female.\nGetting embedding for A man is drinking juice.\nGetting embedding for People are skydiving.\nGetting embedding for A dog is outside playing in the water.\nGetting embedding for There is a city.\nGetting embedding for People are waiting for a race.\nGetting embedding for A woman eats a banana and walks across a street, and there is a man trailing behind her.\nGetting embedding for The woman is outdoors with a machine.\nGetting embedding for Two men are in the kitchen.\nGetting embedding for There is snow on the ground.\nGetting embedding for The child is outdoors in his bright colored shirt.\nGetting embedding for The people are on bikes.\nGetting embedding for The people are holding onto the rail.\nGetting embedding for A man is holding an accordian.\nGetting embedding for A child plays on a playground.\nGetting embedding for A boy painting a log.\nGetting embedding for An old couple dance in by a juke box while a dude wearing shorts sleeps near a table\nGetting embedding for The women and the man are on a bench.\nGetting embedding for A man is wearing many silver necklaces.\nGetting embedding for Someone is browsing jewelry.\nGetting embedding for A town has witnessed the arrival of three bikers.\nGetting embedding for The woman is wearing green.\nGetting embedding for A baseball player is swinging to hit the ball.\nGetting embedding for There are two men outside in this picture\nGetting embedding for A man sits at a desk.\nGetting embedding for Firemen walking outside\nGetting embedding for A violinist among other string musicians\nGetting embedding for There are soccer players on the field.\nGetting embedding for A man is outside, near the street.\nGetting embedding for Men watching motorcyclist.\nGetting embedding for A couple is holding a child\nGetting embedding for There are people on a sidewalk.\nGetting embedding for There is a woman in a room.\nGetting embedding for Three men are outside.\nGetting embedding for The man is being stared at.\nGetting embedding for A little boy is underwater.\nGetting embedding for The bicycles are on a road.\nGetting embedding for A man is playing the guitar.\nGetting embedding for Two people walk away from a restaurant across a street.\nGetting embedding for A man being photographed while he does a trick on his skateboard down the stairs.\nGetting embedding for There are people in an urban area.\nGetting embedding for A woman is outside.\nGetting embedding for Man standing with three men in army uniform\nGetting embedding for Two people are walking down a path.\nGetting embedding for A woman is inside.\nGetting embedding for one person sits while three stand near a body of water.\nGetting embedding for A woman with children.\nGetting embedding for A woman talks to two other women.\nGetting embedding for A man and a woman are outside.\nGetting embedding for An couple is outside.\nGetting embedding for A boy walking away.\nGetting embedding for There are people just getting on a train\nGetting embedding for The man and woman are outdoors.\nGetting embedding for A crowded street in Asia.\nGetting embedding for A worker is doing something to a boat.\nGetting embedding for there was a speed breaker on the road by which people are taking care\nGetting embedding for The man walked alongside the crew.\nGetting embedding for A woman is blonde\nGetting embedding for A couple is talking.\nGetting embedding for A crowd gesticulates.\nGetting embedding for The chorus is singing.\nGetting embedding for There is a little boy in brown pants.\nGetting embedding for They are walking with a sign.\nGetting embedding for The person skiis\nGetting embedding for A man is wearing a shirt.\nGetting embedding for someone in his twenties kicks at the ground\nGetting embedding for Someone is making music.\nGetting embedding for The man is pushing a stroller.\nGetting embedding for A woman throws something for a dog.\nGetting embedding for There is an individual waiting indoors.\nGetting embedding for The person is surfing.\nGetting embedding for A child swimming.\nGetting embedding for An oddly dressed man walking.\nGetting embedding for some car passing outside\nGetting embedding for A child is dancing.\nGetting embedding for The woman is wearing a coat.\nGetting embedding for A man is outside.\nGetting embedding for The man knows how to play guitar.\nGetting embedding for A woman is near a fountain.\nGetting embedding for A young lady is looking at her camera.\nGetting embedding for A person eating.\nGetting embedding for the dog is in the water\nGetting embedding for A boy in a tri-colored shirt has his arms out to the side.\nGetting embedding for A person is performing.\nGetting embedding for the kid has milk on his face\nGetting embedding for A woman with a green sweater has a happy expression.\nGetting embedding for An animal is jumping in a place that is not hot.\nGetting embedding for A person is cooking.\nGetting embedding for The woman has one foot in the air.\nGetting embedding for The man is cooking\nGetting embedding for A woman is dressed stylishly in native garb.\nGetting embedding for Two people are having a conversation.\nGetting embedding for there are three dogs\nGetting embedding for a man and woman are getting intimate\nGetting embedding for A dog is playing in the grass.\nGetting embedding for A person is painting.\nGetting embedding for Two girls are playing outside.\nGetting embedding for couple walking\nGetting embedding for Some people are cheering on a field.\nGetting embedding for The young violinist is a woman.\nGetting embedding for A man is playing an instrument.\nGetting embedding for A group of tourist waiting for a train at a train station.\nGetting embedding for The two girls are outside.\nGetting embedding for Thre are girls.\nGetting embedding for A dog is swimming.\nGetting embedding for A man is showing a woman something\nGetting embedding for A girl is dancing in a brown shirt and blue jean skirt on a wooden floor\nGetting embedding for A person holding a green bowling ball stands by the ball return machine in a bowling alley.\nGetting embedding for A team is trying to tag a runner out.\nGetting embedding for A boy sits at peoples feet.\nGetting embedding for A couple of people are loading brush onto a trailer that is attached to a truck.\nGetting embedding for the woman is outside\nGetting embedding for The three men are outside.\nGetting embedding for A man wearing weird clothes is walking through a park.\nGetting embedding for There are a group of people are standing outside.\nGetting embedding for A man is painting a landscape of an outdoors area.\nGetting embedding for A skater is in the pool.\nGetting embedding for A dog swims towards the camera.\nGetting embedding for a player has a penalty kick\nGetting embedding for There are people sitting down.\nGetting embedding for The men are drinking.\nGetting embedding for Someone is on a phone.\nGetting embedding for There are people outdoors.\nGetting embedding for A mother is with her two children at walmart\nGetting embedding for The women are exercising.\nGetting embedding for There are people on bicycles.\nGetting embedding for The man seems happy\nGetting embedding for A person rides a bike outdoors.\nGetting embedding for A dog on the beach.\nGetting embedding for A man and a woman are standing.\nGetting embedding for A young man is sitting.\nGetting embedding for Cheerleaders cheering.\nGetting embedding for Two adults walk across the street.\nGetting embedding for People are near snow.\nGetting embedding for Two people standing standing near a large statue, with other states nearby.\nGetting embedding for People have bikes\nGetting embedding for A woman wearing a blue shirt and green hat looks at the camera\nGetting embedding for A man walks near a store\nGetting embedding for The man is holding a balloon.\nGetting embedding for A person is near a watermelon.\nGetting embedding for A person is sitting down.\nGetting embedding for City people in street\nGetting embedding for A man, wearing a revolutionary apparel is making noise with a bell.\nGetting embedding for Someone is toting packages in an urban setting.\nGetting embedding for The water was choppy as the man parasailed.\nGetting embedding for There is a crowded street.\nGetting embedding for There is a family taking a walk outside.\nGetting embedding for Two Girls are doing exercise.\nGetting embedding for A dog is outside\nGetting embedding for A woman is filling a suitcase.\nGetting embedding for A woman is in Walmart\nGetting embedding for The toddler is getting a checkup.\nGetting embedding for The couple danced.\nGetting embedding for there is a group of people waiting outside\nGetting embedding for A vendor trying to attract costumers.\nGetting embedding for A man is being moved.\nGetting embedding for A man and woman walk on a street.\nGetting embedding for A dog is in the water.\nGetting embedding for There's a group of men hanging up a picture.\nGetting embedding for Man riding bike\nGetting embedding for Two people are seated together.\nGetting embedding for A dog is running outside.\nGetting embedding for a woman in an office building talks to two other women and a man with notepads\nGetting embedding for Two boys are skateboarding outside.\nGetting embedding for There are two people in this picture\nGetting embedding for Bikers stop and wait for traffic at the intersection.\nGetting embedding for A human is riding a vehicle.\nGetting embedding for The cars are outside.\nGetting embedding for there is a bmx biker who is perfect in ravine jumping\nGetting embedding for A couple carrying a child are walking along water.\nGetting embedding for J.P. Morgan Chase held a track event.\nGetting embedding for a man is looking at his webcam\nGetting embedding for Two girls lay next to wooden blocks.\nGetting embedding for boy walks accross patio\nGetting embedding for A man is wearing a bright green shirt\nGetting embedding for A dog carries a stick in his mouth.\nGetting embedding for A barber is at work.\nGetting embedding for girl throws stones\nGetting embedding for The woman is outdoors.\nGetting embedding for The child had milk all over his face.\nGetting embedding for A human standing.\nGetting embedding for The men are working.\nGetting embedding for Two guys cook using some rice milk.\nGetting embedding for young people are gathered around a table\nGetting embedding for Two people dancing outdoors.\nGetting embedding for A dog is nearby a person\nGetting embedding for There are at least four people.\nGetting embedding for A man hangs a poster in front of a shop.\nGetting embedding for People are playing soccer.\nGetting embedding for A building is being painted.\nGetting embedding for group of people running\nGetting embedding for The Asian city is full of sights.\nGetting embedding for Pitcher is winding up a throw\nGetting embedding for there are some groups one of them of young females are in dresses strolling for a ramp walk\nGetting embedding for A woman talks to others indoors.\nGetting embedding for There is a lot of fruit.\nGetting embedding for The people are walking outdoors.\nGetting embedding for Humans playing jump rope\nGetting embedding for An artist is sculpting with clay.\nGetting embedding for A group of people are playing a game of soccer.\nGetting embedding for People standing near sculptures\nGetting embedding for A biker is doing jumps by trees.\nGetting embedding for A young man has his head on the table.\nGetting embedding for The people are listening to a speaker.\nGetting embedding for A lady is close to the floor.\nGetting embedding for People are holding hands and walking.\nGetting embedding for Three puppies are playing outside.\nGetting embedding for A woman is lying down.\nGetting embedding for A woman is wearing a red jacket\nGetting embedding for the man is dancing\nGetting embedding for A man stares at a passing couple.\nGetting embedding for Two old men in winter coats talk outside.\nGetting embedding for A couple of people are holding hands while walking.\nGetting embedding for A man is decorating a shop.\nGetting embedding for A man is advertising for a restaurant.\nGetting embedding for Woman wearing a shirt sewing.\nGetting embedding for A person on a horse jumps over a broken down airplane.\nGetting embedding for A female violinist surrounded by other violinists.\nGetting embedding for The water was choppy as the man parasailed.\nGetting embedding for Toddler with milk around his mouth.\nGetting embedding for Young woman in a cafe checking her cellphone.\nGetting embedding for the three boys are all holding onto a flotation device in the water.\nGetting embedding for two little girls, one in a green jacket and one in a pink jacket, and a little boy in a green jacket holding an apple sitting on a rock.\nGetting embedding for The school children head home.\nGetting embedding for three dogs are outside\nGetting embedding for a skateboarder skates in the pool.\nGetting embedding for young people are gathered around a table\nGetting embedding for man in red canada shirt standing with three men in army uniform\nGetting embedding for Two old men in winter coats talk outside.\nGetting embedding for A young girl dancing in her socks on a wooden floor strewn with pink balloons.\nGetting embedding for one person sits while three stand near a body of water.\nGetting embedding for There is a family taking a walk outside.\nGetting embedding for The water was choppy as the man parasailed.\nGetting embedding for man sitting down playing a game of chess alone\nGetting embedding for Three small puppies bite and play together in the grass.\nGetting embedding for A woman in colorful native attire featuring a blue shirt with a colorful design displays her dark hair braided with red ribbons.\nGetting embedding for The man walked alongside the crew.\nGetting embedding for Wet brown dog swims towards camera.\nGetting embedding for girl throws stones\nGetting embedding for Oddly dressed man walking down the street.\nGetting embedding for Two people dancing outdoors.\nGetting embedding for The adults are both male and female.\nGetting embedding for Three wheelchair basketball players wearing team uniforms are attempting to reach the descending basketball with other players in the background.\nGetting embedding for one person sits while three stand near a body of water.\nGetting embedding for Two guys cook using some rice milk.\nGetting embedding for there was a speed breaker on the road by which people are taking care\nGetting embedding for A model posing to look as if she's a real female soccer player.\nGetting embedding for Indian lady and a guy in a blue suit dancing in the sunlight.\nGetting embedding for Various people hanging around outside of a building.\nGetting embedding for Two old men in winter coats talk outside.\nGetting embedding for Little girl walking along a dirt, rock, and grass path wearing a blue hat many steps behind two people wearing backpacks and holding umbrellas.\nGetting embedding for The furry brown dog is swimming in the ocean.\nGetting embedding for A woman is there.\nGetting embedding for The skier is wearing a yellow jumpsuit and sliding across a yellow rail.\nGetting embedding for There are a group of people are standing outside.\nGetting embedding for Cheerleaders cheering.\nGetting embedding for A man is hanging up a picture of a child.\nGetting embedding for a skateboarder skates in the pool.\nGetting embedding for Two women holding children talking to one another.\nGetting embedding for The cars are outside.\nGetting embedding for A woman wearing orange looking upward.\nGetting embedding for Three wheelchair basketball players playing basketball in field.\nGetting embedding for Two pre-teen girls listening to music on an MP3 player with headphones.\nGetting embedding for Two adults walk across the street.\nGetting embedding for A woman with dark hair is wearing a green sweater.\nGetting embedding for Two men are around a bowl.\nGetting embedding for Young woman running as two guys in the back try to catch up to her.\nGetting embedding for Various people hanging around outside of a building.\nGetting embedding for The water was choppy as the man parasailed.\nGetting embedding for A skater is in the pool.\nGetting embedding for Two women are walking down a dirt path carrying loads on their heads.\nGetting embedding for People are looking in a car.\nGetting embedding for Two guys cook using some rice milk.\nGetting embedding for Firemen emerge from a subway station.\nGetting embedding for Somebody is hanging up pictures while people watch.\nGetting embedding for man sitting down playing a game of chess alone\nGetting embedding for A woman sitting in a laundromat looking at the camera.\nGetting embedding for People sit and relax next to a pool in a plaza.\nGetting embedding for The man walked alongside the crew.\nGetting embedding for The women are exercising.\nGetting embedding for A meeting of young people sitting at a conference table.\nGetting embedding for There is a woman holding a baby, along with a man with a save the children bag.\nGetting embedding for Mothers with children talking at a gathering.\nGetting embedding for A woman is near a fountain.\nGetting embedding for two small girls walk along the leaves.\nGetting embedding for Three men standing on grass by the water looking at something on a table.\nGetting embedding for a bunch of people are playing soccer\nGetting embedding for Small blond-haired girl drinking a glass of juice.\nGetting embedding for Two young men drink beer, leaning on a graffitied wall.\nGetting embedding for There are people on bicycles.\nGetting embedding for a young girl in a flowery dress surrounded by watermelons\nGetting embedding for Three puppies are playing outside.\nGetting embedding for There are people in an urban area.\nGetting embedding for A young man in a red quilted vest displays an assortment of silver pendants around his neck as he watches a woman in a yellow bikini top, a black jacket, and bright pink fingerless gloves go by.\nGetting embedding for A group of people sitting at a table outside talking.\nGetting embedding for Two men cook together with a metal bowl, near a hanging plant.\nGetting embedding for There is a woman holding a baby, along with a man with a save the children bag.\nGetting embedding for There are soccer players on the field.\nGetting embedding for Two young men drink beer, leaning on a graffitied wall.\nGetting embedding for A white bike is leaning against a post.\nGetting embedding for the kid has milk on his face\nGetting embedding for A young Asian man sits behind a set chessboard waiting for the other player to arrive.\nGetting embedding for A woman is walking across the street eating a banana, while a man is following with his briefcase.\nGetting embedding for the kid has milk on his face\nGetting embedding for two small girls walk along the leaves.\nGetting embedding for Five men, one wearing a white shirt standing on something, hanging up a picture of a child.\nGetting embedding for The three young childeren were hold an apple with a bite on it\nGetting embedding for Two girls, each in a dress walking together.\nGetting embedding for A woman with dark hair is wearing a green sweater.\nGetting embedding for a woman in a red jacket watches as a black and brown dog runs away from her in woodland clearing.\nGetting embedding for Soccer teams play on a field as the sun sets behind a line of trees.\nGetting embedding for The man walked alongside the crew.\nGetting embedding for The city has a lot of people in it.\nGetting embedding for there are some groups one of them of young females are in dresses strolling for a ramp walk\nGetting embedding for A child has milk on their face.\nGetting embedding for Two men with heads down signing a paper.\nGetting embedding for A woman is lying down.\nGetting embedding for A man sitting in a barber shop.\nGetting embedding for Three wheelchair basketball players wearing team uniforms are attempting to reach the descending basketball with other players in the background.\nGetting embedding for A man is wearing a bright green shirt\nGetting embedding for The man is being stared at.\nGetting embedding for There is a lot of fruit.\nGetting embedding for City people in street\nGetting embedding for The water was choppy as the man parasailed.\nGetting embedding for People going for a long walk to the mountains.\nGetting embedding for A little kid enjoying some sledding on a winter day.\nGetting embedding for Two young girls are playing outside in a non-urban environment.\nGetting embedding for Two sumo wrestlers compete in a match.\nGetting embedding for Someone is making music.\nGetting embedding for A man in a white shirt hangs a painting in a run down store while other men watch.\nGetting embedding for Two older men in winter coats talking outside of a shop with a grassy lawn covered in a light coat of snow in front of it.\nGetting embedding for The Arsenal football club warms-up on the soccer field as a few fans watch.\nGetting embedding for The child had milk all over his face.\nGetting embedding for A woman with children.\nGetting embedding for An old couple dance in by a juke box while a dude wearing shorts sleeps near a table\nGetting embedding for Children going home from school.\nGetting embedding for Various people hanging around outside of a building.\nGetting embedding for A man is hanging a picture of a child.\nGetting embedding for Two people are walking down a path.\nGetting embedding for Workers are sitting next to a work zone eating food.\nGetting embedding for The three young childeren were hold an apple with a bite on it\nGetting embedding for Two tan and white dogs and one tan dog racing down the beach near the water.\nGetting embedding for People in a truck-full of sacks in a field full of sheep.\nGetting embedding for man sitting down playing a game of chess alone\nGetting embedding for a sports game is being played\nGetting embedding for man in red canada shirt standing with three men in army uniform\nGetting embedding for Two people standing in front of a large statue of a woman, other statues and busts visible in the background.\nGetting embedding for Two tan and white dogs and one tan dog racing down the beach near the water.\nGetting embedding for People in orange vests and blue pants with a yellow stripe at the bottom await the beginning of a race.\nGetting embedding for The woman is wearing green.\nGetting embedding for Two people in wheelchairs are reaching in the air for a basketball.\nGetting embedding for there are some groups one of them of young females are in dresses strolling for a ramp walk\nGetting embedding for A young man is sitting.\nGetting embedding for A woman with dark hair is wearing a green sweater.\nGetting embedding for Someone is toting packages in an urban setting.\nGetting embedding for Three small puppies bite and play together in the grass.\nGetting embedding for The people are listening to a speaker.\nGetting embedding for Two girls lay next to wooden blocks.\nGetting embedding for Thre are girls.\nGetting embedding for There are people just getting on a train\nGetting embedding for The surfer catches a big wave but stays on his board.\nGetting embedding for Woman with green sweater and sunglasses smiling\nGetting embedding for A group of people are playing a game of soccer.\nGetting embedding for Two people standing in front of a large statue of a woman, other statues and busts visible in the background.\nGetting embedding for There's a biker\nGetting embedding for People are holding hands and walking.\nGetting embedding for Three children pose for a picture.\nGetting embedding for A young boy paddles across the water in a makeshift boat.\nGetting embedding for girl throws stones\nGetting embedding for A man wearing blue jeans and red bowling shoes stands in a bowling alley lane with a green ball in his hand.\nGetting embedding for A young man has his head on the table.\nGetting embedding for girl throws stones\nGetting embedding for There are soccer players on the field.\nGetting embedding for A person is painting.\nGetting embedding for a player has a penalty kick\nGetting embedding for A little boy swimming underwater with a toy in his hand.\nGetting embedding for The person is surfing.\nGetting embedding for Four people stand near a wall speaking to each other.\nGetting embedding for The dogs run and play with a red ball.\nGetting embedding for Motorcyclist performing while two men watch.\nGetting embedding for A woman wearing a green headscarf laughs while a woman in the background paddles a boat.\nGetting embedding for there are three dogs\nGetting embedding for There are people outdoors.\nGetting embedding for BMX biker jumps over a ravine.\nGetting embedding for The person skiis\nGetting embedding for the woman is outside\nGetting embedding for Hispanic woman wearing a red plaid shirt works on sewing an article of clothing.\nGetting embedding for the workers are waiting for next work\nGetting embedding for The people are on bikes.\nGetting embedding for there is a group of people waiting outside\nGetting embedding for A woman in a white dress with a tiara sings in a chorus, which has a row of men in sailor hats.\nGetting embedding for A pregnant lady shares the sounds of her pregnancy.\nGetting embedding for A woman talks to two other women and a man with notepads in an office building with large windows.\nGetting embedding for The men are drinking.\nGetting embedding for A woman in costume is marching with a large drum.\nGetting embedding for a sports game is being played\nGetting embedding for People are skydiving.\nGetting embedding for boy walks accross patio\nGetting embedding for People in a meeting setting paying attention to a speaker in an orange shirt.\nGetting embedding for Indian couple holding child near riverbank.\nGetting embedding for A little boy drinks milk and gets milk all over his face and table.\nGetting embedding for A skier is near the rail.\nGetting embedding for Two people are walking down a path.\nGetting embedding for Two little girls lie on the carpet next to an O made of wooden blocks.\nGetting embedding for A vendor trying to attract costumers.\nGetting embedding for three bikers stop in town.\nGetting embedding for a woman in a red jacket watches as a black and brown dog runs away from her in woodland clearing.\nGetting embedding for The woman is wearing white.\nGetting embedding for A man is painting a portrait of an outside scene that includes a street sign with a bicycle chained to it.\nGetting embedding for there is a bmx biker who is perfect in ravine jumping\nGetting embedding for Two girls are playing outside.\nGetting embedding for A view of buildings and people walking across the streets in Times Square, New York City.\nGetting embedding for There are two people in this picture\nGetting embedding for A couple is talking.\nGetting embedding for A woman is walking across the street eating a banana, while a man is following with his briefcase.\nGetting embedding for Three girls on an amusement ride enjoying themselves.\nGetting embedding for A young woman is playing the violin.\nGetting embedding for Woman in white in foreground and a man slightly behind walking with a sign for John's Pizza and Gyro in the background.\nGetting embedding for Group of people dancing\nGetting embedding for Busy ChinaTown street corner where people are walking past an open front store.\nGetting embedding for A man, wearing a revolutionary apparel is making noise with a bell.\nGetting embedding for The young man is waiting with others on the sidewalk.\nGetting embedding for young people are gathered around a table\nGetting embedding for A young woman packs belongings into a black luggage carrier.\nGetting embedding for Two people walk away from a restaurant across a street.\nGetting embedding for young people are gathered around a table\nGetting embedding for An expectant woman happily lets another listen to the baby inside of her.\nGetting embedding for A young woman is playing the violin.\nGetting embedding for People are holding hands and walking.\nGetting embedding for A man being airlifted to safety after being in danger.\nGetting embedding for High fashion ladies wait outside a tram beside a crowd of people in the city.\nGetting embedding for The toddler is getting a checkup.\nGetting embedding for the workers are waiting for next work\nGetting embedding for A man is advertising for a restaurant.\nGetting embedding for Three men are smiling and posing behind a truck loaded with various construction supplies.\nGetting embedding for Two people are walking down a path.\nGetting embedding for A man with a beard, wearing a red shirt with gray sleeves and work gloves, pulling on a rope.\nGetting embedding for a woman in an office building talks to two other women and a man with notepads\nGetting embedding for a dog drops a red disc\nGetting embedding for A mother is with her two children at walmart\nGetting embedding for A woman in a blue shirt and green hat looks up at the camera.\nGetting embedding for The man seems happy\nGetting embedding for A team is trying to tag a runner out.\nGetting embedding for He is wearing a multicolored shirt\nGetting embedding for a skateboarder skates in the pool.\nGetting embedding for A person is painting.\nGetting embedding for Asian city scene of people in street with bright lights and glass buildings behind.\nGetting embedding for a couple are holding hands behind their backs while walking down a street, and the man has his arm around her shoulder.\nGetting embedding for There is a woman in a room.\nGetting embedding for Three people are sitting on a bench.\nGetting embedding for Humans playing jump rope\nGetting embedding for A man is being bucked on a horse.\nGetting embedding for Two elderly men having a conversation, snow covered grass in the background.\nGetting embedding for a woman in a red jacket watches as a black and brown dog runs away from her in woodland clearing.\nGetting embedding for There is a woman in a room.\nGetting embedding for Thre are girls.\nGetting embedding for Several people are dancing together in sync.\nGetting embedding for Two people wearing blue clothing are making hand gestures next to one another.\nGetting embedding for The Arsenal football club warms-up on the soccer field as a few fans watch.\nGetting embedding for Two sumo wrestlers compete in a match.\nGetting embedding for A woman standing behind a grill outside with a blue basket of food in her hands.\nGetting embedding for The woman is wearing white.\nGetting embedding for A woman talks to two other women.\nGetting embedding for a bunch of people are playing soccer\nGetting embedding for three dogs are outside\nGetting embedding for Three wheelchair basketball players wearing team uniforms are attempting to reach the descending basketball with other players in the background.\nGetting embedding for The women and the man are on a bench.\nGetting embedding for Small boy in pool holds toy.\nGetting embedding for Two men are on scaffolding as they paint above a storefront while a man on the sidewalk stands next to them talking on the phone.\nGetting embedding for Choir singing in mass.\nGetting embedding for A person is hanging up pictures of women with a few onlookers watching surrounded by bikes.\nGetting embedding for A young man and young lady dancing on a carpeted floor with a picture from the movie Toy Story on a big screen in the background.\nGetting embedding for Two people are walking down a path.\nGetting embedding for A woman is near a fountain.\nGetting embedding for young people are gathered around a table\nGetting embedding for A man is putting up a poster in front of a shop.\nGetting embedding for Three men are smiling\nGetting embedding for Two women are walking down a dirt path carrying loads on their heads.\nGetting embedding for The women are exercising.\nGetting embedding for A man carrying a load of fresh direct boxes on car with wheels in the city streets, as a woman walks towards him.\nGetting embedding for there was a speed breaker on the road by which people are taking care\nGetting embedding for The Arsenal football club warms-up on the soccer field as a few fans watch.\nGetting embedding for There is a woman holding a baby, along with a man with a save the children bag.\nGetting embedding for Two men are in the kitchen.\nGetting embedding for A dog swims in a body of water.\nGetting embedding for Two pre-teen girls listening to music on an MP3 player with headphones.\nGetting embedding for Someone is browsing jewelry.\nGetting embedding for They are walking with a sign.\nGetting embedding for The man is holding a balloon.\nGetting embedding for The people are all jumping into a body of water.\nGetting embedding for Skydivers in formation.\nGetting embedding for The person is surfing.\nGetting embedding for Two people are having a conversation.\nGetting embedding for The school children head home.\nGetting embedding for Woman in white in foreground and a man slightly behind walking with a sign for John's Pizza and Gyro in the background.\nGetting embedding for A team is trying to tag a runner out.\nGetting embedding for Men fish on a concrete slab.\nGetting embedding for Busy ChinaTown street corner where people are walking past an open front store.\nGetting embedding for The young violinist is a woman.\nGetting embedding for Uniformed men work.\nGetting embedding for There is a woman holding a baby, along with a man with a save the children bag.\nGetting embedding for A small white dog running on a pebble covered beach.\nGetting embedding for A man wearing a blue shirt is sitting on a brick planter next to some paintings.\nGetting embedding for He is wearing a multicolored shirt\nGetting embedding for Small blond-haired girl drinking a glass of juice.\nGetting embedding for girl throws stones\nGetting embedding for there was a speed breaker on the road by which people are taking care\nGetting embedding for Two women are walking down a dirt path carrying loads on their heads.\nGetting embedding for An artist is sculpting with clay.\nGetting embedding for two female medical personnel read their charts.\nGetting embedding for People going for a long walk to the mountains.\nGetting embedding for Small blond-haired girl drinking a glass of juice.\nGetting embedding for Skydivers in formation.\nGetting embedding for The men are drinking.\nGetting embedding for People are playing soccer.\nGetting embedding for there is a group of people waiting outside\nGetting embedding for Two boys are skateboarding outside.\nGetting embedding for Two women and one man sit on a bench.\nGetting embedding for Tourists waiting at a train stop.\nGetting embedding for A small group of church-goers watch a choir practice.\nGetting embedding for Uniformed men work.\nGetting embedding for A mother with her four children.\nGetting embedding for She plays in a band.\nGetting embedding for The dogs were outdoors running along the shore\nGetting embedding for A violinist among other string musicians\nGetting embedding for Two girls are playing outside.\nGetting embedding for The man and woman are outdoors.\nGetting embedding for Women are with their kids\nGetting embedding for a woman in a red jacket watches as a black and brown dog runs away from her in woodland clearing.\nGetting embedding for Young woman in a cafe checking her cellphone.\nGetting embedding for Two women, each embracing a little girl, catch up at a small family gathering.\nGetting embedding for a woman in a red jacket watches as a black and brown dog runs away from her in woodland clearing.\nGetting embedding for A skater is in the pool.\nGetting embedding for Toddler in striped sweatshirt plays on rope on playground.\nGetting embedding for A man stares at a passing couple.\nGetting embedding for A man and a woman are standing next to sculptures, talking while another man looks at other sculptures.\nGetting embedding for There are four Eastern guys working on and hanging pictures of humans.\nGetting embedding for A man hangs a poster in front of a shop.\nGetting embedding for A sumo wrestler with a brown belt is pushing another wrestler in a bout.\nGetting embedding for Various people hanging around outside of a building.\nGetting embedding for The city has a lot of people in it.\nGetting embedding for Three guys and a girl are all jumping in a pool together.\nGetting embedding for Two people walk away from a restaurant across a street.\nGetting embedding for People are holding hands and walking.\nGetting embedding for People are playing soccer.\nGetting embedding for A woman laughs while another paddles a boat.\nGetting embedding for Three girls on an amusement ride enjoying themselves.\nGetting embedding for A man is outside.\nGetting embedding for There is snow on the ground.\nGetting embedding for Two people standing in front of a large statue of a woman, other statues and busts visible in the background.\nGetting embedding for Children playing a game.\nGetting embedding for The bicyclists are outside.\nGetting embedding for There are people next to sculptures.\nGetting embedding for A boy in a shirt plays outside.\nGetting embedding for Workers are on break.\nGetting embedding for A woman has her back to the camera.\nGetting embedding for Two people are looking at a clipboard.\nGetting embedding for A man wants a woman to look at his clipboard\nGetting embedding for The boy is wearing a shirt.\nGetting embedding for The child is painting.\nGetting embedding for One person running next to their bike with the person riding their bike behind them.\nGetting embedding for Two men are barefoot on the lawn.\nGetting embedding for A soccer game is in progress.\nGetting embedding for A family walks along a dirt path.\nGetting embedding for There is a man and a woman.\nGetting embedding for There are people outdoors.\nGetting embedding for A man in a colorful shirt is playing an instrument.\nGetting embedding for the man is working on the computer\nGetting embedding for the person is hanging pictures.\nGetting embedding for A man is outside.\nGetting embedding for The white and brown dog is in the air.\nGetting embedding for people sit around table\nGetting embedding for A woman stand on a fountain and dips her toes in.\nGetting embedding for The shop sign says  \"Welcome to Golden\"\nGetting embedding for The man is drinking water.\nGetting embedding for There are two woman in this picture.\nGetting embedding for A child.\nGetting embedding for Someone is on top of a cart full of items, while someone else observes.\nGetting embedding for A log is being painted by a child.\nGetting embedding for There are people in front of buildings that are brightly lit.\nGetting embedding for A boy is wearing a shirt\nGetting embedding for a person in orange\nGetting embedding for A man is windsurfing.\nGetting embedding for The child was walking near the grass making a funny face.\nGetting embedding for THe woman is sitting down\nGetting embedding for The racer is driving.\nGetting embedding for The red and black team are playing a game.\nGetting embedding for a guy is performing a bike trick\nGetting embedding for The firemen are gathered one is holding his helmet.\nGetting embedding for There is a girl standing\nGetting embedding for A person looks in her purse at a restaurant.\nGetting embedding for People are playing soccer.\nGetting embedding for The cheerleaders are wearing clothes.\nGetting embedding for People are about to eat.\nGetting embedding for pwople are on stage\nGetting embedding for A person holding a boombox.\nGetting embedding for The child is happy.\nGetting embedding for People are playing a sport.\nGetting embedding for An elderly woman puts carrots into a casserole\nGetting embedding for There is a man in front of the shop.\nGetting embedding for A pair of dogs tease a third with nibbles.\nGetting embedding for Two animals getting to know each other.\nGetting embedding for A woman in white.\nGetting embedding for A car drives through the water.\nGetting embedding for The people stretched on yoga mats.\nGetting embedding for The boy is making a mess.\nGetting embedding for A human walks up some stairs.\nGetting embedding for People are working.\nGetting embedding for Two kids are running.\nGetting embedding for A few people share a bench.\nGetting embedding for A lady looks at a picture on her camera\nGetting embedding for Three men looking at a car.\nGetting embedding for Children playing soccer while the sun sets.\nGetting embedding for The old man is standing outside of a building.\nGetting embedding for An animal is walking outside.\nGetting embedding for The people are doing yoga\nGetting embedding for A woman is talking to children.\nGetting embedding for Kids pose in front of a mountain background.\nGetting embedding for The man is playing music on an instrument.\nGetting embedding for Kids are playing outdoors.\nGetting embedding for Someone sitting outside behind a chessboard.\nGetting embedding for A man doing a wheelie\nGetting embedding for People are fishing and walking next to the water.\nGetting embedding for A man is photographing another man.\nGetting embedding for People are having a discussion.\nGetting embedding for The boy is young.\nGetting embedding for A woman walking outside.\nGetting embedding for People are outside.\nGetting embedding for People are waiting to eat.\nGetting embedding for  Three men are grouped around the back of a car.\nGetting embedding for The cowboy waved to the crowd.\nGetting embedding for Two men serving food.\nGetting embedding for A dog is outside.\nGetting embedding for A woman on top of her clothes.\nGetting embedding for A group of people are outside.\nGetting embedding for A group of people gathering on the grass.\nGetting embedding for A man is outdoors.\nGetting embedding for A cowboy rides a bull at a rodeo.\nGetting embedding for A guy stands on stage with his guitar.\nGetting embedding for The workers are standing still.\nGetting embedding for A group of bikers are in the street.\nGetting embedding for A female is next to a man.\nGetting embedding for Two adults walking across a road\nGetting embedding for A groups of people acts on stage.\nGetting embedding for Two people are next to a fountain together.\nGetting embedding for A woman is at a machine.\nGetting embedding for Schoolchildren together\nGetting embedding for The Seiko building is large.\nGetting embedding for People are in the street.\nGetting embedding for A yoga class is in progress.\nGetting embedding for There is a soccer game.\nGetting embedding for There are bicyclists stopped at a road.\nGetting embedding for An Asian woman is smiling at while another lady is rowing.\nGetting embedding for There are some guys in this picture\nGetting embedding for A firefighter sets up tape in a city\nGetting embedding for The girl blows a butterfly.\nGetting embedding for Children enjoy playing together.\nGetting embedding for A classroom is discussing the topics of the day.\nGetting embedding for A group of people point forwards while doing something.\nGetting embedding for the bike is tied to a sign\nGetting embedding for A man plays bowling.\nGetting embedding for People in orange vests await the beginning of a race.\nGetting embedding for A city filled with people in the middle of the daytime.\nGetting embedding for A fireman is working hard to keep people safe.\nGetting embedding for A woman is wearing a green sweatshirt.\nGetting embedding for A man is running  behind a sled.\nGetting embedding for Someone is wearing formal clothes.\nGetting embedding for Children are jumping rope.\nGetting embedding for man ringing a bell\nGetting embedding for A young girl has a bowl on her head\nGetting embedding for Brown dog treads through water as he is soaked in water\nGetting embedding for People are near water.\nGetting embedding for The man has something to tell the woman.\nGetting embedding for The people are by the wall.\nGetting embedding for Peole stand by a building supply truck.\nGetting embedding for The man is fertilizering his garden.\nGetting embedding for A school is hosting an event.\nGetting embedding for People playing jump rope.\nGetting embedding for Lady sits gazing at a camera.\nGetting embedding for The man rides an animal.\nGetting embedding for The boy does a skateboarding trick.\nGetting embedding for The man is outside.\nGetting embedding for A man is wearing something with writing on it.\nGetting embedding for A woman preparing to glaze\nGetting embedding for A decorated man sees a scantily clad female.\nGetting embedding for There are three men working.\nGetting embedding for A male is getting a drink of water.\nGetting embedding for People are standing on a grassy field\nGetting embedding for A person throwing something for her dog.\nGetting embedding for A lady wearing a blue shirt.\nGetting embedding for The woman throws a Frisbee to the dog.\nGetting embedding for A person looks up.\nGetting embedding for A group of children are posing.\nGetting embedding for Four guys are playing basketball.\nGetting embedding for One man writes on papers, while another man stands by.\nGetting embedding for A dog with an object in it's mouth is in the water.\nGetting embedding for Two adults walk across a street.\nGetting embedding for A foreign family walks by a dirt trail along a body of water.\nGetting embedding for Pirate on the sidewalk\nGetting embedding for a old man was talking\nGetting embedding for A man stands at the bottom of the stairs.\nGetting embedding for There is a soccer game with a team in yellow.\nGetting embedding for There is a person processing vegetables.\nGetting embedding for The person stares off into the distance.\nGetting embedding for The man is laying down to sleep\nGetting embedding for A street performer is trying to earn extra money.\nGetting embedding for Two men are laughing and enjoying themselves.\nGetting embedding for Two people are outside.\nGetting embedding for There are scultupres nearby.\nGetting embedding for She is packing.\nGetting embedding for a woman looking at her cellphone\nGetting embedding for Bicyclists waiting their turn to cross.\nGetting embedding for People pose for a picture.\nGetting embedding for A photographer takes a picture of the boy's parents by the fountain.\nGetting embedding for A child was making a mess with milk.\nGetting embedding for The family is outside.\nGetting embedding for A family of foreigners walks by the water.\nGetting embedding for A woman is like to touch the water in fountain\nGetting embedding for There are people waiting on a train.\nGetting embedding for People are walking outdoors.\nGetting embedding for A car is flooding.\nGetting embedding for Man sitting on a motorcycle on the sidewalk\nGetting embedding for A dog is running outdoors.\nGetting embedding for two people by a fountain\nGetting embedding for The toddler has milk around the corners of his mouth.\nGetting embedding for A young lady playing in front of the capitol building.\nGetting embedding for The old man is painting a portrait.\nGetting embedding for The woman and man are outdoors.\nGetting embedding for A baby walks on the ground.\nGetting embedding for The children are playing in a rocky field.\nGetting embedding for the dogs see each other\nGetting embedding for a woman and a german shepherd are pictured\nGetting embedding for a woman is talking\nGetting embedding for The woman in green and pink is dancing.\nGetting embedding for They are outside wearing coats.\nGetting embedding for A man makes a ruckus.\nGetting embedding for Near a couple of restaurants, two people walk across the street.\nGetting embedding for People are wearing colorful clothes\nGetting embedding for A cart is full of items.\nGetting embedding for Some men are sitting outdoors.\nGetting embedding for A soccer player jumping up while a game is in progess.\nGetting embedding for A blond man drinking water from a fountain.\nGetting embedding for They are avoiding trees.\nGetting embedding for Man looking at the camera.\nGetting embedding for A couple are having a conversation\nGetting embedding for A man photographs a skateboarder doing tricks.\nGetting embedding for A man is pulling on a rope.\nGetting embedding for Four people congregate near the water.\nGetting embedding for A man is taking photos of skateboarding tricks.\nGetting embedding for Firemen are walking.\nGetting embedding for The man is outside sledding.\nGetting embedding for A person is indoors.\nGetting embedding for A woman is wearing an apron.\nGetting embedding for There are some people outside.\nGetting embedding for Workers are resting during a meal break.\nGetting embedding for The dog is in the snow.\nGetting embedding for Girls and boys are having fun outdoors\nGetting embedding for People are playing a sport in honor of crippled people.\nGetting embedding for The diners are at a restaurant.\nGetting embedding for A lady with a serious face is standing with two guys in front of steps outside.\nGetting embedding for The family is admiring the water\nGetting embedding for A dog is catching a stick.\nGetting embedding for A girl is picking an item up.\nGetting embedding for The man with the gray hat and pitchfork is directing the cart.\nGetting embedding for More than one person on a bicycle is obeying traffic laws.\nGetting embedding for Community members are spending time in the park near a foundtain.\nGetting embedding for A male sitting indoors.\nGetting embedding for the child is working with wood.\nGetting embedding for A person jumps in the air.\nGetting embedding for Two kids wearing costumes are outside.\nGetting embedding for A person on a bike is near a street.\nGetting embedding for A man is playing a game\nGetting embedding for People cheering.\nGetting embedding for There is a windsurfer balancing on choppy water.\nGetting embedding for The goalie wants to prevent a goal.\nGetting embedding for The child is painting.\nGetting embedding for A woman is looking at a man's possessions\nGetting embedding for man playing soccer\nGetting embedding for The man is able to grow a beard.\nGetting embedding for someone is playing an instrument\nGetting embedding for Human rides two wheeled vehicle.\nGetting embedding for The people are outside.\nGetting embedding for A boy in multi-colored shirt hold his arms out from his sides\nGetting embedding for A man is sitting on a motorcycle.\nGetting embedding for A female adult is near some kids.\nGetting embedding for A guy is driving a dirt bike.\nGetting embedding for There are people watching another person hang up pictures.\nGetting embedding for A kite surfer is falling\nGetting embedding for The woman is on a trolley.\nGetting embedding for The bikers are in the town.\nGetting embedding for A man cooks.\nGetting embedding for A man is wearing a cap\nGetting embedding for A woman walked pasted the front of a building.\nGetting embedding for the runners waited to start the race\nGetting embedding for The people are moving.\nGetting embedding for There are some people in a street\nGetting embedding for There are women showing affection.\nGetting embedding for A view of a crowed place in an asian country.\nGetting embedding for The people are outdoors.\nGetting embedding for An old man is enjoying a beverage at a cafe.\nGetting embedding for A woman is interacting with a dog.\nGetting embedding for Man in blue glasses walking pass a building\nGetting embedding for Two women hug each other.\nGetting embedding for A woman with a yellow to sits.\nGetting embedding for A man is on a dirt bike.\nGetting embedding for a woman eating a banana crosses a street\nGetting embedding for A man pulls on a rope.\nGetting embedding for A soccer game.\nGetting embedding for Cheerleaders cheer on a field for an activity.\nGetting embedding for Two kids are playing with a big rock in the field\nGetting embedding for Bikers stop in towns\nGetting embedding for the woman is wearing a red shirt.\nGetting embedding for The buildings are tall.\nGetting embedding for a child was there\nGetting embedding for Some women are talking.\nGetting embedding for A middle aged oriental woman in a green headscarf and blue shirt is flashing a giant smile\nGetting embedding for A couple is in a hot tub.\nGetting embedding for A small girl with a necklace is in the water\nGetting embedding for A boy is holding his arms out.\nGetting embedding for A blonde woman looks for things in a suitcase.\nGetting embedding for a man is photographing a man skateboarding.\nGetting embedding for The crowd looked on while the players prepared themselves.\nGetting embedding for A man is taking the picture of a skateboarder who is performing a trick.\nGetting embedding for A man sits in front of a set up chess game.\nGetting embedding for People take photos outdoors while a man performs exciting skateboarding tricks.\nGetting embedding for An old man is standing by a building in downtown.\nGetting embedding for lots of people are in the street\nGetting embedding for People wait on traffic.\nGetting embedding for The young man has glasses on his face.\nGetting embedding for The couple is outdoors.\nGetting embedding for The dog is running.\nGetting embedding for The child is painting the wood.\nGetting embedding for The man is outside.\nGetting embedding for The man plays guitar\nGetting embedding for The guys are playing a game.\nGetting embedding for A man is walking with his horse up a country road.\nGetting embedding for Someone is filming.\nGetting embedding for A man is making a loud noise.\nGetting embedding for A man is bowling.\nGetting embedding for A person is dipping her foot into water.\nGetting embedding for A man is wakeboarding.\nGetting embedding for The man is putting up a poster.\nGetting embedding for Firefighters are checking a car.\nGetting embedding for dogs attacking another dog\nGetting embedding for The girl is under the age of 88 years old.\nGetting embedding for people are together\nGetting embedding for A man is standing in front of a shop.\nGetting embedding for There are people at work.\nGetting embedding for A picture of a city is on a street\nGetting embedding for A man is holding a girls hand and walking through a creek.\nGetting embedding for A man is wearing blue.\nGetting embedding for The boy mugs for the camera.\nGetting embedding for There are children present\nGetting embedding for The girl is sitting.\nGetting embedding for A person and their pet are outdoors\nGetting embedding for The person is interested in a water jet.\nGetting embedding for Toddler wearing mik\nGetting embedding for A man waiting with his computer.\nGetting embedding for The baby is playing.\nGetting embedding for Two kids are with a wagon.\nGetting embedding for People with bikes.\nGetting embedding for A child plays at a park.\nGetting embedding for A happy woman smiling\nGetting embedding for A dog is fetching a stick out of very clear water.\nGetting embedding for A man has facial hair.\nGetting embedding for The couple is dancing together.\nGetting embedding for A couple taking a picture\nGetting embedding for A family of three is at the beach.\nGetting embedding for The man is standing.\nGetting embedding for A man is bowling.\nGetting embedding for a bearded man pulls a rope\nGetting embedding for A man is working on a bike.\nGetting embedding for Several children are jumping rope in the middle of a road while other kids watch\nGetting embedding for Several people in an alleyway.\nGetting embedding for A man resting on a street.\nGetting embedding for The biker is jumping into a hole.\nGetting embedding for Grafffiti on a brick wall.\nGetting embedding for The brightly dressed skier slid down the race course.\nGetting embedding for Somebody is engaging in winter sports.\nGetting embedding for A man shows a woman something.\nGetting embedding for The man is outside.\nGetting embedding for a group of men and women converse\nGetting embedding for Men and women outside on a street corner.\nGetting embedding for The man is smoking something while sitting on the scooter.\nGetting embedding for Some women are reading.\nGetting embedding for A woman in capri jeans crouches on the edge of a fountain with her left foot kicked out to touch the falling water.\nGetting embedding for A picture of a city with a sign welcoming travelers on a busy street.\nGetting embedding for Two dogs biting another dog in a field.\nGetting embedding for A lady with a serious face is standing with two guys in front of steps outside.\nGetting embedding for A woman is talking to children.\nGetting embedding for More than one person on a bicycle is obeying traffic laws.\nGetting embedding for a group of men and women converse\nGetting embedding for The people are doing yoga\nGetting embedding for the bike is tied to a sign\nGetting embedding for Two kids are playing with a big rock in the field\nGetting embedding for The man is drinking water.\nGetting embedding for A man in a colorful shirt is playing an instrument.\nGetting embedding for The boy mugs for the camera.\nGetting embedding for There are people in front of buildings that are brightly lit.\nGetting embedding for A woman stand on a fountain and dips her toes in.\nGetting embedding for Man smokes while sitting on a parked scooter.\nGetting embedding for An Asian woman is smiling at while another lady is rowing.\nGetting embedding for Two people are outside.\nGetting embedding for Children's soccer game being played while the sun sets in the background.\nGetting embedding for Several people in an alleyway.\nGetting embedding for the dogs see each other\nGetting embedding for people are together\nGetting embedding for The dog is running.\nGetting embedding for A dog is fetching a stick out of very clear water.\nGetting embedding for The man is drinking water.\nGetting embedding for A young boy with a blue coat makes a funny face as he walks towards the grass.\nGetting embedding for People take photos outdoors while a man performs exciting skateboarding tricks.\nGetting embedding for Two people are looking at a clipboard.\nGetting embedding for The couple is dancing together.\nGetting embedding for A pair of dogs tease a third with nibbles.\nGetting embedding for a man with a white covering is walking up a flight of stairs.\nGetting embedding for someone is playing an instrument\nGetting embedding for Four people near a body of water, one sitting and three standing, while two people walk on a nearby sidewalk.\nGetting embedding for The family is admiring the water\nGetting embedding for a motorcyclist does a nose wheelie.\nGetting embedding for Girls and boys are having fun outdoors\nGetting embedding for A person and their pet are outdoors\nGetting embedding for A man is sleeping on the grass.\nGetting embedding for the dogs see each other\nGetting embedding for The man is outside.\nGetting embedding for The firemen are gathered one is holding his helmet.\nGetting embedding for someone is playing an instrument\nGetting embedding for People in orange vests await the beginning of a race.\nGetting embedding for a woman is talking\nGetting embedding for A person jumps in the air.\nGetting embedding for The young man has glasses on his face.\nGetting embedding for Two people are next to a fountain together.\nGetting embedding for A man sits in front of a set up chess game.\nGetting embedding for lots of people are in the street\nGetting embedding for A woman is interacting with a dog.\nGetting embedding for The boy is making a mess.\nGetting embedding for The guys are playing a game.\nGetting embedding for Children smiling and waving at camera\nGetting embedding for The boy is young.\nGetting embedding for A man wearing a colorful and striped sweater plays music in the street.\nGetting embedding for many people relax in the yard.\nGetting embedding for There are children present\nGetting embedding for Two dogs biting another dog in a field.\nGetting embedding for People are in the street.\nGetting embedding for Two men serving food.\nGetting embedding for Children smiling and waving at camera\nGetting embedding for A man in a green shirt holds out a clipboard for a woman in pink's attention.\nGetting embedding for the runners waited to start the race\nGetting embedding for People are working.\nGetting embedding for The workers are standing still.\nGetting embedding for Two children, in colorful outfits, playing in a field with a big rock in the middle.\nGetting embedding for Two animals getting to know each other.\nGetting embedding for The man is laying down to sleep\nGetting embedding for A mountain biker jumping a slope outdoors in a forest area.\nGetting embedding for People waiting at a light on bikes.\nGetting embedding for a woman eating a banana crosses a street\nGetting embedding for The white and brown dog is in the air.\nGetting embedding for A person looks in her purse at a restaurant.\nGetting embedding for A man with a red shirt is watching another man who is standing on top of a attached cart filled to the top.\nGetting embedding for A small boy has gotten into the cabinet and gotten flour and crisco all over himself.\nGetting embedding for A happy woman smiling\nGetting embedding for One person running next to their bike with the person riding their bike behind them.\nGetting embedding for a woman is talking\nGetting embedding for The woman throws a Frisbee to the dog.\nGetting embedding for lots of people are in the street\nGetting embedding for An old man is standing by a building in downtown.\nGetting embedding for A man dressed in blue shirt and shorts sits at a table while playing black in chess.\nGetting embedding for People are near water.\nGetting embedding for A man is photographing another man.\nGetting embedding for The man is playing music on an instrument.\nGetting embedding for Schoolchildren together\nGetting embedding for One soccer team is playing against another.\nGetting embedding for Two people are next to a fountain with a red bottom and arches of water.\nGetting embedding for woman and child on trolley car labeled Powell and market and bay and taylor\nGetting embedding for The child is happy.\nGetting embedding for They are avoiding trees.\nGetting embedding for Two men are laughing and enjoying themselves.\nGetting embedding for dogs attacking another dog\nGetting embedding for The man wearing lots of medals is watching the girl in the yellow bikini top.\nGetting embedding for Man wearing black t-shirt sitting at a computer desk.\nGetting embedding for Two people are next to a fountain with a red bottom and arches of water.\nGetting embedding for A woman wearing a green and pink dress is dancing with someone wearing a blue top with white pants.\nGetting embedding for Children are jumping rope.\nGetting embedding for Cheerleaders cheer on a field for an activity.\nGetting embedding for People cheering.\nGetting embedding for Cheerleaders are doing a cheer at a football field.\nGetting embedding for Two people pose for the camera.\nGetting embedding for The old man is painting a portrait.\nGetting embedding for a guy is performing a bike trick\nGetting embedding for She is packing.\nGetting embedding for A man is sitting on a motorcycle.\nGetting embedding for The man wearing lots of medals is watching the girl in the yellow bikini top.\nGetting embedding for someone is playing an instrument\nGetting embedding for Children playing a game in a field.\nGetting embedding for Children smiling and waving at camera\nGetting embedding for There are scultupres nearby.\nGetting embedding for A woman is interacting with a dog.\nGetting embedding for A view of a crowed place in an asian country.\nGetting embedding for a woman eating a banana crosses a street\nGetting embedding for A man with khaki shorts on is holding a little girls hand while she walks in the water of a creek.\nGetting embedding for The toddler has milk around the corners of his mouth.\nGetting embedding for Community members are spending time in the park near a foundtain.\nGetting embedding for Some women are reading.\nGetting embedding for There is a windsurfer balancing on choppy water.\nGetting embedding for White small child wearing a brown and gray striped hoodie plays at park.\nGetting embedding for A man wearing a striped top and jeans does a skateboard trick on some steps while a man who is hunched over photographs him.\nGetting embedding for A good-looking firefighter sets up \"Do Not Cross\" tape in the city.\nGetting embedding for Children enjoy playing together.\nGetting embedding for a man with a white covering is walking up a flight of stairs.\nGetting embedding for The couple is outdoors.\nGetting embedding for Two women hug each other.\nGetting embedding for A man sits in front of a set up chess game.\nGetting embedding for Children's soccer game being played while the sun sets in the background.\nGetting embedding for A happy woman smiling\nGetting embedding for Two children, in colorful outfits, playing in a field with a big rock in the middle.\nGetting embedding for There are people watching another person hang up pictures.\nGetting embedding for a man is photographing a man skateboarding.\nGetting embedding for People are near water.\nGetting embedding for A young woman frolicking on the lawn in front of the us capitol building.\nGetting embedding for The boy is making a mess.\nGetting embedding for One person running next to their bike with the person riding their bike behind them.\nGetting embedding for The child was walking near the grass making a funny face.\nGetting embedding for People are working.\nGetting embedding for A woman in blue jeans and a dark jacket walks in front of a building.\nGetting embedding for There are two woman in this picture.\nGetting embedding for Two children, in colorful outfits, playing in a field with a big rock in the middle.\nGetting embedding for A middle aged oriental woman in a green headscarf and blue shirt is flashing a giant smile\nGetting embedding for People are outside.\nGetting embedding for A man is wakeboarding.\nGetting embedding for The person is interested in a water jet.\nGetting embedding for The red and black team are playing a game.\nGetting embedding for People with bikes.\nGetting embedding for The guys are playing a game.\nGetting embedding for a group of men and women converse\nGetting embedding for People wait on traffic.\nGetting embedding for The biker is jumping into a hole.\nGetting embedding for An elderly man is drinking orange juice at a cafe.\nGetting embedding for two people by a fountain\nGetting embedding for People are walking outdoors.\nGetting embedding for the woman is wearing a red shirt.\nGetting embedding for Firefighters are checking a car.\nGetting embedding for Students practicing yoga in a class setting.\nGetting embedding for The people stretched on yoga mats.\nGetting embedding for Woman in white in foreground and a man slightly behind walking with a sign for John's Pizza and Gyro in the background.\nGetting embedding for a woman is talking\nGetting embedding for The man plays guitar\nGetting embedding for Man in blue glasses walking pass a building\nGetting embedding for People are having a discussion.\nGetting embedding for Three men are grouped around the back of a car with its tailgate out, two of the men clothed in yellow uniforms and one in blue.\nGetting embedding for Outside by the trees, a woman wearing jeans and red jacket throws something for a German shepherd to chase.\nGetting embedding for Several people in an alleyway.\nGetting embedding for a bearded man pulls a rope\nGetting embedding for A person is a red hat and winter jacket is looking into the distance.\nGetting embedding for Two children play outside in a field.\nGetting embedding for lots of people are in the street\nGetting embedding for The silhouette of three people in front of a wall.\nGetting embedding for Two kids are playing with a big rock in the field\nGetting embedding for There is a soccer game.\nGetting embedding for Four guys are playing basketball.\nGetting embedding for The boy in the blue and yellow top is standing with arms outstretched.\nGetting embedding for The red and black team are playing a game.\nGetting embedding for The blond woman is searching for medical supplies in a suitcase.\nGetting embedding for The boy does a skateboarding trick.\nGetting embedding for Street performer in colorful shirt performing with small guitar.\nGetting embedding for lots of people are in the street\nGetting embedding for the dogs see each other\nGetting embedding for Man wearing black t-shirt sitting at a computer desk.\nGetting embedding for two people by a fountain\nGetting embedding for The girl blows a butterfly.\nGetting embedding for The people are by the wall.\nGetting embedding for A white horse is pulling a cart while a man stands and watches.\nGetting embedding for Schoolchildren together\nGetting embedding for They are outside wearing coats.\nGetting embedding for The dog is running.\nGetting embedding for Two people enjoying a water fountain display.\nGetting embedding for Two people enjoying a water fountain display.\nGetting embedding for a woman on a yellow shirt is on the floor.\nGetting embedding for People walking around in a big city.\nGetting embedding for The man is outside.\nGetting embedding for There are bicyclists stopped at a road.\nGetting embedding for The couple is outdoors.\nGetting embedding for The people are outside.\nGetting embedding for Two older men in coats are standing outside.\nGetting embedding for Two men are laughing and enjoying themselves.\nGetting embedding for A street performer is trying to earn extra money.\nGetting embedding for The dog is running.\nGetting embedding for There are people at work.\nGetting embedding for Two barefoot men are playing on a green lawn outside a building with other people in the background.\nGetting embedding for A man wearing a colorful and striped sweater plays music in the street.\nGetting embedding for a woman looking at her cellphone\nGetting embedding for The man is laying down to sleep\nGetting embedding for Woman in white in foreground and a man slightly behind walking with a sign for John's Pizza and Gyro in the background.\nGetting embedding for An Asian woman in a blue top and green headscarf smiling widely as another woman rows a boat in the background.\nGetting embedding for The dog is in the snow.\nGetting embedding for There are scultupres nearby.\nGetting embedding for The crowd looked on while the players prepared themselves.\nGetting embedding for A yellow uniformed skier is performing a trick across a railed object.\nGetting embedding for man ringing a bell\nGetting embedding for Two people are outside.\nGetting embedding for Two kids are playing with a big rock in the field\nGetting embedding for a person in orange\nGetting embedding for The parents of the younger male are posing for a picture in front of a water fountain.\nGetting embedding for two men serving preparing food.\nGetting embedding for many people relax in the yard.\nGetting embedding for Two children play outside in a field.\nGetting embedding for The man is drinking water.\nGetting embedding for the child is working with wood.\nGetting embedding for dogs attacking another dog\nGetting embedding for There are scultupres nearby.\nGetting embedding for The man is laying down to sleep\nGetting embedding for man ringing a bell\nGetting embedding for A woman wearing all white and eating, walks next to a man holding a briefcase.\nGetting embedding for A street performer is trying to earn extra money.\nGetting embedding for Child in red and blue shirt painting a log.\nGetting embedding for A group of people gathers on the grass in a backyard with tents, tables, and chairs set up.\nGetting embedding for The couple is outdoors.\nGetting embedding for A man is running  behind a sled.\nGetting embedding for Three people stand proudly by a truck stocked with building supplies in the street.\nGetting embedding for a man with a white covering is walking up a flight of stairs.\nGetting embedding for Four guys in wheelchairs on a basketball court two are trying to grab a basketball in midair.\nGetting embedding for a man is photographing a man skateboarding.\nGetting embedding for A woman in colorful garb with her back to the camera and cloth on her hear.\nGetting embedding for Some women are talking.\nGetting embedding for The dog is running.\nGetting embedding for Two soccer teams are competing on a soccer field.\nGetting embedding for A person is dipping her foot into water.\nGetting embedding for People are near water.\nGetting embedding for A man in the distance is walking past a brick wall painted with words and graffiti.\nGetting embedding for Grafffiti on a brick wall.\nGetting embedding for Two people with bicycles, one in front running with a bike and one in back riding.\nGetting embedding for Two people enjoying a water fountain display.\nGetting embedding for The bikers are in the town.\nGetting embedding for A group of people sitting at some sort of gathering.\nGetting embedding for People are in the street.\nGetting embedding for Kids are playing outdoors.\nGetting embedding for An old man is standing by a building in downtown.\nGetting embedding for Workers are on break.\nGetting embedding for The child is happy.\nGetting embedding for a man with a white covering is walking up a flight of stairs.\nGetting embedding for Two kids are playing with a big rock in the field\nGetting embedding for lots of people are in the street\nGetting embedding for Two people with bicycles, one in front running with a bike and one in back riding.\nGetting embedding for A woman is at a machine.\nGetting embedding for Street performer in colorful shirt performing with small guitar.\nGetting embedding for People are about to eat.\nGetting embedding for Man sitting on a motorcycle on the sidewalk\nGetting embedding for Two blond women are hugging one another.\nGetting embedding for the child is working with wood.\nGetting embedding for Some women are reading.\nGetting embedding for A woman preparing to glaze\nGetting embedding for There is a windsurfer balancing on choppy water.\nGetting embedding for The man is able to grow a beard.\nGetting embedding for THe woman is sitting down\nGetting embedding for Two women hug each other.\nGetting embedding for There is a girl standing\nGetting embedding for A man sits in front of a set up chess game.\nGetting embedding for Men are playing soccer, the one in front is about to kick the ball.\nGetting embedding for The man is playing music on an instrument.\nGetting embedding for A white and brown dog is leaping through the air.\nGetting embedding for The racer is driving.\nGetting embedding for a bearded man pulls a rope\nGetting embedding for Six soccer players on field with player in red uniform in the air and ball airborne.\nGetting embedding for The boy is young.\nGetting embedding for Cheerleaders cheer on a field for an activity.\nGetting embedding for Two people are next to a fountain with a red bottom and arches of water.\nGetting embedding for A woman talking to four little children outside.\nGetting embedding for Two children in hats play in an open, rocky field.\nGetting embedding for a woman with a straw hat working on a strange machine with coconuts at her side.\nGetting embedding for The guys are playing a game.\nGetting embedding for The blond woman is searching for medical supplies in a suitcase.\nGetting embedding for The child is painting.\nGetting embedding for A woman holding a boombox.\nGetting embedding for A woman stand on a fountain and dips her toes in.\nGetting embedding for dogs attacking another dog\nGetting embedding for Four people near a body of water, one sitting and three standing, while two people walk on a nearby sidewalk.\nGetting embedding for People are working.\nGetting embedding for The girl is under the age of 88 years old.\nGetting embedding for Workers are on break.\nGetting embedding for three bikers stop in town.\nGetting embedding for Two kids are with a wagon.\nGetting embedding for Exhausted looking firemen are walking.\nGetting embedding for An older man dressed in blue historical clothing is ringing a bell in his right hand.\nGetting embedding for A woman walking outside.\nGetting embedding for A dog is running outdoors.\nGetting embedding for A crowded city during daytime.\nGetting embedding for A man wants a woman to look at his clipboard\nGetting embedding for There are scultupres nearby.\nGetting embedding for A woman holding a boombox.\nGetting embedding for man playing soccer\nGetting embedding for A woman in capri jeans crouches on the edge of a fountain with her left foot kicked out to touch the falling water.\nGetting embedding for lots of people are in the street\nGetting embedding for People are fishing and walking next to the water.\nGetting embedding for two men serving preparing food.\nGetting embedding for the bike is tied to a sign\nGetting embedding for Some women are talking.\nGetting embedding for The man is outside.\nGetting embedding for many people relax in the yard.\nGetting embedding for Some children are playing jump rope.\nGetting embedding for A middle aged oriental woman in a green headscarf and blue shirt is flashing a giant smile\nGetting embedding for People are in the street.\nGetting embedding for The man wearing lots of medals is watching the girl in the yellow bikini top.\nGetting embedding for a man wearing blue plays soccer.\nGetting embedding for The biker is jumping into a hole.\nGetting embedding for man playing soccer\nGetting embedding for There is a soccer game with a team in yellow.\nGetting embedding for Two children in hats play in an open, rocky field.\nGetting embedding for A man is on a dirt bike.\nGetting embedding for Someone sitting outside behind a chessboard.\nGetting embedding for The old man is painting a portrait.\nGetting embedding for There are some people outside.\nGetting embedding for many people relax in the yard.\nGetting embedding for A young girl sitting at a table with a bowl on her head\nGetting embedding for Three men are grouped around the back of a car with its tailgate out, two of the men clothed in yellow uniforms and one in blue.\nGetting embedding for A woman is like to touch the water in fountain\nGetting embedding for The boy is young.\nGetting embedding for Three construction workers posing with construction materials.\nGetting embedding for The man is putting up a poster.\nGetting embedding for a woman is talking\nGetting embedding for Small laughing child with blond-hair sitting at a table holding a green sippy cup.\nGetting embedding for A woman with a yellow to sits.\n</pre> In\u00a0[8]: Copied! <pre># calculate accuracy (and its standard error) of predicting label=1 if similarity&gt;x\n# x is optimized by sweeping from -1 to 1 in steps of 0.01\ndef accuracy_and_se(cosine_similarity: float, labeled_similarity: int) -&gt; Tuple[float]:\n    accuracies = []\n    for threshold_thousandths in range(-1000, 1000, 1):\n        threshold = threshold_thousandths / 1000\n        total = 0\n        correct = 0\n        for cs, ls in zip(cosine_similarity, labeled_similarity):\n            total += 1\n            if cs &gt; threshold:\n                prediction = 1\n            else:\n                prediction = -1\n            if prediction == ls:\n                correct += 1\n        accuracy = correct / total\n        accuracies.append(accuracy)\n    a = max(accuracies)\n    n = len(cosine_similarity)\n    standard_error = (a * (1 - a) / n) ** 0.5  # standard error of binomial\n    return a, standard_error\n\n\n# check that training and test sets are balanced\npx.histogram(\n    df,\n    x=\"cosine_similarity\",\n    color=\"label\",\n    barmode=\"overlay\",\n    width=500,\n    facet_row=\"dataset\",\n).show()\n\nfor dataset in [\"train\", \"test\"]:\n    data = df[df[\"dataset\"] == dataset]\n    a, se = accuracy_and_se(data[\"cosine_similarity\"], data[\"label\"])\n    print(f\"{dataset} accuracy: {a:0.1%} \u00b1 {1.96 * se:0.1%}\")\n</pre> # calculate accuracy (and its standard error) of predicting label=1 if similarity&gt;x # x is optimized by sweeping from -1 to 1 in steps of 0.01 def accuracy_and_se(cosine_similarity: float, labeled_similarity: int) -&gt; Tuple[float]:     accuracies = []     for threshold_thousandths in range(-1000, 1000, 1):         threshold = threshold_thousandths / 1000         total = 0         correct = 0         for cs, ls in zip(cosine_similarity, labeled_similarity):             total += 1             if cs &gt; threshold:                 prediction = 1             else:                 prediction = -1             if prediction == ls:                 correct += 1         accuracy = correct / total         accuracies.append(accuracy)     a = max(accuracies)     n = len(cosine_similarity)     standard_error = (a * (1 - a) / n) ** 0.5  # standard error of binomial     return a, standard_error   # check that training and test sets are balanced px.histogram(     df,     x=\"cosine_similarity\",     color=\"label\",     barmode=\"overlay\",     width=500,     facet_row=\"dataset\", ).show()  for dataset in [\"train\", \"test\"]:     data = df[df[\"dataset\"] == dataset]     a, se = accuracy_and_se(data[\"cosine_similarity\"], data[\"label\"])     print(f\"{dataset} accuracy: {a:0.1%} \u00b1 {1.96 * se:0.1%}\")  <pre>train accuracy: 89.1% \u00b1 2.4%\ntest accuracy: 88.8% \u00b1 2.4%\n</pre> In\u00a0[9]: Copied! <pre>def embedding_multiplied_by_matrix(\n    embedding: List[float], matrix: torch.tensor\n) -&gt; np.array:\n    embedding_tensor = torch.tensor(embedding).float()\n    modified_embedding = embedding_tensor @ matrix\n    modified_embedding = modified_embedding.detach().numpy()\n    return modified_embedding\n\n\n# compute custom embeddings and new cosine similarities\ndef apply_matrix_to_embeddings_dataframe(matrix: torch.tensor, df: pd.DataFrame):\n    for column in [\"text_1_embedding\", \"text_2_embedding\"]:\n        df[f\"{column}_custom\"] = df[column].apply(\n            lambda x: embedding_multiplied_by_matrix(x, matrix)\n        )\n    df[\"cosine_similarity_custom\"] = df.apply(\n        lambda row: cosine_similarity(\n            row[\"text_1_embedding_custom\"], row[\"text_2_embedding_custom\"]\n        ),\n        axis=1,\n    )\n</pre> def embedding_multiplied_by_matrix(     embedding: List[float], matrix: torch.tensor ) -&gt; np.array:     embedding_tensor = torch.tensor(embedding).float()     modified_embedding = embedding_tensor @ matrix     modified_embedding = modified_embedding.detach().numpy()     return modified_embedding   # compute custom embeddings and new cosine similarities def apply_matrix_to_embeddings_dataframe(matrix: torch.tensor, df: pd.DataFrame):     for column in [\"text_1_embedding\", \"text_2_embedding\"]:         df[f\"{column}_custom\"] = df[column].apply(             lambda x: embedding_multiplied_by_matrix(x, matrix)         )     df[\"cosine_similarity_custom\"] = df.apply(         lambda row: cosine_similarity(             row[\"text_1_embedding_custom\"], row[\"text_2_embedding_custom\"]         ),         axis=1,     )  In\u00a0[10]: Copied! <pre>def optimize_matrix(\n    modified_embedding_length: int = 2048,  # in my brief experimentation, bigger was better (2048 is length of babbage encoding)\n    batch_size: int = 100,\n    max_epochs: int = 100,\n    learning_rate: float = 100.0,  # seemed to work best when similar to batch size - feel free to try a range of values\n    dropout_fraction: float = 0.0,  # in my testing, dropout helped by a couple percentage points (definitely not necessary)\n    df: pd.DataFrame = df,\n    print_progress: bool = True,\n    save_results: bool = True,\n) -&gt; torch.tensor:\n\"\"\"Return matrix optimized to minimize loss on training data.\"\"\"\n    run_id = random.randint(0, 2 ** 31 - 1)  # (range is arbitrary)\n    # convert from dataframe to torch tensors\n    # e is for embedding, s for similarity label\n    def tensors_from_dataframe(\n        df: pd.DataFrame,\n        embedding_column_1: str,\n        embedding_column_2: str,\n        similarity_label_column: str,\n    ) -&gt; Tuple[torch.tensor]:\n        e1 = np.stack(np.array(df[embedding_column_1].values))\n        e2 = np.stack(np.array(df[embedding_column_2].values))\n        s = np.stack(np.array(df[similarity_label_column].astype(\"float\").values))\n\n        e1 = torch.from_numpy(e1).float()\n        e2 = torch.from_numpy(e2).float()\n        s = torch.from_numpy(s).float()\n\n        return e1, e2, s\n\n    e1_train, e2_train, s_train = tensors_from_dataframe(\n        df[df[\"dataset\"] == \"train\"], \"text_1_embedding\", \"text_2_embedding\", \"label\"\n    )\n    e1_test, e2_test, s_test = tensors_from_dataframe(\n        df[df[\"dataset\"] == \"test\"], \"text_1_embedding\", \"text_2_embedding\", \"label\"\n    )\n\n    # create dataset and loader\n    dataset = torch.utils.data.TensorDataset(e1_train, e2_train, s_train)\n    train_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=batch_size, shuffle=True\n    )\n\n    # define model (similarity of projected embeddings)\n    def model(embedding_1, embedding_2, matrix, dropout_fraction=dropout_fraction):\n        e1 = torch.nn.functional.dropout(embedding_1, p=dropout_fraction)\n        e2 = torch.nn.functional.dropout(embedding_2, p=dropout_fraction)\n        modified_embedding_1 = e1 @ matrix  # @ is matrix multiplication\n        modified_embedding_2 = e2 @ matrix\n        similarity = torch.nn.functional.cosine_similarity(\n            modified_embedding_1, modified_embedding_2\n        )\n        return similarity\n\n    # define loss function to minimize\n    def mse_loss(predictions, targets):\n        difference = predictions - targets\n        return torch.sum(difference * difference) / difference.numel()\n\n    # initialize projection matrix\n    embedding_length = len(df[\"text_1_embedding\"].values[0])\n    matrix = torch.randn(\n        embedding_length, modified_embedding_length, requires_grad=True\n    )\n\n    epochs, types, losses, accuracies, matrices = [], [], [], [], []\n    for epoch in range(1, 1 + max_epochs):\n        # iterate through training dataloader\n        for a, b, actual_similarity in train_loader:\n            # generate prediction\n            predicted_similarity = model(a, b, matrix)\n            # get loss and perform backpropagation\n            loss = mse_loss(predicted_similarity, actual_similarity)\n            loss.backward()\n            # update the weights\n            with torch.no_grad():\n                matrix -= matrix.grad * learning_rate\n                # set gradients to zero\n                matrix.grad.zero_()\n        # calculate test loss\n        test_predictions = model(e1_test, e2_test, matrix)\n        test_loss = mse_loss(test_predictions, s_test)\n\n        # compute custom embeddings and new cosine similarities\n        apply_matrix_to_embeddings_dataframe(matrix, df)\n\n        # calculate test accuracy\n        for dataset in [\"train\", \"test\"]:\n            data = df[df[\"dataset\"] == dataset]\n            a, se = accuracy_and_se(data[\"cosine_similarity_custom\"], data[\"label\"])\n\n            # record results of each epoch\n            epochs.append(epoch)\n            types.append(dataset)\n            losses.append(loss.item() if dataset == \"train\" else test_loss.item())\n            accuracies.append(a)\n            matrices.append(matrix.detach().numpy())\n\n            # optionally print accuracies\n            if print_progress is True:\n                print(\n                    f\"Epoch {epoch}/{max_epochs}: {dataset} accuracy: {a:0.1%} \u00b1 {1.96 * se:0.1%}\"\n                )\n\n    data = pd.DataFrame(\n        {\"epoch\": epochs, \"type\": types, \"loss\": losses, \"accuracy\": accuracies}\n    )\n    data[\"run_id\"] = run_id\n    data[\"modified_embedding_length\"] = modified_embedding_length\n    data[\"batch_size\"] = batch_size\n    data[\"max_epochs\"] = max_epochs\n    data[\"learning_rate\"] = learning_rate\n    data[\"dropout_fraction\"] = dropout_fraction\n    data[\n        \"matrix\"\n    ] = matrices  # saving every single matrix can get big; feel free to delete/change\n    if save_results is True:\n        data.to_csv(f\"{run_id}_optimization_results.csv\", index=False)\n\n    return data\n</pre> def optimize_matrix(     modified_embedding_length: int = 2048,  # in my brief experimentation, bigger was better (2048 is length of babbage encoding)     batch_size: int = 100,     max_epochs: int = 100,     learning_rate: float = 100.0,  # seemed to work best when similar to batch size - feel free to try a range of values     dropout_fraction: float = 0.0,  # in my testing, dropout helped by a couple percentage points (definitely not necessary)     df: pd.DataFrame = df,     print_progress: bool = True,     save_results: bool = True, ) -&gt; torch.tensor:     \"\"\"Return matrix optimized to minimize loss on training data.\"\"\"     run_id = random.randint(0, 2 ** 31 - 1)  # (range is arbitrary)     # convert from dataframe to torch tensors     # e is for embedding, s for similarity label     def tensors_from_dataframe(         df: pd.DataFrame,         embedding_column_1: str,         embedding_column_2: str,         similarity_label_column: str,     ) -&gt; Tuple[torch.tensor]:         e1 = np.stack(np.array(df[embedding_column_1].values))         e2 = np.stack(np.array(df[embedding_column_2].values))         s = np.stack(np.array(df[similarity_label_column].astype(\"float\").values))          e1 = torch.from_numpy(e1).float()         e2 = torch.from_numpy(e2).float()         s = torch.from_numpy(s).float()          return e1, e2, s      e1_train, e2_train, s_train = tensors_from_dataframe(         df[df[\"dataset\"] == \"train\"], \"text_1_embedding\", \"text_2_embedding\", \"label\"     )     e1_test, e2_test, s_test = tensors_from_dataframe(         df[df[\"dataset\"] == \"test\"], \"text_1_embedding\", \"text_2_embedding\", \"label\"     )      # create dataset and loader     dataset = torch.utils.data.TensorDataset(e1_train, e2_train, s_train)     train_loader = torch.utils.data.DataLoader(         dataset, batch_size=batch_size, shuffle=True     )      # define model (similarity of projected embeddings)     def model(embedding_1, embedding_2, matrix, dropout_fraction=dropout_fraction):         e1 = torch.nn.functional.dropout(embedding_1, p=dropout_fraction)         e2 = torch.nn.functional.dropout(embedding_2, p=dropout_fraction)         modified_embedding_1 = e1 @ matrix  # @ is matrix multiplication         modified_embedding_2 = e2 @ matrix         similarity = torch.nn.functional.cosine_similarity(             modified_embedding_1, modified_embedding_2         )         return similarity      # define loss function to minimize     def mse_loss(predictions, targets):         difference = predictions - targets         return torch.sum(difference * difference) / difference.numel()      # initialize projection matrix     embedding_length = len(df[\"text_1_embedding\"].values[0])     matrix = torch.randn(         embedding_length, modified_embedding_length, requires_grad=True     )      epochs, types, losses, accuracies, matrices = [], [], [], [], []     for epoch in range(1, 1 + max_epochs):         # iterate through training dataloader         for a, b, actual_similarity in train_loader:             # generate prediction             predicted_similarity = model(a, b, matrix)             # get loss and perform backpropagation             loss = mse_loss(predicted_similarity, actual_similarity)             loss.backward()             # update the weights             with torch.no_grad():                 matrix -= matrix.grad * learning_rate                 # set gradients to zero                 matrix.grad.zero_()         # calculate test loss         test_predictions = model(e1_test, e2_test, matrix)         test_loss = mse_loss(test_predictions, s_test)          # compute custom embeddings and new cosine similarities         apply_matrix_to_embeddings_dataframe(matrix, df)          # calculate test accuracy         for dataset in [\"train\", \"test\"]:             data = df[df[\"dataset\"] == dataset]             a, se = accuracy_and_se(data[\"cosine_similarity_custom\"], data[\"label\"])              # record results of each epoch             epochs.append(epoch)             types.append(dataset)             losses.append(loss.item() if dataset == \"train\" else test_loss.item())             accuracies.append(a)             matrices.append(matrix.detach().numpy())              # optionally print accuracies             if print_progress is True:                 print(                     f\"Epoch {epoch}/{max_epochs}: {dataset} accuracy: {a:0.1%} \u00b1 {1.96 * se:0.1%}\"                 )      data = pd.DataFrame(         {\"epoch\": epochs, \"type\": types, \"loss\": losses, \"accuracy\": accuracies}     )     data[\"run_id\"] = run_id     data[\"modified_embedding_length\"] = modified_embedding_length     data[\"batch_size\"] = batch_size     data[\"max_epochs\"] = max_epochs     data[\"learning_rate\"] = learning_rate     data[\"dropout_fraction\"] = dropout_fraction     data[         \"matrix\"     ] = matrices  # saving every single matrix can get big; feel free to delete/change     if save_results is True:         data.to_csv(f\"{run_id}_optimization_results.csv\", index=False)      return data  In\u00a0[11]: Copied! <pre># example hyperparameter search\n# I recommend starting with max_epochs=10 while initially exploring\nresults = []\nmax_epochs = 30\ndropout_fraction = 0.2\nfor batch_size, learning_rate in [(10, 10), (100, 100), (1000, 1000)]:\n    result = optimize_matrix(\n        batch_size=batch_size,\n        learning_rate=learning_rate,\n        max_epochs=max_epochs,\n        dropout_fraction=dropout_fraction,\n        save_results=False,\n    )\n    results.append(result)\n</pre> # example hyperparameter search # I recommend starting with max_epochs=10 while initially exploring results = [] max_epochs = 30 dropout_fraction = 0.2 for batch_size, learning_rate in [(10, 10), (100, 100), (1000, 1000)]:     result = optimize_matrix(         batch_size=batch_size,         learning_rate=learning_rate,         max_epochs=max_epochs,         dropout_fraction=dropout_fraction,         save_results=False,     )     results.append(result)  <pre>Epoch 1/30: train accuracy: 89.1% \u00b1 2.4%\nEpoch 1/30: test accuracy: 88.4% \u00b1 2.4%\nEpoch 2/30: train accuracy: 89.5% \u00b1 2.3%\nEpoch 2/30: test accuracy: 88.8% \u00b1 2.4%\nEpoch 3/30: train accuracy: 90.6% \u00b1 2.2%\nEpoch 3/30: test accuracy: 89.3% \u00b1 2.3%\nEpoch 4/30: train accuracy: 91.2% \u00b1 2.2%\nEpoch 4/30: test accuracy: 89.7% \u00b1 2.3%\nEpoch 5/30: train accuracy: 91.5% \u00b1 2.1%\nEpoch 5/30: test accuracy: 90.0% \u00b1 2.3%\nEpoch 6/30: train accuracy: 91.9% \u00b1 2.1%\nEpoch 6/30: test accuracy: 90.4% \u00b1 2.2%\nEpoch 7/30: train accuracy: 92.2% \u00b1 2.0%\nEpoch 7/30: test accuracy: 90.7% \u00b1 2.2%\nEpoch 8/30: train accuracy: 92.7% \u00b1 2.0%\nEpoch 8/30: test accuracy: 90.9% \u00b1 2.2%\nEpoch 9/30: train accuracy: 92.7% \u00b1 2.0%\nEpoch 9/30: test accuracy: 91.0% \u00b1 2.2%\nEpoch 10/30: train accuracy: 93.0% \u00b1 1.9%\nEpoch 10/30: test accuracy: 91.6% \u00b1 2.1%\nEpoch 11/30: train accuracy: 93.1% \u00b1 1.9%\nEpoch 11/30: test accuracy: 91.8% \u00b1 2.1%\nEpoch 12/30: train accuracy: 93.4% \u00b1 1.9%\nEpoch 12/30: test accuracy: 92.1% \u00b1 2.0%\nEpoch 13/30: train accuracy: 93.6% \u00b1 1.9%\nEpoch 13/30: test accuracy: 92.4% \u00b1 2.0%\nEpoch 14/30: train accuracy: 93.7% \u00b1 1.8%\nEpoch 14/30: test accuracy: 92.7% \u00b1 2.0%\nEpoch 15/30: train accuracy: 93.7% \u00b1 1.8%\nEpoch 15/30: test accuracy: 92.7% \u00b1 2.0%\nEpoch 16/30: train accuracy: 94.0% \u00b1 1.8%\nEpoch 16/30: test accuracy: 93.0% \u00b1 1.9%\nEpoch 17/30: train accuracy: 94.0% \u00b1 1.8%\nEpoch 17/30: test accuracy: 93.0% \u00b1 1.9%\nEpoch 18/30: train accuracy: 94.2% \u00b1 1.8%\nEpoch 18/30: test accuracy: 93.1% \u00b1 1.9%\nEpoch 19/30: train accuracy: 94.2% \u00b1 1.8%\nEpoch 19/30: test accuracy: 93.1% \u00b1 1.9%\nEpoch 20/30: train accuracy: 94.3% \u00b1 1.8%\nEpoch 20/30: test accuracy: 93.0% \u00b1 1.9%\nEpoch 21/30: train accuracy: 94.5% \u00b1 1.7%\nEpoch 21/30: test accuracy: 93.1% \u00b1 1.9%\nEpoch 22/30: train accuracy: 94.5% \u00b1 1.7%\nEpoch 22/30: test accuracy: 93.3% \u00b1 1.9%\nEpoch 23/30: train accuracy: 94.6% \u00b1 1.7%\nEpoch 23/30: test accuracy: 93.3% \u00b1 1.9%\nEpoch 24/30: train accuracy: 94.6% \u00b1 1.7%\nEpoch 24/30: test accuracy: 93.3% \u00b1 1.9%\nEpoch 25/30: train accuracy: 94.8% \u00b1 1.7%\nEpoch 25/30: test accuracy: 93.3% \u00b1 1.9%\nEpoch 26/30: train accuracy: 94.8% \u00b1 1.7%\nEpoch 26/30: test accuracy: 93.4% \u00b1 1.9%\nEpoch 27/30: train accuracy: 94.8% \u00b1 1.7%\nEpoch 27/30: test accuracy: 93.4% \u00b1 1.9%\nEpoch 28/30: train accuracy: 94.9% \u00b1 1.7%\nEpoch 28/30: test accuracy: 93.4% \u00b1 1.9%\nEpoch 29/30: train accuracy: 94.9% \u00b1 1.7%\nEpoch 29/30: test accuracy: 93.4% \u00b1 1.9%\nEpoch 30/30: train accuracy: 94.9% \u00b1 1.7%\nEpoch 30/30: test accuracy: 93.3% \u00b1 1.9%\nEpoch 1/30: train accuracy: 89.7% \u00b1 2.3%\nEpoch 1/30: test accuracy: 89.1% \u00b1 2.4%\nEpoch 2/30: train accuracy: 89.8% \u00b1 2.3%\nEpoch 2/30: test accuracy: 89.9% \u00b1 2.3%\nEpoch 3/30: train accuracy: 90.3% \u00b1 2.2%\nEpoch 3/30: test accuracy: 90.0% \u00b1 2.3%\nEpoch 4/30: train accuracy: 91.0% \u00b1 2.2%\nEpoch 4/30: test accuracy: 90.3% \u00b1 2.2%\nEpoch 5/30: train accuracy: 91.3% \u00b1 2.1%\nEpoch 5/30: test accuracy: 90.3% \u00b1 2.2%\nEpoch 6/30: train accuracy: 91.8% \u00b1 2.1%\nEpoch 6/30: test accuracy: 90.4% \u00b1 2.2%\nEpoch 7/30: train accuracy: 92.4% \u00b1 2.0%\nEpoch 7/30: test accuracy: 91.0% \u00b1 2.2%\nEpoch 8/30: train accuracy: 92.8% \u00b1 2.0%\nEpoch 8/30: test accuracy: 91.3% \u00b1 2.1%\nEpoch 9/30: train accuracy: 93.1% \u00b1 1.9%\nEpoch 9/30: test accuracy: 91.6% \u00b1 2.1%\nEpoch 10/30: train accuracy: 93.4% \u00b1 1.9%\nEpoch 10/30: test accuracy: 91.9% \u00b1 2.1%\nEpoch 11/30: train accuracy: 93.4% \u00b1 1.9%\nEpoch 11/30: test accuracy: 91.8% \u00b1 2.1%\nEpoch 12/30: train accuracy: 93.6% \u00b1 1.9%\nEpoch 12/30: test accuracy: 92.1% \u00b1 2.0%\nEpoch 13/30: train accuracy: 93.7% \u00b1 1.8%\nEpoch 13/30: test accuracy: 92.4% \u00b1 2.0%\nEpoch 14/30: train accuracy: 93.7% \u00b1 1.8%\nEpoch 14/30: test accuracy: 92.5% \u00b1 2.0%\nEpoch 15/30: train accuracy: 93.9% \u00b1 1.8%\nEpoch 15/30: test accuracy: 92.8% \u00b1 2.0%\nEpoch 16/30: train accuracy: 94.0% \u00b1 1.8%\nEpoch 16/30: test accuracy: 92.8% \u00b1 2.0%\nEpoch 17/30: train accuracy: 94.0% \u00b1 1.8%\nEpoch 17/30: test accuracy: 92.8% \u00b1 2.0%\nEpoch 18/30: train accuracy: 94.2% \u00b1 1.8%\nEpoch 18/30: test accuracy: 92.8% \u00b1 2.0%\nEpoch 19/30: train accuracy: 94.2% \u00b1 1.8%\nEpoch 19/30: test accuracy: 92.8% \u00b1 2.0%\nEpoch 20/30: train accuracy: 94.2% \u00b1 1.8%\nEpoch 20/30: test accuracy: 93.1% \u00b1 1.9%\nEpoch 21/30: train accuracy: 94.3% \u00b1 1.8%\nEpoch 21/30: test accuracy: 93.3% \u00b1 1.9%\nEpoch 22/30: train accuracy: 94.3% \u00b1 1.8%\nEpoch 22/30: test accuracy: 93.3% \u00b1 1.9%\nEpoch 23/30: train accuracy: 94.5% \u00b1 1.7%\nEpoch 23/30: test accuracy: 93.3% \u00b1 1.9%\nEpoch 24/30: train accuracy: 94.5% \u00b1 1.7%\nEpoch 24/30: test accuracy: 93.3% \u00b1 1.9%\nEpoch 25/30: train accuracy: 94.6% \u00b1 1.7%\nEpoch 25/30: test accuracy: 93.4% \u00b1 1.9%\nEpoch 26/30: train accuracy: 94.6% \u00b1 1.7%\nEpoch 26/30: test accuracy: 93.3% \u00b1 1.9%\nEpoch 27/30: train accuracy: 94.6% \u00b1 1.7%\nEpoch 27/30: test accuracy: 93.4% \u00b1 1.9%\nEpoch 28/30: train accuracy: 94.8% \u00b1 1.7%\nEpoch 28/30: test accuracy: 93.4% \u00b1 1.9%\nEpoch 29/30: train accuracy: 94.8% \u00b1 1.7%\nEpoch 29/30: test accuracy: 93.3% \u00b1 1.9%\nEpoch 30/30: train accuracy: 94.8% \u00b1 1.7%\nEpoch 30/30: test accuracy: 93.4% \u00b1 1.9%\nEpoch 1/30: train accuracy: 90.7% \u00b1 2.2%\nEpoch 1/30: test accuracy: 89.9% \u00b1 2.3%\nEpoch 2/30: train accuracy: 90.9% \u00b1 2.2%\nEpoch 2/30: test accuracy: 90.3% \u00b1 2.2%\nEpoch 3/30: train accuracy: 91.6% \u00b1 2.1%\nEpoch 3/30: test accuracy: 90.3% \u00b1 2.2%\nEpoch 4/30: train accuracy: 92.2% \u00b1 2.0%\nEpoch 4/30: test accuracy: 90.7% \u00b1 2.2%\nEpoch 5/30: train accuracy: 92.4% \u00b1 2.0%\nEpoch 5/30: test accuracy: 91.3% \u00b1 2.1%\nEpoch 6/30: train accuracy: 92.5% \u00b1 2.0%\nEpoch 6/30: test accuracy: 91.8% \u00b1 2.1%\nEpoch 7/30: train accuracy: 93.0% \u00b1 1.9%\nEpoch 7/30: test accuracy: 92.2% \u00b1 2.0%\nEpoch 8/30: train accuracy: 93.1% \u00b1 1.9%\nEpoch 8/30: test accuracy: 92.7% \u00b1 2.0%\nEpoch 9/30: train accuracy: 93.3% \u00b1 1.9%\nEpoch 9/30: test accuracy: 92.5% \u00b1 2.0%\nEpoch 10/30: train accuracy: 93.4% \u00b1 1.9%\nEpoch 10/30: test accuracy: 92.7% \u00b1 2.0%\nEpoch 11/30: train accuracy: 93.6% \u00b1 1.9%\nEpoch 11/30: test accuracy: 92.8% \u00b1 2.0%\nEpoch 12/30: train accuracy: 93.7% \u00b1 1.8%\nEpoch 12/30: test accuracy: 92.8% \u00b1 2.0%\nEpoch 13/30: train accuracy: 94.0% \u00b1 1.8%\nEpoch 13/30: test accuracy: 93.0% \u00b1 1.9%\nEpoch 14/30: train accuracy: 93.9% \u00b1 1.8%\nEpoch 14/30: test accuracy: 93.0% \u00b1 1.9%\nEpoch 15/30: train accuracy: 94.2% \u00b1 1.8%\nEpoch 15/30: test accuracy: 93.0% \u00b1 1.9%\nEpoch 16/30: train accuracy: 94.2% \u00b1 1.8%\nEpoch 16/30: test accuracy: 93.0% \u00b1 1.9%\nEpoch 17/30: train accuracy: 94.3% \u00b1 1.8%\nEpoch 17/30: test accuracy: 93.0% \u00b1 1.9%\nEpoch 18/30: train accuracy: 94.5% \u00b1 1.7%\nEpoch 18/30: test accuracy: 93.1% \u00b1 1.9%\nEpoch 19/30: train accuracy: 94.5% \u00b1 1.7%\nEpoch 19/30: test accuracy: 93.1% \u00b1 1.9%\nEpoch 20/30: train accuracy: 94.6% \u00b1 1.7%\nEpoch 20/30: test accuracy: 93.3% \u00b1 1.9%\nEpoch 21/30: train accuracy: 94.8% \u00b1 1.7%\nEpoch 21/30: test accuracy: 93.3% \u00b1 1.9%\nEpoch 22/30: train accuracy: 94.8% \u00b1 1.7%\nEpoch 22/30: test accuracy: 93.4% \u00b1 1.9%\nEpoch 23/30: train accuracy: 94.8% \u00b1 1.7%\nEpoch 23/30: test accuracy: 93.4% \u00b1 1.9%\nEpoch 24/30: train accuracy: 94.8% \u00b1 1.7%\nEpoch 24/30: test accuracy: 93.4% \u00b1 1.9%\nEpoch 25/30: train accuracy: 94.8% \u00b1 1.7%\nEpoch 25/30: test accuracy: 93.4% \u00b1 1.9%\nEpoch 26/30: train accuracy: 94.9% \u00b1 1.7%\nEpoch 26/30: test accuracy: 93.6% \u00b1 1.9%\nEpoch 27/30: train accuracy: 94.9% \u00b1 1.7%\nEpoch 27/30: test accuracy: 93.6% \u00b1 1.9%\nEpoch 28/30: train accuracy: 94.9% \u00b1 1.7%\nEpoch 28/30: test accuracy: 93.6% \u00b1 1.9%\nEpoch 29/30: train accuracy: 95.1% \u00b1 1.6%\nEpoch 29/30: test accuracy: 93.6% \u00b1 1.9%\nEpoch 30/30: train accuracy: 95.1% \u00b1 1.6%\nEpoch 30/30: test accuracy: 93.6% \u00b1 1.9%\n</pre> In\u00a0[12]: Copied! <pre>runs_df = pd.concat(results)\n\n# plot training loss and test loss over time\npx.line(\n    runs_df,\n    line_group=\"run_id\",\n    x=\"epoch\",\n    y=\"loss\",\n    color=\"type\",\n    hover_data=[\"batch_size\", \"learning_rate\", \"dropout_fraction\"],\n    facet_row=\"learning_rate\",\n    facet_col=\"batch_size\",\n    width=500,\n).show()\n\n# plot accuracy over time\npx.line(\n    runs_df,\n    line_group=\"run_id\",\n    x=\"epoch\",\n    y=\"accuracy\",\n    color=\"type\",\n    hover_data=[\"batch_size\", \"learning_rate\", \"dropout_fraction\"],\n    facet_row=\"learning_rate\",\n    facet_col=\"batch_size\",\n    width=500,\n).show()\n</pre> runs_df = pd.concat(results)  # plot training loss and test loss over time px.line(     runs_df,     line_group=\"run_id\",     x=\"epoch\",     y=\"loss\",     color=\"type\",     hover_data=[\"batch_size\", \"learning_rate\", \"dropout_fraction\"],     facet_row=\"learning_rate\",     facet_col=\"batch_size\",     width=500, ).show()  # plot accuracy over time px.line(     runs_df,     line_group=\"run_id\",     x=\"epoch\",     y=\"accuracy\",     color=\"type\",     hover_data=[\"batch_size\", \"learning_rate\", \"dropout_fraction\"],     facet_row=\"learning_rate\",     facet_col=\"batch_size\",     width=500, ).show()  In\u00a0[13]: Copied! <pre># apply result of best run to original data\nbest_run = runs_df.sort_values(by=\"accuracy\", ascending=False).iloc[0]\nbest_matrix = best_run[\"matrix\"]\napply_matrix_to_embeddings_dataframe(best_matrix, df)\n</pre> # apply result of best run to original data best_run = runs_df.sort_values(by=\"accuracy\", ascending=False).iloc[0] best_matrix = best_run[\"matrix\"] apply_matrix_to_embeddings_dataframe(best_matrix, df)  In\u00a0[14]: Copied! <pre># plot similarity distribution BEFORE customization\npx.histogram(\n    df,\n    x=\"cosine_similarity\",\n    color=\"label\",\n    barmode=\"overlay\",\n    width=500,\n    facet_row=\"dataset\",\n).show()\n\ntest_df = df[df[\"dataset\"] == \"test\"]\na, se = accuracy_and_se(test_df[\"cosine_similarity\"], test_df[\"label\"])\nprint(f\"Test accuracy: {a:0.1%} \u00b1 {1.96 * se:0.1%}\")\n\n# plot similarity distribution AFTER customization\npx.histogram(\n    df,\n    x=\"cosine_similarity_custom\",\n    color=\"label\",\n    barmode=\"overlay\",\n    width=500,\n    facet_row=\"dataset\",\n).show()\n\na, se = accuracy_and_se(test_df[\"cosine_similarity_custom\"], test_df[\"label\"])\nprint(f\"Test accuracy after customization: {a:0.1%} \u00b1 {1.96 * se:0.1%}\")\n</pre> # plot similarity distribution BEFORE customization px.histogram(     df,     x=\"cosine_similarity\",     color=\"label\",     barmode=\"overlay\",     width=500,     facet_row=\"dataset\", ).show()  test_df = df[df[\"dataset\"] == \"test\"] a, se = accuracy_and_se(test_df[\"cosine_similarity\"], test_df[\"label\"]) print(f\"Test accuracy: {a:0.1%} \u00b1 {1.96 * se:0.1%}\")  # plot similarity distribution AFTER customization px.histogram(     df,     x=\"cosine_similarity_custom\",     color=\"label\",     barmode=\"overlay\",     width=500,     facet_row=\"dataset\", ).show()  a, se = accuracy_and_se(test_df[\"cosine_similarity_custom\"], test_df[\"label\"]) print(f\"Test accuracy after customization: {a:0.1%} \u00b1 {1.96 * se:0.1%}\")  <pre>Test accuracy: 88.8% \u00b1 2.4%\n</pre> <pre>Test accuracy after customization: 93.6% \u00b1 1.9%\n</pre> In\u00a0[15]: Copied! <pre>best_matrix  # this is what you can multiply your embeddings by\n</pre> best_matrix  # this is what you can multiply your embeddings by  Out[15]: <pre>array([[-1.2566795e+00, -1.5297449e+00, -1.3271648e-01, ...,\n        -1.2859761e+00, -5.3254390e-01,  4.8364732e-01],\n       [-1.4826347e+00,  9.2656955e-02, -4.2437232e-01, ...,\n         1.1872858e+00, -1.0831847e+00, -1.0683593e+00],\n       [-2.2029283e+00, -1.9703420e+00,  3.1125939e-01, ...,\n         2.2947595e+00,  5.5780332e-03, -6.0171342e-01],\n       ...,\n       [-1.1019799e-01,  1.3599515e+00, -4.7677776e-01, ...,\n         6.5626711e-01,  7.2359240e-01,  3.0733588e+00],\n       [ 1.6624762e-03,  4.2648423e-01, -1.1380885e+00, ...,\n         8.7202555e-01,  9.3173909e-01, -1.6760436e+00],\n       [ 7.7449006e-01,  4.9213606e-01,  3.5407653e-01, ...,\n         1.3460466e+00, -1.9509128e-01,  7.7514690e-01]], dtype=float32)</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/Customizing_embeddings/#Customizing-embeddings","title":"Customizing embeddings\u00b6","text":"<p>This notebook demonstrates one way to customize OpenAI embeddings to a particular task.</p> <p>The input is training data in the form of [text_1, text_2, label] where label is +1 if the pairs are similar and -1 if the pairs are dissimilar.</p> <p>The output is a matrix that you can use to multiply your embeddings. The product of this multiplication is a 'custom embedding' that will better emphasize aspects of the text relevant to your use case. In binary classification use cases, we've seen error rates drop by as much as 50%.</p> <p>In the following example, I use 1,000 sentence pairs picked from the SNLI corpus. Each pair of sentences are logically entailed (i.e., one implies the other). These pairs are our positives (label = 1). We generate synthetic negatives by combining sentences from different pairs, which are presumed to not be logically entailed (label = -1).</p> <p>For a clustering use case, you can generate positives by creating pairs from texts in the same clusters and generate negatives by creating pairs from sentences in different clusters.</p> <p>With other data sets, we have seen decent improvement with as little as ~100 training examples. Of course, performance will be better with  more examples.</p>"},{"location":"examples/Customizing_embeddings/#0.-Imports","title":"0. Imports\u00b6","text":""},{"location":"examples/Customizing_embeddings/#1.-Inputs","title":"1. Inputs\u00b6","text":"<p>Most inputs are here. The key things to change are where to load your datset from, where to save a cache of embeddings to, and which embedding engine you want to use.</p> <p>Depending on how your data is formatted, you'll want to rewrite the process_input_data function.</p>"},{"location":"examples/Customizing_embeddings/#2.-Load-and-process-input-data","title":"2. Load and process input data\u00b6","text":""},{"location":"examples/Customizing_embeddings/#3.-Split-data-into-training-test-sets","title":"3. Split data into training test sets\u00b6","text":"<p>Note that it's important to split data into training and test sets before generating synethetic negatives or positives. You don't want any text strings in the training data to show up in the test data. If there's contamination, the test metrics will look better than they'll actually be in production.</p>"},{"location":"examples/Customizing_embeddings/#4.-Generate-synthetic-negatives","title":"4. Generate synthetic negatives\u00b6","text":"<p>This is another piece of the code that you will need to modify to match your use case.</p> <p>If you have data with positives and negatives, you can skip this section.</p> <p>If you have data with only positives, you can mostly keep it as is, where it generates negatives only.</p> <p>If you have multiclass data, you will want to generate both positives and negatives. The positives can be pairs of text that share labels, and the negatives can be pairs of text that do not share labels.</p> <p>The final output should be a dataframe with text pairs, where each pair is labeled -1 or 1.</p>"},{"location":"examples/Customizing_embeddings/#5.-Calculate-embeddings-and-cosine-similarities","title":"5. Calculate embeddings and cosine similarities\u00b6","text":"<p>Here, I create a cache to save the embeddings. This is handy so that you don't have to pay again if you want to run the code again.</p>"},{"location":"examples/Customizing_embeddings/#6.-Plot-distribution-of-cosine-similarity","title":"6. Plot distribution of cosine similarity\u00b6","text":"<p>Here we measure similarity of text using cosine similarity. In our experience, most distance functions (L1, L2, cosine similarity) all work about the same. Note that our embeddings are already normalized to length 1, so cosine similarity is equivalent to dot product.</p> <p>The graphs show how much the overlap there is between the distribution of cosine similarities for similar and dissimilar pairs. If there is a high amount of overlap, that means there are some dissimilar pairs with greater cosine similarity than some similar pairs.</p> <p>The accuracy I compute is the accuracy of a simple rule that predicts 'similar (1)' if the cosine similarity is above some threshold X and otherwise predicts 'dissimilar (0)'.</p>"},{"location":"examples/Customizing_embeddings/#7.-Optimize-the-matrix-using-the-training-data-provided","title":"7. Optimize the matrix using the training data provided\u00b6","text":""},{"location":"examples/Customizing_embeddings/#8.-Plot-the-before-&-after,-showing-the-results-of-the-best-matrix-found-during-training","title":"8. Plot the before &amp; after, showing the results of the best matrix found during training\u00b6","text":"<p>The better the matrix is, the more cleanly it will separate the similar and dissimilar pairs.</p>"},{"location":"examples/Embedding_long_inputs/","title":"Embedding texts that are longer than the model's maximum context length","text":"In\u00a0[1]: Copied! <pre>import openai\nfrom tenacity import retry, wait_random_exponential, stop_after_attempt, retry_if_not_exception_type\n\n\nEMBEDDING_MODEL = 'text-embedding-ada-002'\nEMBEDDING_CTX_LENGTH = 8191\nEMBEDDING_ENCODING = 'cl100k_base'\n\n# let's make sure to not retry on an invalid request, because that is what we want to demonstrate\n@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6), retry=retry_if_not_exception_type(openai.InvalidRequestError))\ndef get_embedding(text_or_tokens, model=EMBEDDING_MODEL):\n    return openai.Embedding.create(input=text_or_tokens, model=model)[\"data\"][0][\"embedding\"]\n</pre> import openai from tenacity import retry, wait_random_exponential, stop_after_attempt, retry_if_not_exception_type   EMBEDDING_MODEL = 'text-embedding-ada-002' EMBEDDING_CTX_LENGTH = 8191 EMBEDDING_ENCODING = 'cl100k_base'  # let's make sure to not retry on an invalid request, because that is what we want to demonstrate @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6), retry=retry_if_not_exception_type(openai.InvalidRequestError)) def get_embedding(text_or_tokens, model=EMBEDDING_MODEL):     return openai.Embedding.create(input=text_or_tokens, model=model)[\"data\"][0][\"embedding\"] <p>The <code>text-embedding-ada-002</code> model has a context length of 8191 tokens with the <code>cl100k_base</code> encoding, and we can see that going over that limit causes an error.</p> In\u00a0[2]: Copied! <pre>long_text = 'AGI ' * 5000\ntry:\n    get_embedding(long_text)\nexcept openai.InvalidRequestError as e:\n    print(e)\n</pre> long_text = 'AGI ' * 5000 try:     get_embedding(long_text) except openai.InvalidRequestError as e:     print(e) <pre>This model's maximum context length is 8191 tokens, however you requested 10001 tokens (10001 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\n</pre> <p>Clearly we want to avoid these errors, particularly when handling programmatically with a large number of embeddings. Yet, we still might be faced with texts that are longer than the maximum context length. Below we describe and provide recipes for the main approaches to handling these longer texts: (1) simply truncating the text to the maximum allowed length, and (2) chunking the text and embedding each chunk individually.</p> In\u00a0[3]: Copied! <pre>import tiktoken\n\ndef truncate_text_tokens(text, encoding_name=EMBEDDING_ENCODING, max_tokens=EMBEDDING_CTX_LENGTH):\n\"\"\"Truncate a string to have `max_tokens` according to the given encoding.\"\"\"\n    encoding = tiktoken.get_encoding(encoding_name)\n    return encoding.encode(text)[:max_tokens]\n</pre> import tiktoken  def truncate_text_tokens(text, encoding_name=EMBEDDING_ENCODING, max_tokens=EMBEDDING_CTX_LENGTH):     \"\"\"Truncate a string to have `max_tokens` according to the given encoding.\"\"\"     encoding = tiktoken.get_encoding(encoding_name)     return encoding.encode(text)[:max_tokens] <p>Our example from before now works without error.</p> In\u00a0[4]: Copied! <pre>truncated = truncate_text_tokens(long_text)\nlen(get_embedding(truncated))\n</pre> truncated = truncate_text_tokens(long_text) len(get_embedding(truncated)) Out[4]: <pre>1536</pre> In\u00a0[5]: Copied! <pre>from itertools import islice\n\ndef batched(iterable, n):\n\"\"\"Batch data into tuples of length n. The last batch may be shorter.\"\"\"\n    # batched('ABCDEFG', 3) --&gt; ABC DEF G\n    if n &lt; 1:\n        raise ValueError('n must be at least one')\n    it = iter(iterable)\n    while (batch := tuple(islice(it, n))):\n        yield batch\n</pre> from itertools import islice  def batched(iterable, n):     \"\"\"Batch data into tuples of length n. The last batch may be shorter.\"\"\"     # batched('ABCDEFG', 3) --&gt; ABC DEF G     if n &lt; 1:         raise ValueError('n must be at least one')     it = iter(iterable)     while (batch := tuple(islice(it, n))):         yield batch <p>Now we define a function that encodes a string into tokens and then breaks it up into chunks.</p> In\u00a0[6]: Copied! <pre>def chunked_tokens(text, encoding_name, chunk_length):\n    encoding = tiktoken.get_encoding(encoding_name)\n    tokens = encoding.encode(text)\n    chunks_iterator = batched(tokens, chunk_length)\n    yield from chunks_iterator\n</pre> def chunked_tokens(text, encoding_name, chunk_length):     encoding = tiktoken.get_encoding(encoding_name)     tokens = encoding.encode(text)     chunks_iterator = batched(tokens, chunk_length)     yield from chunks_iterator <p>Finally, we can write a function that safely handles embedding requests, even when the input text is longer than the maximum context length, by chunking the input tokens and embedding each chunk individually. The <code>average</code> flag can be set to <code>True</code> to return the weighted average of the chunk embeddings, or <code>False</code> to simply return the unmodified list of chunk embeddings.</p> In\u00a0[7]: Copied! <pre>import numpy as np\n\n\ndef len_safe_get_embedding(text, model=EMBEDDING_MODEL, max_tokens=EMBEDDING_CTX_LENGTH, encoding_name=EMBEDDING_ENCODING, average=True):\n    chunk_embeddings = []\n    chunk_lens = []\n    for chunk in chunked_tokens(text, encoding_name=encoding_name, chunk_length=max_tokens):\n        chunk_embeddings.append(get_embedding(chunk, model=model))\n        chunk_lens.append(len(chunk))\n\n    if average:\n        chunk_embeddings = np.average(chunk_embeddings, axis=0, weights=chunk_lens)\n        chunk_embeddings = chunk_embeddings / np.linalg.norm(chunk_embeddings)  # normalizes length to 1\n        chunk_embeddings = chunk_embeddings.tolist()\n    return chunk_embeddings\n</pre> import numpy as np   def len_safe_get_embedding(text, model=EMBEDDING_MODEL, max_tokens=EMBEDDING_CTX_LENGTH, encoding_name=EMBEDDING_ENCODING, average=True):     chunk_embeddings = []     chunk_lens = []     for chunk in chunked_tokens(text, encoding_name=encoding_name, chunk_length=max_tokens):         chunk_embeddings.append(get_embedding(chunk, model=model))         chunk_lens.append(len(chunk))      if average:         chunk_embeddings = np.average(chunk_embeddings, axis=0, weights=chunk_lens)         chunk_embeddings = chunk_embeddings / np.linalg.norm(chunk_embeddings)  # normalizes length to 1         chunk_embeddings = chunk_embeddings.tolist()     return chunk_embeddings <p>Once again, we can now handle long input texts.</p> In\u00a0[8]: Copied! <pre>average_embedding_vector = len_safe_get_embedding(long_text, average=True)\nchunks_embedding_vectors = len_safe_get_embedding(long_text, average=False)\n\nprint(f\"Setting average=True gives us a single {len(average_embedding_vector)}-dimensional embedding vector for our long text.\")\nprint(f\"Setting average=False gives us {len(chunks_embedding_vectors)} embedding vectors, one for each of the chunks.\")\n</pre> average_embedding_vector = len_safe_get_embedding(long_text, average=True) chunks_embedding_vectors = len_safe_get_embedding(long_text, average=False)  print(f\"Setting average=True gives us a single {len(average_embedding_vector)}-dimensional embedding vector for our long text.\") print(f\"Setting average=False gives us {len(chunks_embedding_vectors)} embedding vectors, one for each of the chunks.\")  <pre>Setting average=True gives us a single 1536-dimensional embedding vector for our long text.\nSetting average=False gives us 2 embedding vectors, one for each of the chunks.\n</pre> <p>In some cases, it may make sense to split chunks on paragraph boundaries or sentence boundaries to help preserve the meaning of the text.</p>"},{"location":"examples/Embedding_long_inputs/#Embedding-texts-that-are-longer-than-the-model's-maximum-context-length","title":"Embedding texts that are longer than the model's maximum context length\u00b6","text":"<p>OpenAI's embedding models cannot embed text that exceeds a maximum length. The maximum length varies by model, and is measured by tokens, not string length. If you are unfamiliar with tokenization, check out How to count tokens with tiktoken.</p> <p>This notebook shows how to handle texts that are longer than a model's maximum context length. We'll demonstrate using embeddings from <code>text-embedding-ada-002</code>, but the same ideas can be applied to other models and tasks. To learn more about embeddings, check out the OpenAI Embeddings Guide.</p>"},{"location":"examples/Embedding_long_inputs/#1.-Model-context-length","title":"1. Model context length\u00b6","text":"<p>First, we select the model and define a function to get embeddings from the API.</p>"},{"location":"examples/Embedding_long_inputs/#1.-Truncating-the-input-text","title":"1. Truncating the input text\u00b6","text":"<p>The simplest solution is to truncate the input text to the maximum allowed length. Because the context length is measured in tokens, we have to first tokenize the text before truncating it. The API accepts inputs both in the form of text or tokens, so as long as you are careful that you are using the appropriate encoding, there is no need to convert the tokens back into string form. Below is an example of such a truncation function.</p>"},{"location":"examples/Embedding_long_inputs/#2.-Chunking-the-input-text","title":"2. Chunking the input text\u00b6","text":"<p>Though truncation works, discarding potentially relevant text is a clear drawback. Another approach is to divide the input text into chunks and then embed each chunk individually. Then, we can either use the chunk embeddings separately, or combine them in some way, such as averaging (weighted by the size of each chunk).</p> <p>We will take a function from Python's own cookbook that breaks up a sequence into chunks.</p>"},{"location":"examples/Entity_extraction_for_long_documents/","title":"Long Document Content Extraction","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install textract\n!pip install tiktoken\n</pre> !pip install textract !pip install tiktoken In\u00a0[5]: Copied! <pre>import textract\nimport os\nimport openai\nimport tiktoken\n\n# Extract the raw text from each PDF using textract\ntext = textract.process('data/fia_f1_power_unit_financial_regulations_issue_1_-_2022-08-16.pdf', method='pdfminer').decode('utf-8')\nclean_text = text.replace(\"  \", \" \").replace(\"\\n\", \"; \").replace(';',' ')\n</pre> import textract import os import openai import tiktoken  # Extract the raw text from each PDF using textract text = textract.process('data/fia_f1_power_unit_financial_regulations_issue_1_-_2022-08-16.pdf', method='pdfminer').decode('utf-8') clean_text = text.replace(\"  \", \" \").replace(\"\\n\", \"; \").replace(';',' ') In\u00a0[28]: Copied! <pre># Example prompt - \ndocument = '&lt;document&gt;'\ntemplate_prompt=f'''Extract key pieces of information from this regulation document.\nIf a particular piece of information is not present, output \\\"Not specified\\\".\nWhen you extract a key piece of information, include the closest page number.\nUse the following format:\\n0. Who is the author\\n1. What is the amount of the \"Power Unit Cost Cap\" in USD, GBP and EUR\\n2. What is the value of External Manufacturing Costs in USD\\n3. What is the Capital Expenditure Limit in USD\\n\\nDocument: \\\"\\\"\\\"{document}\\\"\\\"\\\"\\n\\n0. Who is the author: Tom Anderson (Page 1)\\n1.'''\nprint(template_prompt)\n</pre> # Example prompt -  document = '' template_prompt=f'''Extract key pieces of information from this regulation document. If a particular piece of information is not present, output \\\"Not specified\\\". When you extract a key piece of information, include the closest page number. Use the following format:\\n0. Who is the author\\n1. What is the amount of the \"Power Unit Cost Cap\" in USD, GBP and EUR\\n2. What is the value of External Manufacturing Costs in USD\\n3. What is the Capital Expenditure Limit in USD\\n\\nDocument: \\\"\\\"\\\"{document}\\\"\\\"\\\"\\n\\n0. Who is the author: Tom Anderson (Page 1)\\n1.''' print(template_prompt) <pre>Extract key pieces of information from this regulation document.\nIf a particular piece of information is not present, output \"Not specified\".\nWhen you extract a key piece of information, include the closest page number.\nUse the following format:\n0. Who is the author\n1. What is the amount of the \"Power Unit Cost Cap\" in USD, GBP and EUR\n2. What is the value of External Manufacturing Costs in USD\n3. What is the Capital Expenditure Limit in USD\n\nDocument: \"\"\"&lt;document&gt;\"\"\"\n\n0. Who is the author: Tom Anderson (Page 1)\n1.\n</pre> In\u00a0[29]: Copied! <pre># Split a text into smaller chunks of size n, preferably ending at the end of a sentence\ndef create_chunks(text, n, tokenizer):\n    tokens = tokenizer.encode(text)\n\"\"\"Yield successive n-sized chunks from text.\"\"\"\n    i = 0\n    while i &lt; len(tokens):\n        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n        j = min(i + int(1.5 * n), len(tokens))\n        while j &gt; i + int(0.5 * n):\n            # Decode the tokens and check for full stop or newline\n            chunk = tokenizer.decode(tokens[i:j])\n            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n                break\n            j -= 1\n        # If no end of sentence found, use n tokens as the chunk size\n        if j == i + int(0.5 * n):\n            j = min(i + n, len(tokens))\n        yield tokens[i:j]\n        i = j\n\ndef extract_chunk(document,template_prompt):\n    \n    prompt=template_prompt.replace('&lt;document&gt;',document)\n\n    response = openai.Completion.create(\n    model='text-davinci-003', \n    prompt=prompt,\n    temperature=0,\n    max_tokens=1500,\n    top_p=1,\n    frequency_penalty=0,\n    presence_penalty=0\n    )\n    return \"1.\" + response['choices'][0]['text']\n</pre> # Split a text into smaller chunks of size n, preferably ending at the end of a sentence def create_chunks(text, n, tokenizer):     tokens = tokenizer.encode(text)     \"\"\"Yield successive n-sized chunks from text.\"\"\"     i = 0     while i &lt; len(tokens):         # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens         j = min(i + int(1.5 * n), len(tokens))         while j &gt; i + int(0.5 * n):             # Decode the tokens and check for full stop or newline             chunk = tokenizer.decode(tokens[i:j])             if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):                 break             j -= 1         # If no end of sentence found, use n tokens as the chunk size         if j == i + int(0.5 * n):             j = min(i + n, len(tokens))         yield tokens[i:j]         i = j  def extract_chunk(document,template_prompt):          prompt=template_prompt.replace('',document)      response = openai.Completion.create(     model='text-davinci-003',      prompt=prompt,     temperature=0,     max_tokens=1500,     top_p=1,     frequency_penalty=0,     presence_penalty=0     )     return \"1.\" + response['choices'][0]['text'] In\u00a0[\u00a0]: Copied! <pre># Initialise tokenizer\ntokenizer = tiktoken.get_encoding(\"cl100k_base\")\n\nresults = []\n    \nchunks = create_chunks(clean_text,1000,tokenizer)\ntext_chunks = [tokenizer.decode(chunk) for chunk in chunks]\n\nfor chunk in text_chunks:\n    results.append(extract_chunk(chunk,template_prompt))\n    #print(chunk)\n    print(results[-1])\n</pre> # Initialise tokenizer tokenizer = tiktoken.get_encoding(\"cl100k_base\")  results = []      chunks = create_chunks(clean_text,1000,tokenizer) text_chunks = [tokenizer.decode(chunk) for chunk in chunks]  for chunk in text_chunks:     results.append(extract_chunk(chunk,template_prompt))     #print(chunk)     print(results[-1])  In\u00a0[31]: Copied! <pre>groups = [r.split('\\n') for r in results]\n\n# zip the groups together\nzipped = list(zip(*groups))\nzipped = [x for y in zipped for x in y if \"Not specified\" not in x and \"__\" not in x]\nzipped\n</pre> groups = [r.split('\\n') for r in results]  # zip the groups together zipped = list(zip(*groups)) zipped = [x for y in zipped for x in y if \"Not specified\" not in x and \"__\" not in x] zipped Out[31]: <pre>['1. What is the amount of the \"Power Unit Cost Cap\" in USD, GBP and EUR: USD 95,000,000 (Page 2); GBP 76,459,000 (Page 2); EUR 90,210,000 (Page 2)',\n '2. What is the value of External Manufacturing Costs in USD: US Dollars 20,000,000 in respect of each of the Full Year Reporting Periods ending on 31 December 2023, 31 December 2024 and 31 December 2025, adjusted for Indexation (Page 10)',\n '3. What is the Capital Expenditure Limit in USD: US Dollars 30,000,000 (Page 32)']</pre>"},{"location":"examples/Entity_extraction_for_long_documents/#Long-Document-Content-Extraction","title":"Long Document Content Extraction\u00b6","text":"<p>GPT-3 can help us extract key figures, dates or other bits of important content from documents that are too big to fit into the context window. One approach for solving this is to chunk the document up and process each chunk separately, before combining into one list of answers.</p> <p>In this notebook we'll run through this approach:</p> <ul> <li>Load in a long PDF and pull the text out</li> <li>Create a prompt to be used to extract key bits of information</li> <li>Chunk up our document and process each chunk to pull any answers out</li> <li>Combine them at the end</li> <li>This simple approach will then be extended to three more difficult questions</li> </ul> In\u00a0[32]: Copied! <pre># Example prompt - \ntemplate_prompt=f'''Extract key pieces of information from this regulation document.\nIf a particular piece of information is not present, output \\\"Not specified\\\".\nWhen you extract a key piece of information, include the closest page number.\nUse the following format:\\n0. Who is the author\\n1. How is a Minor Overspend Breach calculated\\n2. How is a Major Overspend Breach calculated\\n3. Which years do these financial regulations apply to\\n\\nDocument: \\\"\\\"\\\"{document}\\\"\\\"\\\"\\n\\n0. Who is the author: Tom Anderson (Page 1)\\n1.'''\nprint(template_prompt)\n</pre> # Example prompt -  template_prompt=f'''Extract key pieces of information from this regulation document. If a particular piece of information is not present, output \\\"Not specified\\\". When you extract a key piece of information, include the closest page number. Use the following format:\\n0. Who is the author\\n1. How is a Minor Overspend Breach calculated\\n2. How is a Major Overspend Breach calculated\\n3. Which years do these financial regulations apply to\\n\\nDocument: \\\"\\\"\\\"{document}\\\"\\\"\\\"\\n\\n0. Who is the author: Tom Anderson (Page 1)\\n1.''' print(template_prompt) <pre>Extract key pieces of information from this regulation document.\nIf a particular piece of information is not present, output \"Not specified\".\nWhen you extract a key piece of information, include the closest page number.\nUse the following format:\n0. Who is the author\n1. How is a Minor Overspend Breach calculated\n2. How is a Major Overspend Breach calculated\n3. Which years do these financial regulations apply to\n\nDocument: \"\"\"&lt;document&gt;\"\"\"\n\n0. Who is the author: Tom Anderson (Page 1)\n1.\n</pre> In\u00a0[34]: Copied! <pre>results = []\n\nfor chunk in text_chunks:\n    results.append(extract_chunk(chunk,template_prompt))\n    \ngroups = [r.split('\\n') for r in results]\n\n# zip the groups together\nzipped = list(zip(*groups))\nzipped = [x for y in zipped for x in y if \"Not specified\" not in x and \"__\" not in x]\nzipped\n</pre> results = []  for chunk in text_chunks:     results.append(extract_chunk(chunk,template_prompt))      groups = [r.split('\\n') for r in results]  # zip the groups together zipped = list(zip(*groups)) zipped = [x for y in zipped for x in y if \"Not specified\" not in x and \"__\" not in x] zipped Out[34]: <pre>['1. How is a Minor Overspend Breach calculated: A Minor Overspend Breach arises when a Power Unit Manufacturer submits its Full Year Reporting Documentation and Relevant Costs reported therein exceed the Power Unit Cost Cap by less than 5% (Page 24)',\n '2. How is a Major Overspend Breach calculated: A Material Overspend Breach arises when a Power Unit Manufacturer submits its Full Year Reporting Documentation and Relevant Costs reported therein exceed the Power Unit Cost Cap by 5% or more (Page 25)',\n '3. Which years do these financial regulations apply to: 2026 onwards (Page 1)',\n '3. Which years do these financial regulations apply to: 2023, 2024, 2025, 2026 and subsequent Full Year Reporting Periods (Page 2)',\n '3. Which years do these financial regulations apply to: 2022-2025 (Page 6)',\n '3. Which years do these financial regulations apply to: 2023, 2024, 2025, 2026 and subsequent Full Year Reporting Periods (Page 10)',\n '3. Which years do these financial regulations apply to: 2022 (Page 14)',\n '3. Which years do these financial regulations apply to: 2022 (Page 16)',\n '3. Which years do these financial regulations apply to: 2022 (Page 19)',\n '3. Which years do these financial regulations apply to: 2022 (Page 21)',\n '3. Which years do these financial regulations apply to: 2026 onwards (Page 26)',\n '3. Which years do these financial regulations apply to: 2026 (Page 2)',\n '3. Which years do these financial regulations apply to: 2022 (Page 30)',\n '3. Which years do these financial regulations apply to: 2022 (Page 32)',\n '3. Which years do these financial regulations apply to: 2023, 2024 and 2025 (Page 1)',\n '3. Which years do these financial regulations apply to: 2022 (Page 37)',\n '3. Which years do these financial regulations apply to: 2026 onwards (Page 40)',\n '3. Which years do these financial regulations apply to: 2022 (Page 1)',\n '3. Which years do these financial regulations apply to: 2026 to 2030 seasons (Page 46)',\n '3. Which years do these financial regulations apply to: 2022 (Page 47)',\n '3. Which years do these financial regulations apply to: 2022 (Page 1)',\n '3. Which years do these financial regulations apply to: 2022 (Page 1)',\n '3. Which years do these financial regulations apply to: 2022 (Page 56)',\n '3. Which years do these financial regulations apply to: 2022 (Page 1)',\n '3. Which years do these financial regulations apply to: 2022 (Page 16)',\n '3. Which years do these financial regulations apply to: 2022 (Page 16)']</pre>"},{"location":"examples/Entity_extraction_for_long_documents/#Approach","title":"Approach\u00b6","text":"<ul> <li>Setup: Take a PDF, a Formula 1 Financial Regulation document on Power Units, and extract the text from it for entity extraction. We'll use this to try to extract answers that are buried in the content.</li> <li>Simple Entity Extraction: Extract key bits of information from chunks of a document by:<ul> <li>Creating a template prompt with our questions and an example of the format it expects</li> <li>Create a function to take a chunk of text as input, combine with the prompt and get a response</li> <li>Run a script to chunk the text, extract answers and output them for parsing</li> </ul> </li> <li>Complex Entity Extraction: Ask some more difficult questions which require tougher reasoning to work out</li> </ul>"},{"location":"examples/Entity_extraction_for_long_documents/#Setup","title":"Setup\u00b6","text":""},{"location":"examples/Entity_extraction_for_long_documents/#Simple-Entity-Extraction","title":"Simple Entity Extraction\u00b6","text":""},{"location":"examples/Entity_extraction_for_long_documents/#Complex-Entity-Extraction","title":"Complex Entity Extraction\u00b6","text":""},{"location":"examples/Entity_extraction_for_long_documents/#Consolidation","title":"Consolidation\u00b6","text":"<p>We've been able to extract the first two answers safely, while the third was confounded by the date that appeared on every page, though the correct answer is in there as well.</p> <p>To tune this further you can consider experimenting with:</p> <ul> <li>A more descriptive or specific prompt</li> <li>If you have sufficient training data, fine-tuning a model to find a set of outputs very well</li> <li>The way you chunk your data - we have gone for 1000 tokens with no overlap, but more intelligent chunking that breaks info into sections, cuts by tokens or similar may get better results</li> </ul> <p>However, with minimal tuning we have now answered 6 questions of varying difficulty using the contents of a long document, and have a reusable approach that we can apply to any long document requiring entity extraction. Look forward to seeing what you can do with this!</p>"},{"location":"examples/Fine-tuned_classification/","title":"Fine tuning classification example","text":"In\u00a0[1]: Copied! <pre>from sklearn.datasets import fetch_20newsgroups\nimport pandas as pd\nimport openai\n\ncategories = ['rec.sport.baseball', 'rec.sport.hockey']\nsports_dataset = fetch_20newsgroups(subset='train', shuffle=True, random_state=42, categories=categories)\n</pre> from sklearn.datasets import fetch_20newsgroups import pandas as pd import openai  categories = ['rec.sport.baseball', 'rec.sport.hockey'] sports_dataset = fetch_20newsgroups(subset='train', shuffle=True, random_state=42, categories=categories) In\u00a0[2]: Copied! <pre>print(sports_dataset['data'][0])\n</pre> print(sports_dataset['data'][0]) <pre>From: dougb@comm.mot.com (Doug Bank)\nSubject: Re: Info needed for Cleveland tickets\nReply-To: dougb@ecs.comm.mot.com\nOrganization: Motorola Land Mobile Products Sector\nDistribution: usa\nNntp-Posting-Host: 145.1.146.35\nLines: 17\n\nIn article &lt;1993Apr1.234031.4950@leland.Stanford.EDU&gt;, bohnert@leland.Stanford.EDU (matthew bohnert) writes:\n\n|&gt; I'm going to be in Cleveland Thursday, April 15 to Sunday, April 18.\n|&gt; Does anybody know if the Tribe will be in town on those dates, and\n|&gt; if so, who're they playing and if tickets are available?\n\nThe tribe will be in town from April 16 to the 19th.\nThere are ALWAYS tickets available! (Though they are playing Toronto,\nand many Toronto fans make the trip to Cleveland as it is easier to\nget tickets in Cleveland than in Toronto.  Either way, I seriously\ndoubt they will sell out until the end of the season.)\n\n-- \nDoug Bank                       Private Systems Division\ndougb@ecs.comm.mot.com          Motorola Communications Sector\ndougb@nwu.edu                   Schaumburg, Illinois\ndougb@casbah.acns.nwu.edu       708-576-8207                    \n\n</pre> In\u00a0[3]: Copied! <pre>sports_dataset.target_names[sports_dataset['target'][0]]\n</pre> sports_dataset.target_names[sports_dataset['target'][0]]  Out[3]: <pre>'rec.sport.baseball'</pre> In\u00a0[4]: Copied! <pre>len_all, len_baseball, len_hockey = len(sports_dataset.data), len([e for e in sports_dataset.target if e == 0]), len([e for e in sports_dataset.target if e == 1])\nprint(f\"Total examples: {len_all}, Baseball examples: {len_baseball}, Hockey examples: {len_hockey}\")\n</pre> len_all, len_baseball, len_hockey = len(sports_dataset.data), len([e for e in sports_dataset.target if e == 0]), len([e for e in sports_dataset.target if e == 1]) print(f\"Total examples: {len_all}, Baseball examples: {len_baseball}, Hockey examples: {len_hockey}\") <pre>Total examples: 1197, Baseball examples: 597, Hockey examples: 600\n</pre> <p>One sample from the baseball category can be seen above. It is an email to a mailing list. We can observe that we have 1197 examples in total, which are evenly split between the two sports.</p> In\u00a0[5]: Copied! <pre>import pandas as pd\n\nlabels = [sports_dataset.target_names[x].split('.')[-1] for x in sports_dataset['target']]\ntexts = [text.strip() for text in sports_dataset['data']]\ndf = pd.DataFrame(zip(texts, labels), columns = ['prompt','completion']) #[:300]\ndf.head()\n</pre> import pandas as pd  labels = [sports_dataset.target_names[x].split('.')[-1] for x in sports_dataset['target']] texts = [text.strip() for text in sports_dataset['data']] df = pd.DataFrame(zip(texts, labels), columns = ['prompt','completion']) #[:300] df.head() Out[5]: prompt completion 0 From: dougb@comm.mot.com (Doug Bank)\\nSubject:... baseball 1 From: gld@cunixb.cc.columbia.edu (Gary L Dare)... hockey 2 From: rudy@netcom.com (Rudy Wade)\\nSubject: Re... baseball 3 From: monack@helium.gas.uug.arizona.edu (david... hockey 4 Subject: Let it be Known\\nFrom: &lt;ISSBTL@BYUVM.... baseball <p>Both baseball and hockey are single tokens. We save the dataset as a jsonl file.</p> In\u00a0[6]: Copied! <pre>df.to_json(\"sport2.jsonl\", orient='records', lines=True)\n</pre> df.to_json(\"sport2.jsonl\", orient='records', lines=True) In\u00a0[7]: Copied! <pre>!pip install --upgrade openai\n</pre> !pip install --upgrade openai In\u00a0[8]: Copied! <pre>!openai tools fine_tunes.prepare_data -f sport2.jsonl -q\n</pre> !openai tools fine_tunes.prepare_data -f sport2.jsonl -q <pre>Analyzing...\n\n- Your file contains 1197 prompt-completion pairs\n- Based on your data it seems like you're trying to fine-tune a model for classification\n- For classification, we recommend you try one of the faster and cheaper models, such as `ada`\n- For classification, you can estimate the expected model performance by keeping a held out dataset, which is not used for training\n- There are 11 examples that are very long. These are rows: [134, 200, 281, 320, 404, 595, 704, 838, 1113, 1139, 1174]\nFor conditional generation, and for classification the examples shouldn't be longer than 2048 tokens.\n- Your data does not contain a common separator at the end of your prompts. Having a separator string appended to the end of the prompt makes it clearer to the fine-tuned model where the completion should begin. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples. If you intend to do open-ended generation, then you should leave the prompts empty\n- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n\nBased on the analysis we will perform the following actions:\n- [Recommended] Remove 11 long examples [Y/n]: Y\n- [Recommended] Add a suffix separator `\\n\\n###\\n\\n` to all prompts [Y/n]: Y\n- [Recommended] Add a whitespace character to the beginning of the completion [Y/n]: Y\n- [Recommended] Would you like to split into training and validation set? [Y/n]: Y\n\n\nYour data will be written to a new JSONL file. Proceed [Y/n]: Y\n\nWrote modified files to `sport2_prepared_train.jsonl` and `sport2_prepared_valid.jsonl`\nFeel free to take a look!\n\nNow use that file when fine-tuning:\n&gt; openai api fine_tunes.create -t \"sport2_prepared_train.jsonl\" -v \"sport2_prepared_valid.jsonl\" --compute_classification_metrics --classification_positive_class \" baseball\"\n\nAfter you\u2019ve fine-tuned a model, remember that your prompt has to end with the indicator string `\\n\\n###\\n\\n` for the model to start generating completions, rather than continuing with the prompt.\nOnce your model starts training, it'll approximately take 30.8 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n</pre> <p>The tool helpfully suggests a few improvements to the dataset and splits the dataset into training and validation set.</p> <p>A suffix between a prompt and a completion is necessary to tell the model that the input text has stopped, and that it now needs to predict the class. Since we use the same separator in each example, the model is able to learn that it is meant to predict either baseball or hockey following the separator. A whitespace prefix in completions is useful, as most word tokens are tokenized with a space prefix. The tool also recognized that this is likely a classification task, so it suggested to split the dataset into training and validation datasets. This will allow us to easily measure expected performance on new data.</p> In\u00a0[9]: Copied! <pre>!openai api fine_tunes.create -t \"sport2_prepared_train.jsonl\" -v \"sport2_prepared_valid.jsonl\" --compute_classification_metrics --classification_positive_class \" baseball\" -m ada\n</pre> !openai api fine_tunes.create -t \"sport2_prepared_train.jsonl\" -v \"sport2_prepared_valid.jsonl\" --compute_classification_metrics --classification_positive_class \" baseball\" -m ada <pre>Upload progress: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.52M/1.52M [00:00&lt;00:00, 1.81Mit/s]\nUploaded file from sport2_prepared_train.jsonl: file-Dxx2xJqyjcwlhfDHpZdmCXlF\nUpload progress: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 388k/388k [00:00&lt;00:00, 507kit/s]\nUploaded file from sport2_prepared_valid.jsonl: file-Mvb8YAeLnGdneSAFcfiVcgcN\nCreated fine-tune: ft-2zaA7qi0rxJduWQpdvOvmGn3\nStreaming events until fine-tuning is complete...\n\n(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n[2021-07-30 13:15:50] Created fine-tune: ft-2zaA7qi0rxJduWQpdvOvmGn3\n[2021-07-30 13:15:52] Fine-tune enqueued. Queue number: 0\n[2021-07-30 13:15:56] Fine-tune started\n[2021-07-30 13:18:55] Completed epoch 1/4\n[2021-07-30 13:20:47] Completed epoch 2/4\n[2021-07-30 13:22:40] Completed epoch 3/4\n[2021-07-30 13:24:31] Completed epoch 4/4\n[2021-07-30 13:26:22] Uploaded model: ada:ft-openai-2021-07-30-12-26-20\n[2021-07-30 13:26:27] Uploaded result file: file-6Ki9RqLQwkChGsr9CHcr1ncg\n[2021-07-30 13:26:28] Fine-tune succeeded\n\nJob complete! Status: succeeded \ud83c\udf89\nTry out your fine-tuned model:\n\nopenai api completions.create -m ada:ft-openai-2021-07-30-12-26-20 -p &lt;YOUR_PROMPT&gt;\n</pre> <p>The model is successfully trained in about ten minutes. We can see the model name is <code>ada:ft-openai-2021-07-30-12-26-20</code>, which we can use for doing inference.</p> In\u00a0[10]: Copied! <pre>!openai api fine_tunes.results -i ft-2zaA7qi0rxJduWQpdvOvmGn3 &gt; result.csv\n</pre> !openai api fine_tunes.results -i ft-2zaA7qi0rxJduWQpdvOvmGn3 &gt; result.csv In\u00a0[11]: Copied! <pre>results = pd.read_csv('result.csv')\nresults[results['classification/accuracy'].notnull()].tail(1)\n</pre> results = pd.read_csv('result.csv') results[results['classification/accuracy'].notnull()].tail(1) Out[11]: step elapsed_tokens elapsed_examples training_loss training_sequence_accuracy training_token_accuracy classification/accuracy classification/precision classification/recall classification/auroc classification/auprc classification/f1.0 validation_loss validation_sequence_accuracy validation_token_accuracy 929 930 3027688 3720 0.044408 1.0 1.0 0.991597 0.983471 1.0 1.0 1.0 0.991667 NaN NaN NaN <p>The accuracy reaches 99.6%. On the plot below we can see how accuracy on the validation set increases during the training run.</p> In\u00a0[12]: Copied! <pre>results[results['classification/accuracy'].notnull()]['classification/accuracy'].plot()\n</pre> results[results['classification/accuracy'].notnull()]['classification/accuracy'].plot() Out[12]: <pre>&lt;AxesSubplot:&gt;</pre> In\u00a0[13]: Copied! <pre>test = pd.read_json('sport2_prepared_valid.jsonl', lines=True)\ntest.head()\n</pre> test = pd.read_json('sport2_prepared_valid.jsonl', lines=True) test.head() Out[13]: prompt completion 0 From: gld@cunixb.cc.columbia.edu (Gary L Dare)... hockey 1 From: smorris@venus.lerc.nasa.gov (Ron Morris ... hockey 2 From: golchowy@alchemy.chem.utoronto.ca (Geral... hockey 3 From: krattige@hpcc01.corp.hp.com (Kim Krattig... baseball 4 From: warped@cs.montana.edu (Doug Dolven)\\nSub... baseball <p>We need to use the same separator following the prompt which we used during fine-tuning. In this case it is <code>\\n\\n###\\n\\n</code>. Since we're concerned with classification, we want the temperature to be as low as possible, and we only require one token completion to determine the prediction of the model.</p> In\u00a0[14]: Copied! <pre>ft_model = 'ada:ft-openai-2021-07-30-12-26-20'\nres = openai.Completion.create(model=ft_model, prompt=test['prompt'][0] + '\\n\\n###\\n\\n', max_tokens=1, temperature=0)\nres['choices'][0]['text']\n</pre> ft_model = 'ada:ft-openai-2021-07-30-12-26-20' res = openai.Completion.create(model=ft_model, prompt=test['prompt'][0] + '\\n\\n###\\n\\n', max_tokens=1, temperature=0) res['choices'][0]['text']  Out[14]: <pre>' hockey'</pre> <p>To get the log probabilities, we can specify logprobs parameter on the completion request</p> In\u00a0[15]: Copied! <pre>res = openai.Completion.create(model=ft_model, prompt=test['prompt'][0] + '\\n\\n###\\n\\n', max_tokens=1, temperature=0, logprobs=2)\nres['choices'][0]['logprobs']['top_logprobs'][0]\n</pre> res = openai.Completion.create(model=ft_model, prompt=test['prompt'][0] + '\\n\\n###\\n\\n', max_tokens=1, temperature=0, logprobs=2) res['choices'][0]['logprobs']['top_logprobs'][0] Out[15]: <pre>&lt;OpenAIObject at 0x7fe114e435c8&gt; JSON: {\n  \" baseball\": -7.6311407,\n  \" hockey\": -0.0006307676\n}</pre> <p>We can see that the model predicts hockey as a lot more likely than baseball, which is the correct prediction. By requesting log_probs, we can see the prediction (log) probability for each class.</p> In\u00a0[16]: Copied! <pre>sample_hockey_tweet = \"\"\"Thank you to the \n@Canes\n and all you amazing Caniacs that have been so supportive! You guys are some of the best fans in the NHL without a doubt! Really excited to start this new chapter in my career with the \n@DetroitRedWings\n !!\"\"\"\nres = openai.Completion.create(model=ft_model, prompt=sample_hockey_tweet + '\\n\\n###\\n\\n', max_tokens=1, temperature=0, logprobs=2)\nres['choices'][0]['text']\n</pre> sample_hockey_tweet = \"\"\"Thank you to the  @Canes  and all you amazing Caniacs that have been so supportive! You guys are some of the best fans in the NHL without a doubt! Really excited to start this new chapter in my career with the  @DetroitRedWings  !!\"\"\" res = openai.Completion.create(model=ft_model, prompt=sample_hockey_tweet + '\\n\\n###\\n\\n', max_tokens=1, temperature=0, logprobs=2) res['choices'][0]['text'] Out[16]: <pre>' hockey'</pre> In\u00a0[17]: Copied! <pre>sample_baseball_tweet=\"\"\"BREAKING: The Tampa Bay Rays are finalizing a deal to acquire slugger Nelson Cruz from the Minnesota Twins, sources tell ESPN.\"\"\"\nres = openai.Completion.create(model=ft_model, prompt=sample_baseball_tweet + '\\n\\n###\\n\\n', max_tokens=1, temperature=0, logprobs=2)\nres['choices'][0]['text']\n</pre> sample_baseball_tweet=\"\"\"BREAKING: The Tampa Bay Rays are finalizing a deal to acquire slugger Nelson Cruz from the Minnesota Twins, sources tell ESPN.\"\"\" res = openai.Completion.create(model=ft_model, prompt=sample_baseball_tweet + '\\n\\n###\\n\\n', max_tokens=1, temperature=0, logprobs=2) res['choices'][0]['text'] Out[17]: <pre>' baseball'</pre>"},{"location":"examples/Fine-tuned_classification/#Fine-tuning-classification-example","title":"Fine tuning classification example\u00b6","text":"<p>We will fine-tune an ada classifier to distinguish between the two sports: Baseball and Hockey.</p>"},{"location":"examples/Fine-tuned_classification/#Data-exploration","title":"Data exploration\u00b6","text":"<p>The newsgroup dataset can be loaded using sklearn. First we will look at the data itself:</p>"},{"location":"examples/Fine-tuned_classification/#Data-Preparation","title":"Data Preparation\u00b6","text":"<p>We transform the dataset into a pandas dataframe, with a column for prompt and completion. The prompt contains the email from the mailing list, and the completion is a name of the sport, either hockey or baseball. For demonstration purposes only and speed of fine-tuning we take only 300 examples. In a real use case the more examples the better the performance.</p>"},{"location":"examples/Fine-tuned_classification/#Data-Preparation-tool","title":"Data Preparation tool\u00b6","text":"<p>We can now use a data preparation tool which will suggest a few improvements to our dataset before fine-tuning. Before launching the tool we update the openai library to ensure we're using the latest data preparation tool. We additionally specify <code>-q</code> which auto-accepts all suggestions.</p>"},{"location":"examples/Fine-tuned_classification/#Fine-tuning","title":"Fine-tuning\u00b6","text":"<p>The tool suggests we run the following command to train the dataset. Since this is a classification task, we would like to know what the generalization performance on the provided validation set is for our classification use case. The tool suggests to add <code>--compute_classification_metrics --classification_positive_class \" baseball\"</code> in order to compute the classification metrics.</p> <p>We can simply copy the suggested command from the CLI tool. We specifically add <code>-m ada</code> to fine-tune a cheaper and faster ada model, which is usually comperable in performance to slower and more expensive models on classification use cases.</p>"},{"location":"examples/Fine-tuned_classification/#%5BAdvanced%5D-Results-and-expected-model-performance","title":"[Advanced] Results and expected model performance\u00b6","text":"<p>We can now download the results file to observe the expected performance on a held out validation set.</p>"},{"location":"examples/Fine-tuned_classification/#Using-the-model","title":"Using the model\u00b6","text":"<p>We can now call the model to get the predictions.</p>"},{"location":"examples/Fine-tuned_classification/#Generalization","title":"Generalization\u00b6","text":"<p>Interestingly, our fine-tuned classifier is quite versatile. Despite being trained on emails to different mailing lists, it also successfully predicts tweets.</p>"},{"location":"examples/Get_embeddings/","title":"Get embeddings","text":"In\u00a0[1]: Copied! <pre>import openai\n\nembedding = openai.Embedding.create(\n    input=\"Your text goes here\", model=\"text-embedding-ada-002\"\n)[\"data\"][0][\"embedding\"]\nlen(embedding)\n</pre> import openai  embedding = openai.Embedding.create(     input=\"Your text goes here\", model=\"text-embedding-ada-002\" )[\"data\"][0][\"embedding\"] len(embedding)  Out[1]: <pre>1536</pre> In\u00a0[2]: Copied! <pre>import openai\nfrom tenacity import retry, wait_random_exponential, stop_after_attempt\n\n\n@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\ndef get_embedding(text: str, model=\"text-embedding-ada-002\") -&gt; list[float]:\n    return openai.Embedding.create(input=[text], model=model)[\"data\"][0][\"embedding\"]\n\n\nembedding = get_embedding(\"Your text goes here\", model=\"text-embedding-ada-002\")\nprint(len(embedding))\n</pre> import openai from tenacity import retry, wait_random_exponential, stop_after_attempt   @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6)) def get_embedding(text: str, model=\"text-embedding-ada-002\") -&gt; list[float]:     return openai.Embedding.create(input=[text], model=model)[\"data\"][0][\"embedding\"]   embedding = get_embedding(\"Your text goes here\", model=\"text-embedding-ada-002\") print(len(embedding))  <pre>1536\n</pre>"},{"location":"examples/Get_embeddings/#Get-embeddings","title":"Get embeddings\u00b6","text":"<p>The function <code>get_embedding</code> will give us an embedding for an input text.</p>"},{"location":"examples/How_to_count_tokens_with_tiktoken/","title":"How to count tokens with tiktoken","text":"In\u00a0[1]: Copied! <pre>%pip install --upgrade tiktoken\n</pre> %pip install --upgrade tiktoken <pre>Requirement already satisfied: tiktoken in /Users/ted/.virtualenvs/openai/lib/python3.9/site-packages (0.3.2)\nRequirement already satisfied: regex&gt;=2022.1.18 in /Users/ted/.virtualenvs/openai/lib/python3.9/site-packages (from tiktoken) (2022.10.31)\nRequirement already satisfied: requests&gt;=2.26.0 in /Users/ted/.virtualenvs/openai/lib/python3.9/site-packages (from tiktoken) (2.28.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/ted/.virtualenvs/openai/lib/python3.9/site-packages (from requests&gt;=2.26.0-&gt;tiktoken) (3.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /Users/ted/.virtualenvs/openai/lib/python3.9/site-packages (from requests&gt;=2.26.0-&gt;tiktoken) (2.0.9)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /Users/ted/.virtualenvs/openai/lib/python3.9/site-packages (from requests&gt;=2.26.0-&gt;tiktoken) (2021.10.8)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /Users/ted/.virtualenvs/openai/lib/python3.9/site-packages (from requests&gt;=2.26.0-&gt;tiktoken) (1.26.7)\nNote: you may need to restart the kernel to use updated packages.\n</pre> In\u00a0[2]: Copied! <pre>import tiktoken\n</pre> import tiktoken  In\u00a0[3]: Copied! <pre>encoding = tiktoken.get_encoding(\"cl100k_base\")\n</pre> encoding = tiktoken.get_encoding(\"cl100k_base\")  <p>Use <code>tiktoken.encoding_for_model()</code> to automatically load the correct encoding for a given model name.</p> In\u00a0[4]: Copied! <pre>encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n</pre> encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\") <p>The <code>.encode()</code> method converts a text string into a list of token integers.</p> In\u00a0[5]: Copied! <pre>encoding.encode(\"tiktoken is great!\")\n</pre> encoding.encode(\"tiktoken is great!\")  Out[5]: <pre>[83, 1609, 5963, 374, 2294, 0]</pre> <p>Count tokens by counting the length of the list returned by <code>.encode()</code>.</p> In\u00a0[6]: Copied! <pre>def num_tokens_from_string(string: str, encoding_name: str) -&gt; int:\n\"\"\"Returns the number of tokens in a text string.\"\"\"\n    encoding = tiktoken.get_encoding(encoding_name)\n    num_tokens = len(encoding.encode(string))\n    return num_tokens\n</pre> def num_tokens_from_string(string: str, encoding_name: str) -&gt; int:     \"\"\"Returns the number of tokens in a text string.\"\"\"     encoding = tiktoken.get_encoding(encoding_name)     num_tokens = len(encoding.encode(string))     return num_tokens  In\u00a0[7]: Copied! <pre>num_tokens_from_string(\"tiktoken is great!\", \"cl100k_base\")\n</pre> num_tokens_from_string(\"tiktoken is great!\", \"cl100k_base\")  Out[7]: <pre>6</pre> <p><code>.decode()</code> converts a list of token integers to a string.</p> In\u00a0[8]: Copied! <pre>encoding.decode([83, 1609, 5963, 374, 2294, 0])\n</pre> encoding.decode([83, 1609, 5963, 374, 2294, 0])  Out[8]: <pre>'tiktoken is great!'</pre> <p>Warning: although <code>.decode()</code> can be applied to single tokens, beware that it can be lossy for tokens that aren't on utf-8 boundaries.</p> <p>For single tokens, <code>.decode_single_token_bytes()</code> safely converts a single integer token to the bytes it represents.</p> In\u00a0[9]: Copied! <pre>[encoding.decode_single_token_bytes(token) for token in [83, 1609, 5963, 374, 2294, 0]]\n</pre> [encoding.decode_single_token_bytes(token) for token in [83, 1609, 5963, 374, 2294, 0]]  Out[9]: <pre>[b't', b'ik', b'token', b' is', b' great', b'!']</pre> <p>(The <code>b</code> in front of the strings indicates that the strings are byte strings.)</p> In\u00a0[10]: Copied! <pre>def compare_encodings(example_string: str) -&gt; None:\n\"\"\"Prints a comparison of three string encodings.\"\"\"\n    # print the example string\n    print(f'\\nExample string: \"{example_string}\"')\n    # for each encoding, print the # of tokens, the token integers, and the token bytes\n    for encoding_name in [\"gpt2\", \"p50k_base\", \"cl100k_base\"]:\n        encoding = tiktoken.get_encoding(encoding_name)\n        token_integers = encoding.encode(example_string)\n        num_tokens = len(token_integers)\n        token_bytes = [encoding.decode_single_token_bytes(token) for token in token_integers]\n        print()\n        print(f\"{encoding_name}: {num_tokens} tokens\")\n        print(f\"token integers: {token_integers}\")\n        print(f\"token bytes: {token_bytes}\")\n</pre> def compare_encodings(example_string: str) -&gt; None:     \"\"\"Prints a comparison of three string encodings.\"\"\"     # print the example string     print(f'\\nExample string: \"{example_string}\"')     # for each encoding, print the # of tokens, the token integers, and the token bytes     for encoding_name in [\"gpt2\", \"p50k_base\", \"cl100k_base\"]:         encoding = tiktoken.get_encoding(encoding_name)         token_integers = encoding.encode(example_string)         num_tokens = len(token_integers)         token_bytes = [encoding.decode_single_token_bytes(token) for token in token_integers]         print()         print(f\"{encoding_name}: {num_tokens} tokens\")         print(f\"token integers: {token_integers}\")         print(f\"token bytes: {token_bytes}\")          In\u00a0[11]: Copied! <pre>compare_encodings(\"antidisestablishmentarianism\")\n</pre> compare_encodings(\"antidisestablishmentarianism\")  <pre>\nExample string: \"antidisestablishmentarianism\"\n\ngpt2: 5 tokens\ntoken integers: [415, 29207, 44390, 3699, 1042]\ntoken bytes: [b'ant', b'idis', b'establishment', b'arian', b'ism']\n\np50k_base: 5 tokens\ntoken integers: [415, 29207, 44390, 3699, 1042]\ntoken bytes: [b'ant', b'idis', b'establishment', b'arian', b'ism']\n\ncl100k_base: 6 tokens\ntoken integers: [519, 85342, 34500, 479, 8997, 2191]\ntoken bytes: [b'ant', b'idis', b'establish', b'ment', b'arian', b'ism']\n</pre> In\u00a0[12]: Copied! <pre>compare_encodings(\"2 + 2 = 4\")\n</pre> compare_encodings(\"2 + 2 = 4\")  <pre>\nExample string: \"2 + 2 = 4\"\n\ngpt2: 5 tokens\ntoken integers: [17, 1343, 362, 796, 604]\ntoken bytes: [b'2', b' +', b' 2', b' =', b' 4']\n\np50k_base: 5 tokens\ntoken integers: [17, 1343, 362, 796, 604]\ntoken bytes: [b'2', b' +', b' 2', b' =', b' 4']\n\ncl100k_base: 7 tokens\ntoken integers: [17, 489, 220, 17, 284, 220, 19]\ntoken bytes: [b'2', b' +', b' ', b'2', b' =', b' ', b'4']\n</pre> In\u00a0[13]: Copied! <pre>compare_encodings(\"\u304a\u8a95\u751f\u65e5\u304a\u3081\u3067\u3068\u3046\")\n</pre> compare_encodings(\"\u304a\u8a95\u751f\u65e5\u304a\u3081\u3067\u3068\u3046\")  <pre>\nExample string: \"\u304a\u8a95\u751f\u65e5\u304a\u3081\u3067\u3068\u3046\"\n\ngpt2: 14 tokens\ntoken integers: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557]\ntoken bytes: [b'\\xe3\\x81', b'\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97', b'\\xa5', b'\\xe3\\x81', b'\\x8a', b'\\xe3\\x82', b'\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8', b'\\xe3\\x81\\x86']\n\np50k_base: 14 tokens\ntoken integers: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557]\ntoken bytes: [b'\\xe3\\x81', b'\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97', b'\\xa5', b'\\xe3\\x81', b'\\x8a', b'\\xe3\\x82', b'\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8', b'\\xe3\\x81\\x86']\n\ncl100k_base: 9 tokens\ntoken integers: [33334, 45918, 243, 21990, 9080, 33334, 62004, 16556, 78699]\ntoken bytes: [b'\\xe3\\x81\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97\\xa5', b'\\xe3\\x81\\x8a', b'\\xe3\\x82\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8\\xe3\\x81\\x86']\n</pre> In\u00a0[14]: Copied! <pre>def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n\"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n    try:\n        encoding = tiktoken.encoding_for_model(model)\n    except KeyError:\n        print(\"Warning: model not found. Using cl100k_base encoding.\")\n        encoding = tiktoken.get_encoding(\"cl100k_base\")\n    if model == \"gpt-3.5-turbo\":\n        print(\"Warning: gpt-3.5-turbo may change over time. Returning num tokens assuming gpt-3.5-turbo-0301.\")\n        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\")\n    elif model == \"gpt-4\":\n        print(\"Warning: gpt-4 may change over time. Returning num tokens assuming gpt-4-0314.\")\n        return num_tokens_from_messages(messages, model=\"gpt-4-0314\")\n    elif model == \"gpt-3.5-turbo-0301\":\n        tokens_per_message = 4  # every message follows &lt;im_start&gt;{role/name}\\n{content}&lt;im_end&gt;\\n\n        tokens_per_name = -1  # if there's a name, the role is omitted\n    elif model == \"gpt-4-0314\":\n        tokens_per_message = 3\n        tokens_per_name = 1\n    else:\n        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n    num_tokens = 0\n    for message in messages:\n        num_tokens += tokens_per_message\n        for key, value in message.items():\n            num_tokens += len(encoding.encode(value))\n            if key == \"name\":\n                num_tokens += tokens_per_name\n    num_tokens += 2  # every reply is primed with &lt;im_start&gt;assistant\n    return num_tokens\n</pre> def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):     \"\"\"Returns the number of tokens used by a list of messages.\"\"\"     try:         encoding = tiktoken.encoding_for_model(model)     except KeyError:         print(\"Warning: model not found. Using cl100k_base encoding.\")         encoding = tiktoken.get_encoding(\"cl100k_base\")     if model == \"gpt-3.5-turbo\":         print(\"Warning: gpt-3.5-turbo may change over time. Returning num tokens assuming gpt-3.5-turbo-0301.\")         return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\")     elif model == \"gpt-4\":         print(\"Warning: gpt-4 may change over time. Returning num tokens assuming gpt-4-0314.\")         return num_tokens_from_messages(messages, model=\"gpt-4-0314\")     elif model == \"gpt-3.5-turbo-0301\":         tokens_per_message = 4  # every message follows {role/name}\\n{content}\\n         tokens_per_name = -1  # if there's a name, the role is omitted     elif model == \"gpt-4-0314\":         tokens_per_message = 3         tokens_per_name = 1     else:         raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")     num_tokens = 0     for message in messages:         num_tokens += tokens_per_message         for key, value in message.items():             num_tokens += len(encoding.encode(value))             if key == \"name\":                 num_tokens += tokens_per_name     num_tokens += 2  # every reply is primed with assistant     return num_tokens  In\u00a0[15]: Copied! <pre># let's verify the function above matches the OpenAI API response\n\nimport openai\n\nexample_messages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n    },\n    {\n        \"role\": \"system\",\n        \"name\": \"example_user\",\n        \"content\": \"New synergies will help drive top-line growth.\",\n    },\n    {\n        \"role\": \"system\",\n        \"name\": \"example_assistant\",\n        \"content\": \"Things working well together will increase revenue.\",\n    },\n    {\n        \"role\": \"system\",\n        \"name\": \"example_user\",\n        \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n    },\n    {\n        \"role\": \"system\",\n        \"name\": \"example_assistant\",\n        \"content\": \"Let's talk later when we're less busy about how to do better.\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n    },\n]\n\nfor model in [\"gpt-3.5-turbo-0301\", \"gpt-4-0314\"]:\n    print(model)\n    # example token count from the function defined above\n    print(f\"{num_tokens_from_messages(example_messages, model)} prompt tokens counted by num_tokens_from_messages().\")\n    # example token count from the OpenAI API\n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=example_messages,\n        temperature=0,\n        max_tokens=1  # we're only counting input tokens here, so let's not waste tokens on the output\n    )\n    print(f'{response[\"usage\"][\"prompt_tokens\"]} prompt tokens counted by the OpenAI API.')\n    print()\n</pre> # let's verify the function above matches the OpenAI API response  import openai  example_messages = [     {         \"role\": \"system\",         \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",     },     {         \"role\": \"system\",         \"name\": \"example_user\",         \"content\": \"New synergies will help drive top-line growth.\",     },     {         \"role\": \"system\",         \"name\": \"example_assistant\",         \"content\": \"Things working well together will increase revenue.\",     },     {         \"role\": \"system\",         \"name\": \"example_user\",         \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",     },     {         \"role\": \"system\",         \"name\": \"example_assistant\",         \"content\": \"Let's talk later when we're less busy about how to do better.\",     },     {         \"role\": \"user\",         \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",     }, ]  for model in [\"gpt-3.5-turbo-0301\", \"gpt-4-0314\"]:     print(model)     # example token count from the function defined above     print(f\"{num_tokens_from_messages(example_messages, model)} prompt tokens counted by num_tokens_from_messages().\")     # example token count from the OpenAI API     response = openai.ChatCompletion.create(         model=model,         messages=example_messages,         temperature=0,         max_tokens=1  # we're only counting input tokens here, so let's not waste tokens on the output     )     print(f'{response[\"usage\"][\"prompt_tokens\"]} prompt tokens counted by the OpenAI API.')     print()  <pre>gpt-3.5-turbo-0301\n126 prompt tokens counted by num_tokens_from_messages().\n126 prompt tokens counted by the OpenAI API.\n\ngpt-4-0314\n128 prompt tokens counted by num_tokens_from_messages().\n128 prompt tokens counted by the OpenAI API.\n\n</pre>"},{"location":"examples/How_to_count_tokens_with_tiktoken/#How-to-count-tokens-with-tiktoken","title":"How to count tokens with tiktoken\u00b6","text":"<p><code>tiktoken</code> is a fast open-source tokenizer by OpenAI.</p> <p>Given a text string (e.g., <code>\"tiktoken is great!\"</code>) and an encoding (e.g., <code>\"cl100k_base\"</code>), a tokenizer can split the text string into a list of tokens (e.g., <code>[\"t\", \"ik\", \"token\", \" is\", \" great\", \"!\"]</code>).</p> <p>Splitting text strings into tokens is useful because GPT models see text in the form of tokens. Knowing how many tokens are in a text string can tell you (a) whether the string is too long for a text model to process and (b) how much an OpenAI API call costs (as usage is priced by token). Different models use different encodings.</p>"},{"location":"examples/How_to_count_tokens_with_tiktoken/#Encodings","title":"Encodings\u00b6","text":"<p>Encodings specify how text is converted into tokens. Different models use different encodings.</p> <p><code>tiktoken</code> supports three encodings used by OpenAI models:</p> Encoding name OpenAI models <code>cl100k_base</code> ChatGPT models, <code>text-embedding-ada-002</code> <code>p50k_base</code> Code models, <code>text-davinci-002</code>, <code>text-davinci-003</code> <code>r50k_base</code> (or <code>gpt2</code>) GPT-3 models like <code>davinci</code> <p>You can retrieve the encoding for a model using <code>tiktoken.encoding_for_model()</code> as follows:</p> <pre>encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n</pre> <p><code>p50k_base</code> overlaps substantially with <code>r50k_base</code>, and for non-code applications, they will usually give the same tokens.</p>"},{"location":"examples/How_to_count_tokens_with_tiktoken/#Tokenizer-libraries-by-language","title":"Tokenizer libraries by language\u00b6","text":"<p>For <code>cl100k_base</code> and <code>p50k_base</code> encodings, <code>tiktoken</code> is the only tokenizer available as of March 2023.</p> <ul> <li>Python: tiktoken</li> </ul> <p>For <code>r50k_base</code> (<code>gpt2</code>) encodings, tokenizers are available in many languages.</p> <ul> <li>Python: tiktoken (or alternatively GPT2TokenizerFast)</li> <li>JavaScript: gpt-3-encoder</li> <li>.NET / C#: GPT Tokenizer</li> <li>Java: gpt2-tokenizer-java</li> <li>PHP: GPT-3-Encoder-PHP</li> </ul> <p>(OpenAI makes no endorsements or guarantees of third-party libraries.)</p>"},{"location":"examples/How_to_count_tokens_with_tiktoken/#How-strings-are-typically-tokenized","title":"How strings are typically tokenized\u00b6","text":"<p>In English, tokens commonly range in length from one character to one word (e.g., <code>\"t\"</code> or <code>\" great\"</code>), though in some languages tokens can be shorter than one character or longer than one word. Spaces are usually grouped with the starts of words (e.g., <code>\" is\"</code> instead of <code>\"is \"</code> or <code>\" \"</code>+<code>\"is\"</code>). You can quickly check how a string is tokenized at the OpenAI Tokenizer.</p>"},{"location":"examples/How_to_count_tokens_with_tiktoken/#0.-Install-tiktoken","title":"0. Install <code>tiktoken</code>\u00b6","text":"<p>Install <code>tiktoken</code> with <code>pip</code>:</p>"},{"location":"examples/How_to_count_tokens_with_tiktoken/#1.-Import-tiktoken","title":"1. Import <code>tiktoken</code>\u00b6","text":""},{"location":"examples/How_to_count_tokens_with_tiktoken/#2.-Load-an-encoding","title":"2. Load an encoding\u00b6","text":"<p>Use <code>tiktoken.get_encoding()</code> to load an encoding by name.</p> <p>The first time this runs, it will require an internet connection to download. Later runs won't need an internet connection.</p>"},{"location":"examples/How_to_count_tokens_with_tiktoken/#3.-Turn-text-into-tokens-with-encoding.encode()","title":"3. Turn text into tokens with <code>encoding.encode()</code>\u00b6","text":""},{"location":"examples/How_to_count_tokens_with_tiktoken/#4.-Turn-tokens-into-text-with-encoding.decode()","title":"4. Turn tokens into text with <code>encoding.decode()</code>\u00b6","text":""},{"location":"examples/How_to_count_tokens_with_tiktoken/#5.-Comparing-encodings","title":"5. Comparing encodings\u00b6","text":"<p>Different encodings can vary in how they split words, group spaces, and handle non-English characters. Using the methods above, we can compare different encodings on a few example strings.</p>"},{"location":"examples/How_to_count_tokens_with_tiktoken/#6.-Counting-tokens-for-chat-API-calls","title":"6. Counting tokens for chat API calls\u00b6","text":"<p>ChatGPT models like <code>gpt-3.5-turbo</code> use tokens in the same way as past completions models, but because of their message-based formatting, it's more difficult to count how many tokens will be used by a conversation.</p> <p>Below is an example function for counting tokens for messages passed to <code>gpt-3.5-turbo-0301</code> or <code>gpt-4-0314</code>.</p> <p>Note that the exact way that messages are converted into tokens may change from model to model. So when future model versions are released, the answers returned by this function may be only approximate. The ChatML documentation explains in more detail how the OpenAI API converts messages into tokens.</p>"},{"location":"examples/How_to_format_inputs_to_ChatGPT_models/","title":"How to format inputs to ChatGPT models","text":"In\u00a0[\u00a0]: Copied! <pre># if needed, install and/or upgrade to the latest version of the OpenAI Python library\n%pip install --upgrade openai\n</pre> # if needed, install and/or upgrade to the latest version of the OpenAI Python library %pip install --upgrade openai  In\u00a0[2]: Copied! <pre># import the OpenAI Python library for calling the OpenAI API\nimport openai\n</pre> # import the OpenAI Python library for calling the OpenAI API import openai  In\u00a0[3]: Copied! <pre># Example OpenAI Python library request\nMODEL = \"gpt-3.5-turbo\"\nresponse = openai.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n        {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n        {\"role\": \"user\", \"content\": \"Orange.\"},\n    ],\n    temperature=0,\n)\n\nresponse\n</pre> # Example OpenAI Python library request MODEL = \"gpt-3.5-turbo\" response = openai.ChatCompletion.create(     model=MODEL,     messages=[         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},         {\"role\": \"user\", \"content\": \"Knock knock.\"},         {\"role\": \"assistant\", \"content\": \"Who's there?\"},         {\"role\": \"user\", \"content\": \"Orange.\"},     ],     temperature=0, )  response  Out[3]: <pre>&lt;OpenAIObject chat.completion id=chatcmpl-6wE0D7QM6dLRUPmN5Vm6YPwF1JNMR at 0x134f0c270&gt; JSON: {\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"message\": {\n        \"content\": \"Orange who?\",\n        \"role\": \"assistant\"\n      }\n    }\n  ],\n  \"created\": 1679334869,\n  \"id\": \"chatcmpl-6wE0D7QM6dLRUPmN5Vm6YPwF1JNMR\",\n  \"model\": \"gpt-3.5-turbo-0301\",\n  \"object\": \"chat.completion\",\n  \"usage\": {\n    \"completion_tokens\": 4,\n    \"prompt_tokens\": 38,\n    \"total_tokens\": 42\n  }\n}</pre> <p>As you can see, the response object has a few fields:</p> <ul> <li><code>id</code>: the ID of the request</li> <li><code>object</code>: the type of object returned (e.g., <code>chat.completion</code>)</li> <li><code>created</code>: the timestamp of the request</li> <li><code>model</code>: the full name of the model used to generate the response</li> <li><code>usage</code>: the number of tokens used to generate the replies, counting prompt, completion, and total</li> <li><code>choices</code>: a list of completion objects (only one, unless you set <code>n</code> greater than 1)<ul> <li><code>message</code>: the message object generated by the model, with <code>role</code> and <code>content</code></li> <li><code>finish_reason</code>: the reason the model stopped generating text (either <code>stop</code>, or <code>length</code> if <code>max_tokens</code> limit was reached)</li> <li><code>index</code>: the index of the completion in the list of choices</li> </ul> </li> </ul> <p>Extract just the reply with:</p> In\u00a0[4]: Copied! <pre>response['choices'][0]['message']['content']\n</pre> response['choices'][0]['message']['content']  Out[4]: <pre>'Orange who?'</pre> <p>Even non-conversation-based tasks can fit into the chat format, by placing the instruction in the first user message.</p> <p>For example, to ask the model to explain asynchronous programming in the style of the pirate Blackbeard, we can structure conversation as follows:</p> In\u00a0[5]: Copied! <pre># example with a system message\nresponse = openai.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n    ],\n    temperature=0,\n)\n\nprint(response['choices'][0]['message']['content'])\n</pre> # example with a system message response = openai.ChatCompletion.create(     model=MODEL,     messages=[         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},         {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},     ],     temperature=0, )  print(response['choices'][0]['message']['content'])  <pre>Ahoy matey! Asynchronous programming be like havin' a crew o' pirates workin' on different tasks at the same time. Ye see, instead o' waitin' for one task to be completed before startin' the next, ye can assign tasks to yer crew and let 'em work on 'em simultaneously. This way, ye can get more done in less time and keep yer ship sailin' smoothly. It be like havin' a lookout keepin' watch while the cook be preparin' the next meal and the navigator be plottin' the course. Each pirate be doin' their own thing, but all workin' together to keep the ship runnin' smoothly. Arrr, that be asynchronous programming in a pirate's tongue!\n</pre> In\u00a0[6]: Copied! <pre># example without a system message\nresponse = openai.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n    ],\n    temperature=0,\n)\n\nprint(response['choices'][0]['message']['content'])\n</pre> # example without a system message response = openai.ChatCompletion.create(     model=MODEL,     messages=[         {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},     ],     temperature=0, )  print(response['choices'][0]['message']['content'])  <pre>\n\nAhoy mateys! Let me tell ye about asynchronous programming, arrr! \n\nYe see, in the world of programming, sometimes we need to wait for certain tasks to be completed before moving on to the next one. But with asynchronous programming, we can keep the ship sailing while we wait for those tasks to finish. \n\nIt's like having a crewmate scrubbing the deck while another is hoisting the sails. They're both working at the same time, but on different tasks. \n\nIn programming, we use something called callbacks or promises to keep track of these tasks. So while one task is waiting for a response from the server, the rest of the code can keep running. \n\nIt's a bit like navigating through a stormy sea. We need to be able to adjust our course and keep moving forward, even when we hit rough waters. And with asynchronous programming, we can do just that, me hearties!\n</pre> In\u00a0[7]: Copied! <pre># An example of a system message that primes the assistant to explain concepts in great depth\nresponse = openai.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a friendly and helpful teaching assistant. You explain concepts in great depth using simple terms, and you give examples to help people learn. At the end of each explanation, you ask a question to check for understanding\"},\n        {\"role\": \"user\", \"content\": \"Can you explain how fractions work?\"},\n    ],\n    temperature=0,\n)\n\nprint(response[\"choices\"][0][\"message\"][\"content\"])\n</pre> # An example of a system message that primes the assistant to explain concepts in great depth response = openai.ChatCompletion.create(     model=MODEL,     messages=[         {\"role\": \"system\", \"content\": \"You are a friendly and helpful teaching assistant. You explain concepts in great depth using simple terms, and you give examples to help people learn. At the end of each explanation, you ask a question to check for understanding\"},         {\"role\": \"user\", \"content\": \"Can you explain how fractions work?\"},     ],     temperature=0, )  print(response[\"choices\"][0][\"message\"][\"content\"])  <pre>Sure! Fractions are a way of representing a part of a whole. The top number of a fraction is called the numerator, and it represents how many parts you have. The bottom number is called the denominator, and it represents how many parts make up the whole. \n\nFor example, if you have a pizza that is cut into 8 equal slices, and you have eaten 3 of those slices, you can represent that as a fraction: 3/8. The numerator is 3 because you have eaten 3 slices, and the denominator is 8 because there are 8 slices in total.\n\nTo add or subtract fractions, you need to have a common denominator. This means that the bottom numbers of the fractions need to be the same. For example, if you want to add 1/4 and 2/3, you need to find a common denominator. One way to do this is to multiply the denominators together: 4 x 3 = 12. Then, you need to convert each fraction so that the denominator is 12. To do this, you can multiply the numerator and denominator of each fraction by the same number. For example, to convert 1/4 to have a denominator of 12, you can multiply both the numerator and denominator by 3: 1/4 x 3/3 = 3/12. To convert 2/3 to have a denominator of 12, you can multiply both the numerator and denominator by 4: 2/3 x 4/4 = 8/12. Now that both fractions have a denominator of 12, you can add them together: 3/12 + 8/12 = 11/12.\n\nDo you have any questions about fractions?\n</pre> In\u00a0[8]: Copied! <pre># An example of a system message that primes the assistant to give brief, to-the-point answers\nresponse = openai.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a laconic assistant. You reply with brief, to-the-point answers with no elaboration.\"},\n        {\"role\": \"user\", \"content\": \"Can you explain how fractions work?\"},\n    ],\n    temperature=0,\n)\n\nprint(response[\"choices\"][0][\"message\"][\"content\"])\n</pre> # An example of a system message that primes the assistant to give brief, to-the-point answers response = openai.ChatCompletion.create(     model=MODEL,     messages=[         {\"role\": \"system\", \"content\": \"You are a laconic assistant. You reply with brief, to-the-point answers with no elaboration.\"},         {\"role\": \"user\", \"content\": \"Can you explain how fractions work?\"},     ],     temperature=0, )  print(response[\"choices\"][0][\"message\"][\"content\"])  <pre>Fractions represent a part of a whole. They consist of a numerator (top number) and a denominator (bottom number) separated by a line. The numerator represents how many parts of the whole are being considered, while the denominator represents the total number of equal parts that make up the whole.\n</pre> In\u00a0[9]: Copied! <pre># An example of a faked few-shot conversation to prime the model into translating business jargon to simpler speech\nresponse = openai.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant.\"},\n        {\"role\": \"user\", \"content\": \"Help me translate the following corporate jargon into plain English.\"},\n        {\"role\": \"assistant\", \"content\": \"Sure, I'd be happy to!\"},\n        {\"role\": \"user\", \"content\": \"New synergies will help drive top-line growth.\"},\n        {\"role\": \"assistant\", \"content\": \"Things working well together will increase revenue.\"},\n        {\"role\": \"user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n        {\"role\": \"assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n        {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n    ],\n    temperature=0,\n)\n\nprint(response[\"choices\"][0][\"message\"][\"content\"])\n</pre> # An example of a faked few-shot conversation to prime the model into translating business jargon to simpler speech response = openai.ChatCompletion.create(     model=MODEL,     messages=[         {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant.\"},         {\"role\": \"user\", \"content\": \"Help me translate the following corporate jargon into plain English.\"},         {\"role\": \"assistant\", \"content\": \"Sure, I'd be happy to!\"},         {\"role\": \"user\", \"content\": \"New synergies will help drive top-line growth.\"},         {\"role\": \"assistant\", \"content\": \"Things working well together will increase revenue.\"},         {\"role\": \"user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},         {\"role\": \"assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},         {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},     ],     temperature=0, )  print(response[\"choices\"][0][\"message\"][\"content\"])  <pre>We don't have enough time to complete everything perfectly for the client.\n</pre> <p>To help clarify that the example messages are not part of a real conversation, and shouldn't be referred back to by the model, you can instead set the <code>name</code> field of <code>system</code> messages to <code>example_user</code> and <code>example_assistant</code>.</p> <p>Transforming the few-shot example above, we could write:</p> In\u00a0[10]: Copied! <pre># The business jargon translation example, but with example names for the example messages\nresponse = openai.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\"},\n        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"New synergies will help drive top-line growth.\"},\n        {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Things working well together will increase revenue.\"},\n        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n        {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n        {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n    ],\n    temperature=0,\n)\n\nprint(response[\"choices\"][0][\"message\"][\"content\"])\n</pre> # The business jargon translation example, but with example names for the example messages response = openai.ChatCompletion.create(     model=MODEL,     messages=[         {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\"},         {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"New synergies will help drive top-line growth.\"},         {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Things working well together will increase revenue.\"},         {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},         {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},         {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},     ],     temperature=0, )  print(response[\"choices\"][0][\"message\"][\"content\"])  <pre>This sudden change in plans means we don't have enough time to do everything for the client's project.\n</pre> <p>Not every attempt at engineering conversations will succeed at first.</p> <p>If your first attempts fail, don't be afraid to experiment with different ways of priming or conditioning the model.</p> <p>As an example, one developer discovered an increase in accuracy when they inserted a user message that said \"Great job so far, these have been perfect\" to help condition the model into providing higher quality responses.</p> <p>For more ideas on how to lift the reliability of the models, consider reading our guide on techniques to increase reliability. It was written for non-chat models, but many of its principles still apply.</p> In\u00a0[11]: Copied! <pre>import tiktoken\n\n\ndef num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n\"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n    try:\n        encoding = tiktoken.encoding_for_model(model)\n    except KeyError:\n        print(\"Warning: model not found. Using cl100k_base encoding.\")\n        encoding = tiktoken.get_encoding(\"cl100k_base\")\n    if model == \"gpt-3.5-turbo\":\n        print(\"Warning: gpt-3.5-turbo may change over time. Returning num tokens assuming gpt-3.5-turbo-0301.\")\n        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\")\n    elif model == \"gpt-4\":\n        print(\"Warning: gpt-4 may change over time. Returning num tokens assuming gpt-4-0314.\")\n        return num_tokens_from_messages(messages, model=\"gpt-4-0314\")\n    elif model == \"gpt-3.5-turbo-0301\":\n        tokens_per_message = 4  # every message follows &lt;im_start&gt;{role/name}\\n{content}&lt;im_end&gt;\\n\n        tokens_per_name = -1  # if there's a name, the role is omitted\n    elif model == \"gpt-4-0314\":\n        tokens_per_message = 3\n        tokens_per_name = 1\n    else:\n        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n    num_tokens = 0\n    for message in messages:\n        num_tokens += tokens_per_message\n        for key, value in message.items():\n            num_tokens += len(encoding.encode(value))\n            if key == \"name\":\n                num_tokens += tokens_per_name\n    num_tokens += 2  # every reply is primed with &lt;im_start&gt;assistant\n    return num_tokens\n</pre> import tiktoken   def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):     \"\"\"Returns the number of tokens used by a list of messages.\"\"\"     try:         encoding = tiktoken.encoding_for_model(model)     except KeyError:         print(\"Warning: model not found. Using cl100k_base encoding.\")         encoding = tiktoken.get_encoding(\"cl100k_base\")     if model == \"gpt-3.5-turbo\":         print(\"Warning: gpt-3.5-turbo may change over time. Returning num tokens assuming gpt-3.5-turbo-0301.\")         return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\")     elif model == \"gpt-4\":         print(\"Warning: gpt-4 may change over time. Returning num tokens assuming gpt-4-0314.\")         return num_tokens_from_messages(messages, model=\"gpt-4-0314\")     elif model == \"gpt-3.5-turbo-0301\":         tokens_per_message = 4  # every message follows {role/name}\\n{content}\\n         tokens_per_name = -1  # if there's a name, the role is omitted     elif model == \"gpt-4-0314\":         tokens_per_message = 3         tokens_per_name = 1     else:         raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")     num_tokens = 0     for message in messages:         num_tokens += tokens_per_message         for key, value in message.items():             num_tokens += len(encoding.encode(value))             if key == \"name\":                 num_tokens += tokens_per_name     num_tokens += 2  # every reply is primed with assistant     return num_tokens  In\u00a0[12]: Copied! <pre># let's verify the function above matches the OpenAI API response\n\nexample_messages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n    },\n    {\n        \"role\": \"system\",\n        \"name\": \"example_user\",\n        \"content\": \"New synergies will help drive top-line growth.\",\n    },\n    {\n        \"role\": \"system\",\n        \"name\": \"example_assistant\",\n        \"content\": \"Things working well together will increase revenue.\",\n    },\n    {\n        \"role\": \"system\",\n        \"name\": \"example_user\",\n        \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n    },\n    {\n        \"role\": \"system\",\n        \"name\": \"example_assistant\",\n        \"content\": \"Let's talk later when we're less busy about how to do better.\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n    },\n]\n\nfor model in [\"gpt-3.5-turbo-0301\", \"gpt-4-0314\"]:\n    print(model)\n    # example token count from the function defined above\n    print(f\"{num_tokens_from_messages(example_messages, model)} prompt tokens counted by num_tokens_from_messages().\")\n    # example token count from the OpenAI API\n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=example_messages,\n        temperature=0,\n        max_tokens=1  # we're only counting input tokens here, so let's not waste tokens on the output\n    )\n    print(f'{response[\"usage\"][\"prompt_tokens\"]} prompt tokens counted by the OpenAI API.')\n    print()\n</pre> # let's verify the function above matches the OpenAI API response  example_messages = [     {         \"role\": \"system\",         \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",     },     {         \"role\": \"system\",         \"name\": \"example_user\",         \"content\": \"New synergies will help drive top-line growth.\",     },     {         \"role\": \"system\",         \"name\": \"example_assistant\",         \"content\": \"Things working well together will increase revenue.\",     },     {         \"role\": \"system\",         \"name\": \"example_user\",         \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",     },     {         \"role\": \"system\",         \"name\": \"example_assistant\",         \"content\": \"Let's talk later when we're less busy about how to do better.\",     },     {         \"role\": \"user\",         \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",     }, ]  for model in [\"gpt-3.5-turbo-0301\", \"gpt-4-0314\"]:     print(model)     # example token count from the function defined above     print(f\"{num_tokens_from_messages(example_messages, model)} prompt tokens counted by num_tokens_from_messages().\")     # example token count from the OpenAI API     response = openai.ChatCompletion.create(         model=model,         messages=example_messages,         temperature=0,         max_tokens=1  # we're only counting input tokens here, so let's not waste tokens on the output     )     print(f'{response[\"usage\"][\"prompt_tokens\"]} prompt tokens counted by the OpenAI API.')     print()  <pre>gpt-3.5-turbo-0301\n126 prompt tokens counted by num_tokens_from_messages().\n126 prompt tokens counted by the OpenAI API.\n\ngpt-4-0314\n128 prompt tokens counted by num_tokens_from_messages().\n128 prompt tokens counted by the OpenAI API.\n\n</pre>"},{"location":"examples/How_to_format_inputs_to_ChatGPT_models/#How-to-format-inputs-to-ChatGPT-models","title":"How to format inputs to ChatGPT models\u00b6","text":"<p>ChatGPT is powered by <code>gpt-3.5-turbo</code>, OpenAI's most advanced model.</p> <p>You can build your own applications with <code>gpt-3.5-turbo</code> using the OpenAI API.</p> <p>Chat models take a series of messages as input, and return an AI-written message as output.</p> <p>This guide illustrates the chat format with a few example API calls.</p>"},{"location":"examples/How_to_format_inputs_to_ChatGPT_models/#1.-Import-the-openai-library","title":"1. Import the openai library\u00b6","text":""},{"location":"examples/How_to_format_inputs_to_ChatGPT_models/#2.-An-example-chat-API-call","title":"2. An example chat API call\u00b6","text":"<p>A chat API call has two required inputs:</p> <ul> <li><code>model</code>: the name of the model you want to use (e.g., <code>gpt-3.5-turbo</code>)</li> <li><code>messages</code>: a list of message objects, where each object has at least two fields:<ul> <li><code>role</code>: the role of the messenger (either <code>system</code>, <code>user</code>, or <code>assistant</code>)</li> <li><code>content</code>: the content of the message (e.g., <code>Write me a beautiful poem</code>)</li> </ul> </li> </ul> <p>Typically, a conversation will start with a system message, followed by alternating user and assistant messages, but you are not required to follow this format.</p> <p>Let's look at an example chat API calls to see how the chat format works in practice.</p>"},{"location":"examples/How_to_format_inputs_to_ChatGPT_models/#3.-Tips-for-instructing-gpt-3.5-turbo-0301","title":"3. Tips for instructing gpt-3.5-turbo-0301\u00b6","text":"<p>Best practices for instructing models may change from model version to model version. The advice that follows applies to <code>gpt-3.5-turbo-0301</code> and may not apply to future models.</p>"},{"location":"examples/How_to_format_inputs_to_ChatGPT_models/#System-messages","title":"System messages\u00b6","text":"<p>The system message can be used to prime the assistant with different personalities or behaviors.</p> <p>However, the model does not generally pay as much attention to the system message, and therefore we recommend placing important instructions in the user message instead.</p>"},{"location":"examples/How_to_format_inputs_to_ChatGPT_models/#Few-shot-prompting","title":"Few-shot prompting\u00b6","text":"<p>In some cases, it's easier to show the model what you want rather than tell the model what you want.</p> <p>One way to show the model what you want is with faked example messages.</p> <p>For example:</p>"},{"location":"examples/How_to_format_inputs_to_ChatGPT_models/#4.-Counting-tokens","title":"4. Counting tokens\u00b6","text":"<p>When you submit your request, the API transforms the messages into a sequence of tokens.</p> <p>The number of tokens used affects:</p> <ul> <li>the cost of the request</li> <li>the time it takes to generate the response</li> <li>when the reply gets cut off from hitting the maximum token limit (4096 for <code>gpt-3.5-turbo</code>)</li> </ul> <p>You can use the following function to count the number of tokens that a list of messages will use.</p>"},{"location":"examples/How_to_handle_rate_limits/","title":"How to handle rate limits","text":"In\u00a0[\u00a0]: Copied! <pre>import openai  # for making OpenAI API requests\n\n# request a bunch of completions in a loop\nfor _ in range(100):\n    openai.Completion.create(\n        model=\"code-cushman-001\",\n        prompt=\"def magic_function():\\n\\t\",\n        max_tokens=10,\n    )\n</pre> import openai  # for making OpenAI API requests  # request a bunch of completions in a loop for _ in range(100):     openai.Completion.create(         model=\"code-cushman-001\",         prompt=\"def magic_function():\\n\\t\",         max_tokens=10,     )  In\u00a0[1]: Copied! <pre>import openai  # for OpenAI API calls\nfrom tenacity import (\n    retry,\n    stop_after_attempt,\n    wait_random_exponential,\n)  # for exponential backoff\n\n\n@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\ndef completion_with_backoff(**kwargs):\n    return openai.Completion.create(**kwargs)\n\n\ncompletion_with_backoff(model=\"text-davinci-002\", prompt=\"Once upon a time,\")\n</pre> import openai  # for OpenAI API calls from tenacity import (     retry,     stop_after_attempt,     wait_random_exponential, )  # for exponential backoff   @retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6)) def completion_with_backoff(**kwargs):     return openai.Completion.create(**kwargs)   completion_with_backoff(model=\"text-davinci-002\", prompt=\"Once upon a time,\")  Out[1]: <pre>&lt;OpenAIObject text_completion id=cmpl-5oowO391reUW8RGVfFyzBM1uBs4A5 at 0x10d8cae00&gt; JSON: {\n  \"choices\": [\n    {\n      \"finish_reason\": \"length\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"text\": \" a little girl dreamed of becoming a model.\\n\\nNowadays, that dream\"\n    }\n  ],\n  \"created\": 1662793900,\n  \"id\": \"cmpl-5oowO391reUW8RGVfFyzBM1uBs4A5\",\n  \"model\": \"text-davinci-002\",\n  \"object\": \"text_completion\",\n  \"usage\": {\n    \"completion_tokens\": 16,\n    \"prompt_tokens\": 5,\n    \"total_tokens\": 21\n  }\n}</pre> In\u00a0[2]: Copied! <pre>import backoff  # for exponential backoff\nimport openai  # for OpenAI API calls\n\n\n@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\ndef completions_with_backoff(**kwargs):\n    return openai.Completion.create(**kwargs)\n\n\ncompletions_with_backoff(model=\"text-davinci-002\", prompt=\"Once upon a time,\")\n</pre> import backoff  # for exponential backoff import openai  # for OpenAI API calls   @backoff.on_exception(backoff.expo, openai.error.RateLimitError) def completions_with_backoff(**kwargs):     return openai.Completion.create(**kwargs)   completions_with_backoff(model=\"text-davinci-002\", prompt=\"Once upon a time,\")  Out[2]: <pre>&lt;OpenAIObject text_completion id=cmpl-5oowPhIdUvshEsF1rBhhwE9KFfI3M at 0x111043680&gt; JSON: {\n  \"choices\": [\n    {\n      \"finish_reason\": \"length\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"text\": \" two children lived in a poor country village. In the winter, the temperature would\"\n    }\n  ],\n  \"created\": 1662793901,\n  \"id\": \"cmpl-5oowPhIdUvshEsF1rBhhwE9KFfI3M\",\n  \"model\": \"text-davinci-002\",\n  \"object\": \"text_completion\",\n  \"usage\": {\n    \"completion_tokens\": 16,\n    \"prompt_tokens\": 5,\n    \"total_tokens\": 21\n  }\n}</pre> In\u00a0[3]: Copied! <pre># imports\nimport random\nimport time\n\nimport openai\n\n# define a retry decorator\ndef retry_with_exponential_backoff(\n    func,\n    initial_delay: float = 1,\n    exponential_base: float = 2,\n    jitter: bool = True,\n    max_retries: int = 10,\n    errors: tuple = (openai.error.RateLimitError,),\n):\n\"\"\"Retry a function with exponential backoff.\"\"\"\n\n    def wrapper(*args, **kwargs):\n        # Initialize variables\n        num_retries = 0\n        delay = initial_delay\n\n        # Loop until a successful response or max_retries is hit or an exception is raised\n        while True:\n            try:\n                return func(*args, **kwargs)\n\n            # Retry on specified errors\n            except errors as e:\n                # Increment retries\n                num_retries += 1\n\n                # Check if max retries has been reached\n                if num_retries &gt; max_retries:\n                    raise Exception(\n                        f\"Maximum number of retries ({max_retries}) exceeded.\"\n                    )\n\n                # Increment the delay\n                delay *= exponential_base * (1 + jitter * random.random())\n\n                # Sleep for the delay\n                time.sleep(delay)\n\n            # Raise exceptions for any errors not specified\n            except Exception as e:\n                raise e\n\n    return wrapper\n\n\n@retry_with_exponential_backoff\ndef completions_with_backoff(**kwargs):\n    return openai.Completion.create(**kwargs)\n\n\ncompletions_with_backoff(model=\"text-davinci-002\", prompt=\"Once upon a time,\")\n</pre> # imports import random import time  import openai  # define a retry decorator def retry_with_exponential_backoff(     func,     initial_delay: float = 1,     exponential_base: float = 2,     jitter: bool = True,     max_retries: int = 10,     errors: tuple = (openai.error.RateLimitError,), ):     \"\"\"Retry a function with exponential backoff.\"\"\"      def wrapper(*args, **kwargs):         # Initialize variables         num_retries = 0         delay = initial_delay          # Loop until a successful response or max_retries is hit or an exception is raised         while True:             try:                 return func(*args, **kwargs)              # Retry on specified errors             except errors as e:                 # Increment retries                 num_retries += 1                  # Check if max retries has been reached                 if num_retries &gt; max_retries:                     raise Exception(                         f\"Maximum number of retries ({max_retries}) exceeded.\"                     )                  # Increment the delay                 delay *= exponential_base * (1 + jitter * random.random())                  # Sleep for the delay                 time.sleep(delay)              # Raise exceptions for any errors not specified             except Exception as e:                 raise e      return wrapper   @retry_with_exponential_backoff def completions_with_backoff(**kwargs):     return openai.Completion.create(**kwargs)   completions_with_backoff(model=\"text-davinci-002\", prompt=\"Once upon a time,\")  Out[3]: <pre>&lt;OpenAIObject text_completion id=cmpl-5oowRsCXv3AkUgVJyyo3TQrVq7hIT at 0x111024220&gt; JSON: {\n  \"choices\": [\n    {\n      \"finish_reason\": \"length\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"text\": \" a man decided to greatly improve his karma by turning his life around.\\n\\n\"\n    }\n  ],\n  \"created\": 1662793903,\n  \"id\": \"cmpl-5oowRsCXv3AkUgVJyyo3TQrVq7hIT\",\n  \"model\": \"text-davinci-002\",\n  \"object\": \"text_completion\",\n  \"usage\": {\n    \"completion_tokens\": 16,\n    \"prompt_tokens\": 5,\n    \"total_tokens\": 21\n  }\n}</pre> In\u00a0[4]: Copied! <pre># imports\nimport time\nimport openai\n\n# Define a function that adds a delay to a Completion API call\ndef delayed_completion(delay_in_seconds: float = 1, **kwargs):\n\"\"\"Delay a completion by a specified amount of time.\"\"\"\n\n    # Sleep for the delay\n    time.sleep(delay_in_seconds)\n\n    # Call the Completion API and return the result\n    return openai.Completion.create(**kwargs)\n\n\n# Calculate the delay based on your rate limit\nrate_limit_per_minute = 20\ndelay = 60.0 / rate_limit_per_minute\n\ndelayed_completion(\n    delay_in_seconds=delay,\n    model=\"text-davinci-002\",\n    prompt=\"Once upon a time,\"\n)\n</pre> # imports import time import openai  # Define a function that adds a delay to a Completion API call def delayed_completion(delay_in_seconds: float = 1, **kwargs):     \"\"\"Delay a completion by a specified amount of time.\"\"\"      # Sleep for the delay     time.sleep(delay_in_seconds)      # Call the Completion API and return the result     return openai.Completion.create(**kwargs)   # Calculate the delay based on your rate limit rate_limit_per_minute = 20 delay = 60.0 / rate_limit_per_minute  delayed_completion(     delay_in_seconds=delay,     model=\"text-davinci-002\",     prompt=\"Once upon a time,\" )  Out[4]: <pre>&lt;OpenAIObject text_completion id=cmpl-5oowVVZnAzdCPtUJ0rifeamtLcZRp at 0x11b2c7680&gt; JSON: {\n  \"choices\": [\n    {\n      \"finish_reason\": \"length\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"text\": \" there was an idyllic little farm that sat by a babbling brook\"\n    }\n  ],\n  \"created\": 1662793907,\n  \"id\": \"cmpl-5oowVVZnAzdCPtUJ0rifeamtLcZRp\",\n  \"model\": \"text-davinci-002\",\n  \"object\": \"text_completion\",\n  \"usage\": {\n    \"completion_tokens\": 16,\n    \"prompt_tokens\": 5,\n    \"total_tokens\": 21\n  }\n}</pre> In\u00a0[5]: Copied! <pre>import openai  # for making OpenAI API requests\n\n\nnum_stories = 10\nprompt = \"Once upon a time,\"\n\n# serial example, with one story completion per request\nfor _ in range(num_stories):\n    response = openai.Completion.create(\n        model=\"curie\",\n        prompt=prompt,\n        max_tokens=20,\n    )\n\n    # print story\n    print(prompt + response.choices[0].text)\n</pre> import openai  # for making OpenAI API requests   num_stories = 10 prompt = \"Once upon a time,\"  # serial example, with one story completion per request for _ in range(num_stories):     response = openai.Completion.create(         model=\"curie\",         prompt=prompt,         max_tokens=20,     )      # print story     print(prompt + response.choices[0].text)  <pre>Once upon a time, before there were grandiloquent tales of the massacre at Fort Mims, there were stories of\nOnce upon a time, a full-sized search and rescue was created. However, CIDIs are the addition of requiring\nOnce upon a time, Schubert was hot with the films. \u201cSchubert sings of honey, flowers,\nOnce upon a time, you could watch these films on your VCR, sometimes years after their initial theatrical release, and there\nOnce upon a time, there was a forest. In that forest, the forest animals ruled. The forest animals had their homes\nOnce upon a time, there were two programs that complained about false positive scans. Peacock and Midnight Manager alike, only\nOnce upon a time, a long, long time ago, tragedy struck. it was the darkest of nights, and there was\nOnce upon a time, when Adam was a perfect little gentleman, he was presented at Court as a guarantee of good character.\nOnce upon a time, Adam and Eve made a mistake. They ate the fruit from the tree of immortality and split the consequences\nOnce upon a time, there was a set of programming fundamental principles known as the \"X model.\" This is a set of\n</pre> In\u00a0[6]: Copied! <pre>import openai  # for making OpenAI API requests\n\n\nnum_stories = 10\nprompts = [\"Once upon a time,\"] * num_stories\n\n# batched example, with 10 stories completions per request\nresponse = openai.Completion.create(\n    model=\"curie\",\n    prompt=prompts,\n    max_tokens=20,\n)\n\n# match completions to prompts by index\nstories = [\"\"] * len(prompts)\nfor choice in response.choices:\n    stories[choice.index] = prompts[choice.index] + choice.text\n\n# print stories\nfor story in stories:\n    print(story)\n</pre> import openai  # for making OpenAI API requests   num_stories = 10 prompts = [\"Once upon a time,\"] * num_stories  # batched example, with 10 stories completions per request response = openai.Completion.create(     model=\"curie\",     prompt=prompts,     max_tokens=20, )  # match completions to prompts by index stories = [\"\"] * len(prompts) for choice in response.choices:     stories[choice.index] = prompts[choice.index] + choice.text  # print stories for story in stories:     print(story)  <pre>Once upon a time, there were two sisters, Eliza Pickering and Ariana 'Ari' Lucas. When these lovely\nOnce upon a time, Keene was stung by a worm \u2014 actually, probably a python \u2014 snaking through his leg\nOnce upon a time, there was a professor of physics during the depression. It was difficult, during this time, to get\nOnce upon a time, before you got sick, you told stories to all and sundry, and your listeners believed in you\nOnce upon a time, there was one very old nice donkey. He was incredibly smart, in a very old, kind of\nOnce upon a time, the property of a common lodging house was a common cup for all the inhabitants. Betimes a constant\nOnce upon a time, in an unspecified country, there was a witch who had an illegal product. It was highly effective,\nOnce upon a time, a long time ago, I turned 13, my beautiful dog Duncan swept me up into his jaws like\nOnce upon a time, as a thoroughly reformed creature from an army of Nazis, he took On Judgement Day myself and his\nOnce upon a time, Capcom made a game for the Atari VCS called Missile Command. While it was innovative at the time\n</pre>"},{"location":"examples/How_to_handle_rate_limits/#How-to-handle-rate-limits","title":"How to handle rate limits\u00b6","text":"<p>When you call the OpenAI API repeatedly, you may encounter error messages that say <code>429: 'Too Many Requests'</code> or <code>RateLimitError</code>. These error messages come from exceeding the API's rate limits.</p> <p>This guide shares tips for avoiding and handling rate limit errors.</p> <p>To see an example script for throttling parallel requests to avoid rate limit errors, see api_request_parallel_processor.py.</p>"},{"location":"examples/How_to_handle_rate_limits/#Why-rate-limits-exist","title":"Why rate limits exist\u00b6","text":"<p>Rate limits are a common practice for APIs, and they're put in place for a few different reasons.</p> <ul> <li>First, they help protect against abuse or misuse of the API. For example, a malicious actor could flood the API with requests in an attempt to overload it or cause disruptions in service. By setting rate limits, OpenAI can prevent this kind of activity.</li> <li>Second, rate limits help ensure that everyone has fair access to the API. If one person or organization makes an excessive number of requests, it could bog down the API for everyone else. By throttling the number of requests that a single user can make, OpenAI ensures that everyone has an opportunity to use the API without experiencing slowdowns.</li> <li>Lastly, rate limits can help OpenAI manage the aggregate load on its infrastructure. If requests to the API increase dramatically, it could tax the servers and cause performance issues. By setting rate limits, OpenAI can help maintain a smooth and consistent experience for all users.</li> </ul> <p>Although hitting rate limits can be frustrating, rate limits exist to protect the reliable operation of the API for its users.</p>"},{"location":"examples/How_to_handle_rate_limits/#Default-rate-limits","title":"Default rate limits\u00b6","text":"<p>As of Jan 2023, the default rate limits are:</p> Text Completion &amp; Embedding endpoints Code &amp; Edit endpoints Free trial users <ul> <li>20 requests / minute</li> <li>150,000 tokens / minute</li> </ul> <ul> <li>20 requests / minute</li> <li>150,000 tokens / minute</li> </ul> Pay-as-you-go users (in your first 48 hours) <ul> <li>60 requests / minute</li> <li>250,000 davinci tokens / minute (and proportionally more for cheaper models)</li> </ul> <ul> <li>20 requests / minute</li> <li>150,000 tokens / minute</li> </ul> Pay-as-you-go users (after your first 48 hours) <ul> <li>3,000 requests / minute</li> <li>250,000 davinci tokens / minute (and proportionally more for cheaper models)</li> </ul> <ul> <li>20 requests / minute</li> <li>150,000 tokens / minute</li> </ul> <p>For reference, 1,000 tokens is roughly a page of text.</p>"},{"location":"examples/How_to_handle_rate_limits/#Other-rate-limit-resources","title":"Other rate limit resources\u00b6","text":"<p>Read more about OpenAI's rate limits in these other resources:</p> <ul> <li>Guide: Rate limits</li> <li>Help Center: Is API usage subject to any rate limits?</li> <li>Help Center: How can I solve 429: 'Too Many Requests' errors?</li> </ul>"},{"location":"examples/How_to_handle_rate_limits/#Requesting-a-rate-limit-increase","title":"Requesting a rate limit increase\u00b6","text":"<p>If you'd like your organization's rate limit increased, please fill out the following form:</p> <ul> <li>OpenAI Rate Limit Increase Request form</li> </ul>"},{"location":"examples/How_to_handle_rate_limits/#Example-rate-limit-error","title":"Example rate limit error\u00b6","text":"<p>A rate limit error will occur when API requests are sent too quickly. If using the OpenAI Python library, they will look something like:</p> <pre><code>RateLimitError: Rate limit reached for default-codex in organization org-{id} on requests per min. Limit: 20.000000 / min. Current: 24.000000 / min. Contact support@openai.com if you continue to have issues or if you\u2019d like to request an increase.\n</code></pre> <p>Below is example code for triggering a rate limit error.</p>"},{"location":"examples/How_to_handle_rate_limits/#How-to-avoid-rate-limit-errors","title":"How to avoid rate limit errors\u00b6","text":""},{"location":"examples/How_to_handle_rate_limits/#Retrying-with-exponential-backoff","title":"Retrying with exponential backoff\u00b6","text":"<p>One easy way to avoid rate limit errors is to automatically retry requests with a random exponential backoff. Retrying with exponential backoff means performing a short sleep when a rate limit error is hit, then retrying the unsuccessful request. If the request is still unsuccessful, the sleep length is increased and the process is repeated. This continues until the request is successful or until a maximum number of retries is reached.</p> <p>This approach has many benefits:</p> <ul> <li>Automatic retries means you can recover from rate limit errors without crashes or missing data</li> <li>Exponential backoff means that your first retries can be tried quickly, while still benefiting from longer delays if your first few retries fail</li> <li>Adding random jitter to the delay helps retries from all hitting at the same time</li> </ul> <p>Note that unsuccessful requests contribute to your per-minute limit, so continuously resending a request won\u2019t work.</p> <p>Below are a few example solutions.</p>"},{"location":"examples/How_to_handle_rate_limits/#Example-%231:-Using-the-Tenacity-library","title":"Example #1: Using the Tenacity library\u00b6","text":"<p>Tenacity is an Apache 2.0 licensed general-purpose retrying library, written in Python, to simplify the task of adding retry behavior to just about anything.</p> <p>To add exponential backoff to your requests, you can use the <code>tenacity.retry</code> decorator. The following example uses the <code>tenacity.wait_random_exponential</code> function to add random exponential backoff to a request.</p> <p>Note that the Tenacity library is a third-party tool, and OpenAI makes no guarantees about its reliability or security.</p>"},{"location":"examples/How_to_handle_rate_limits/#Example-%232:-Using-the-backoff-library","title":"Example #2: Using the backoff library\u00b6","text":"<p>Another library that provides function decorators for backoff and retry is backoff.</p> <p>Like Tenacity, the backoff library is a third-party tool, and OpenAI makes no guarantees about its reliability or security.</p>"},{"location":"examples/How_to_handle_rate_limits/#Example-3:-Manual-backoff-implementation","title":"Example 3: Manual backoff implementation\u00b6","text":"<p>If you don't want to use third-party libraries, you can implement your own backoff logic.</p>"},{"location":"examples/How_to_handle_rate_limits/#How-to-maximize-throughput-of-batch-processing-given-rate-limits","title":"How to maximize throughput of batch processing given rate limits\u00b6","text":"<p>If you're processing real-time requests from users, backoff and retry is a great strategy to minimize latency while avoiding rate limit errors.</p> <p>However, if you're processing large volumes of batch data, where throughput matters more than latency, there are a few other things you can do in addition to backoff and retry.</p>"},{"location":"examples/How_to_handle_rate_limits/#Proactively-adding-delay-between-requests","title":"Proactively adding delay between requests\u00b6","text":"<p>If you are constantly hitting the rate limit, then backing off, then hitting the rate limit again, then backing off again, it's possible that a good fraction of your request budget will be 'wasted' on requests that need to be retried. This limits your processing throughput, given a fixed rate limit.</p> <p>Here, one potential solution is to calculate your rate limit and add a delay equal to its reciprocal (e.g., if your rate limit 20 requests per minute, add a delay of 3\u20136 seconds to each request). This can help you operate near the rate limit ceiling without hitting it and incurring wasted requests.</p>"},{"location":"examples/How_to_handle_rate_limits/#Example-of-adding-delay-to-a-request","title":"Example of adding delay to a request\u00b6","text":""},{"location":"examples/How_to_handle_rate_limits/#Batching-requests","title":"Batching requests\u00b6","text":"<p>The OpenAI API has separate limits for requests per minute and tokens per minute.</p> <p>If you're hitting the limit on requests per minute, but have headroom on tokens per minute, you can increase your throughput by batching multiple tasks into each request. This will allow you to process more tokens per minute, especially with the smaller models.</p> <p>Sending in a batch of prompts works exactly the same as a normal API call, except that pass in a list of strings to <code>prompt</code> parameter instead of a single string.</p> <p>Warning: the response object may not return completions in the order of the prompts, so always remember to match responses back to prompts using the <code>index</code> field.</p>"},{"location":"examples/How_to_handle_rate_limits/#Example-without-batching","title":"Example without batching\u00b6","text":""},{"location":"examples/How_to_handle_rate_limits/#Example-with-batching","title":"Example with batching\u00b6","text":""},{"location":"examples/How_to_handle_rate_limits/#Example-parallel-processing-script","title":"Example parallel processing script\u00b6","text":"<p>We've written an example script for parallel processing large quantities of API requests: api_request_parallel_processor.py.</p> <p>The script combines some handy features:</p> <ul> <li>Streams requests from file, to avoid running out of memory for giant jobs</li> <li>Makes requests concurrently, to maximize throughput</li> <li>Throttles both request and token usage, to stay under rate limits</li> <li>Retries failed requests, to avoid missing data</li> <li>Logs errors, to diagnose problems with requests</li> </ul> <p>Feel free to use it as is or modify it to suit your needs.</p>"},{"location":"examples/How_to_stream_completions/","title":"How to stream completions","text":"In\u00a0[1]: Copied! <pre># imports\nimport openai  # for OpenAI API calls\nimport time  # for measuring time duration of API calls\n</pre> # imports import openai  # for OpenAI API calls import time  # for measuring time duration of API calls In\u00a0[2]: Copied! <pre># Example of an OpenAI ChatCompletion request\n# https://platform.openai.com/docs/guides/chat\n\n# record the time before the request is sent\nstart_time = time.time()\n\n# send a ChatCompletion request to count to 100\nresponse = openai.ChatCompletion.create(\n    model='gpt-3.5-turbo',\n    messages=[\n        {'role': 'user', 'content': 'Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}\n    ],\n    temperature=0,\n)\n\n# calculate the time it took to receive the response\nresponse_time = time.time() - start_time\n\n# print the time delay and text received\nprint(f\"Full response received {response_time:.2f} seconds after request\")\nprint(f\"Full response received:\\n{response}\")\n</pre> # Example of an OpenAI ChatCompletion request # https://platform.openai.com/docs/guides/chat  # record the time before the request is sent start_time = time.time()  # send a ChatCompletion request to count to 100 response = openai.ChatCompletion.create(     model='gpt-3.5-turbo',     messages=[         {'role': 'user', 'content': 'Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}     ],     temperature=0, )  # calculate the time it took to receive the response response_time = time.time() - start_time  # print the time delay and text received print(f\"Full response received {response_time:.2f} seconds after request\") print(f\"Full response received:\\n{response}\")  <pre>Full response received 3.03 seconds after request\nFull response received:\n{\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"message\": {\n        \"content\": \"\\n\\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100.\",\n        \"role\": \"assistant\"\n      }\n    }\n  ],\n  \"created\": 1677825456,\n  \"id\": \"chatcmpl-6ptKqrhgRoVchm58Bby0UvJzq2ZuQ\",\n  \"model\": \"gpt-3.5-turbo-0301\",\n  \"object\": \"chat.completion\",\n  \"usage\": {\n    \"completion_tokens\": 301,\n    \"prompt_tokens\": 36,\n    \"total_tokens\": 337\n  }\n}\n</pre> <p>The reply can be extracted with <code>response['choices'][0]['message']</code>.</p> <p>The content of the reply can be extracted with <code>response['choices'][0]['message']['content']</code>.</p> In\u00a0[3]: Copied! <pre>reply = response['choices'][0]['message']\nprint(f\"Extracted reply: \\n{reply}\")\n\nreply_content = response['choices'][0]['message']['content']\nprint(f\"Extracted content: \\n{reply_content}\")\n</pre> reply = response['choices'][0]['message'] print(f\"Extracted reply: \\n{reply}\")  reply_content = response['choices'][0]['message']['content'] print(f\"Extracted content: \\n{reply_content}\")  <pre>Extracted reply: \n{\n  \"content\": \"\\n\\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100.\",\n  \"role\": \"assistant\"\n}\nExtracted content: \n\n\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100.\n</pre> In\u00a0[4]: Copied! <pre># Example of an OpenAI ChatCompletion request with stream=True\n# https://platform.openai.com/docs/guides/chat\n\n# a ChatCompletion request\nresponse = openai.ChatCompletion.create(\n    model='gpt-3.5-turbo',\n    messages=[\n        {'role': 'user', 'content': \"What's 1+1? Answer in one word.\"}\n    ],\n    temperature=0,\n    stream=True  # this time, we set stream=True\n)\n\nfor chunk in response:\n    print(chunk)\n</pre> # Example of an OpenAI ChatCompletion request with stream=True # https://platform.openai.com/docs/guides/chat  # a ChatCompletion request response = openai.ChatCompletion.create(     model='gpt-3.5-turbo',     messages=[         {'role': 'user', 'content': \"What's 1+1? Answer in one word.\"}     ],     temperature=0,     stream=True  # this time, we set stream=True )  for chunk in response:     print(chunk) <pre>{\n  \"choices\": [\n    {\n      \"delta\": {\n        \"role\": \"assistant\"\n      },\n      \"finish_reason\": null,\n      \"index\": 0\n    }\n  ],\n  \"created\": 1677825464,\n  \"id\": \"chatcmpl-6ptKyqKOGXZT6iQnqiXAH8adNLUzD\",\n  \"model\": \"gpt-3.5-turbo-0301\",\n  \"object\": \"chat.completion.chunk\"\n}\n{\n  \"choices\": [\n    {\n      \"delta\": {\n        \"content\": \"\\n\\n\"\n      },\n      \"finish_reason\": null,\n      \"index\": 0\n    }\n  ],\n  \"created\": 1677825464,\n  \"id\": \"chatcmpl-6ptKyqKOGXZT6iQnqiXAH8adNLUzD\",\n  \"model\": \"gpt-3.5-turbo-0301\",\n  \"object\": \"chat.completion.chunk\"\n}\n{\n  \"choices\": [\n    {\n      \"delta\": {\n        \"content\": \"2\"\n      },\n      \"finish_reason\": null,\n      \"index\": 0\n    }\n  ],\n  \"created\": 1677825464,\n  \"id\": \"chatcmpl-6ptKyqKOGXZT6iQnqiXAH8adNLUzD\",\n  \"model\": \"gpt-3.5-turbo-0301\",\n  \"object\": \"chat.completion.chunk\"\n}\n{\n  \"choices\": [\n    {\n      \"delta\": {},\n      \"finish_reason\": \"stop\",\n      \"index\": 0\n    }\n  ],\n  \"created\": 1677825464,\n  \"id\": \"chatcmpl-6ptKyqKOGXZT6iQnqiXAH8adNLUzD\",\n  \"model\": \"gpt-3.5-turbo-0301\",\n  \"object\": \"chat.completion.chunk\"\n}\n</pre> <p>As you can see above, streaming responses have a <code>delta</code> field rather than a <code>message</code> field. <code>delta</code> can hold things like:</p> <ul> <li>a role token (e.g., <code>{\"role\": \"assistant\"}</code>)</li> <li>a content token (e.g., <code>{\"content\": \"\\n\\n\"}</code>)</li> <li>nothing (e.g., <code>{}</code>), when the stream is over</li> </ul> In\u00a0[6]: Copied! <pre># Example of an OpenAI ChatCompletion request with stream=True\n# https://platform.openai.com/docs/guides/chat\n\n# record the time before the request is sent\nstart_time = time.time()\n\n# send a ChatCompletion request to count to 100\nresponse = openai.ChatCompletion.create(\n    model='gpt-3.5-turbo',\n    messages=[\n        {'role': 'user', 'content': 'Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}\n    ],\n    temperature=0,\n    stream=True  # again, we set stream=True\n)\n\n# create variables to collect the stream of chunks\ncollected_chunks = []\ncollected_messages = []\n# iterate through the stream of events\nfor chunk in response:\n    chunk_time = time.time() - start_time  # calculate the time delay of the chunk\n    collected_chunks.append(chunk)  # save the event response\n    chunk_message = chunk['choices'][0]['delta']  # extract the message\n    collected_messages.append(chunk_message)  # save the message\n    print(f\"Message received {chunk_time:.2f} seconds after request: {chunk_message}\")  # print the delay and text\n\n# print the time delay and text received\nprint(f\"Full response received {chunk_time:.2f} seconds after request\")\nfull_reply_content = ''.join([m.get('content', '') for m in collected_messages])\nprint(f\"Full conversation received: {full_reply_content}\")\n</pre> # Example of an OpenAI ChatCompletion request with stream=True # https://platform.openai.com/docs/guides/chat  # record the time before the request is sent start_time = time.time()  # send a ChatCompletion request to count to 100 response = openai.ChatCompletion.create(     model='gpt-3.5-turbo',     messages=[         {'role': 'user', 'content': 'Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}     ],     temperature=0,     stream=True  # again, we set stream=True )  # create variables to collect the stream of chunks collected_chunks = [] collected_messages = [] # iterate through the stream of events for chunk in response:     chunk_time = time.time() - start_time  # calculate the time delay of the chunk     collected_chunks.append(chunk)  # save the event response     chunk_message = chunk['choices'][0]['delta']  # extract the message     collected_messages.append(chunk_message)  # save the message     print(f\"Message received {chunk_time:.2f} seconds after request: {chunk_message}\")  # print the delay and text  # print the time delay and text received print(f\"Full response received {chunk_time:.2f} seconds after request\") full_reply_content = ''.join([m.get('content', '') for m in collected_messages]) print(f\"Full conversation received: {full_reply_content}\")  <pre>Message received 0.10 seconds after request: {\n  \"role\": \"assistant\"\n}\nMessage received 0.10 seconds after request: {\n  \"content\": \"\\n\\n\"\n}\nMessage received 0.10 seconds after request: {\n  \"content\": \"1\"\n}\nMessage received 0.11 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 0.12 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 0.13 seconds after request: {\n  \"content\": \"2\"\n}\nMessage received 0.14 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 0.15 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 0.16 seconds after request: {\n  \"content\": \"3\"\n}\nMessage received 0.17 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 0.18 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 0.19 seconds after request: {\n  \"content\": \"4\"\n}\nMessage received 0.20 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 0.21 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 0.22 seconds after request: {\n  \"content\": \"5\"\n}\nMessage received 0.23 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 0.24 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 0.25 seconds after request: {\n  \"content\": \"6\"\n}\nMessage received 0.26 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 0.27 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 0.28 seconds after request: {\n  \"content\": \"7\"\n}\nMessage received 0.29 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 0.30 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 0.30 seconds after request: {\n  \"content\": \"8\"\n}\nMessage received 0.31 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 0.32 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 0.33 seconds after request: {\n  \"content\": \"9\"\n}\nMessage received 0.34 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 0.35 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 0.37 seconds after request: {\n  \"content\": \"10\"\n}\nMessage received 0.40 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 0.43 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 0.43 seconds after request: {\n  \"content\": \"11\"\n}\nMessage received 0.43 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 0.43 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 0.43 seconds after request: {\n  \"content\": \"12\"\n}\nMessage received 0.43 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 0.44 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 0.45 seconds after request: {\n  \"content\": \"13\"\n}\nMessage received 0.46 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 0.47 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 0.48 seconds after request: {\n  \"content\": \"14\"\n}\nMessage received 0.49 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 0.50 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 0.51 seconds after request: {\n  \"content\": \"15\"\n}\nMessage received 0.52 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 0.53 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 0.53 seconds after request: {\n  \"content\": \"16\"\n}\nMessage received 0.55 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 0.55 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 0.56 seconds after request: {\n  \"content\": \"17\"\n}\nMessage received 0.57 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 0.58 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 0.59 seconds after request: {\n  \"content\": \"18\"\n}\nMessage received 0.60 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 0.61 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 0.62 seconds after request: {\n  \"content\": \"19\"\n}\nMessage received 0.63 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 0.64 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 0.65 seconds after request: {\n  \"content\": \"20\"\n}\nMessage received 0.66 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 0.67 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 0.68 seconds after request: {\n  \"content\": \"21\"\n}\nMessage received 0.69 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 0.70 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 0.71 seconds after request: {\n  \"content\": \"22\"\n}\nMessage received 0.72 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 0.73 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 0.74 seconds after request: {\n  \"content\": \"23\"\n}\nMessage received 0.75 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 0.75 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 0.76 seconds after request: {\n  \"content\": \"24\"\n}\nMessage received 0.79 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 0.79 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 0.79 seconds after request: {\n  \"content\": \"25\"\n}\nMessage received 0.80 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 0.81 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 0.82 seconds after request: {\n  \"content\": \"26\"\n}\nMessage received 0.83 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 0.84 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 0.85 seconds after request: {\n  \"content\": \"27\"\n}\nMessage received 0.86 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 0.87 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 0.88 seconds after request: {\n  \"content\": \"28\"\n}\nMessage received 0.89 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 0.90 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 0.92 seconds after request: {\n  \"content\": \"29\"\n}\nMessage received 0.92 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 0.93 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 0.94 seconds after request: {\n  \"content\": \"30\"\n}\nMessage received 0.95 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 0.96 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 0.97 seconds after request: {\n  \"content\": \"31\"\n}\nMessage received 0.98 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 0.99 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 1.00 seconds after request: {\n  \"content\": \"32\"\n}\nMessage received 1.01 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 1.02 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 1.03 seconds after request: {\n  \"content\": \"33\"\n}\nMessage received 1.04 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 1.05 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 1.06 seconds after request: {\n  \"content\": \"34\"\n}\nMessage received 1.07 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 1.08 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 1.09 seconds after request: {\n  \"content\": \"35\"\n}\nMessage received 1.10 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 1.11 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 1.12 seconds after request: {\n  \"content\": \"36\"\n}\nMessage received 1.13 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 1.13 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 1.14 seconds after request: {\n  \"content\": \"37\"\n}\nMessage received 1.15 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 1.17 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 1.18 seconds after request: {\n  \"content\": \"38\"\n}\nMessage received 1.19 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 1.19 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 1.20 seconds after request: {\n  \"content\": \"39\"\n}\nMessage received 1.21 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 1.22 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 1.23 seconds after request: {\n  \"content\": \"40\"\n}\nMessage received 1.24 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 1.25 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 1.26 seconds after request: {\n  \"content\": \"41\"\n}\nMessage received 1.27 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 1.28 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 1.29 seconds after request: {\n  \"content\": \"42\"\n}\nMessage received 1.30 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 1.31 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 1.32 seconds after request: {\n  \"content\": \"43\"\n}\nMessage received 1.33 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 1.34 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 1.37 seconds after request: {\n  \"content\": \"44\"\n}\nMessage received 1.37 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 1.37 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 1.37 seconds after request: {\n  \"content\": \"45\"\n}\nMessage received 2.10 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.10 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.10 seconds after request: {\n  \"content\": \"46\"\n}\nMessage received 2.10 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.10 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.10 seconds after request: {\n  \"content\": \"47\"\n}\nMessage received 2.10 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.10 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.10 seconds after request: {\n  \"content\": \"48\"\n}\nMessage received 2.10 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.10 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.10 seconds after request: {\n  \"content\": \"49\"\n}\nMessage received 2.10 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.10 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.10 seconds after request: {\n  \"content\": \"50\"\n}\nMessage received 2.10 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.10 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.10 seconds after request: {\n  \"content\": \"51\"\n}\nMessage received 2.10 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.10 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.10 seconds after request: {\n  \"content\": \"52\"\n}\nMessage received 2.10 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.10 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.10 seconds after request: {\n  \"content\": \"53\"\n}\nMessage received 2.10 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.10 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.10 seconds after request: {\n  \"content\": \"54\"\n}\nMessage received 2.24 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.24 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.24 seconds after request: {\n  \"content\": \"55\"\n}\nMessage received 2.24 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.24 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.24 seconds after request: {\n  \"content\": \"56\"\n}\nMessage received 2.24 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.24 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.24 seconds after request: {\n  \"content\": \"57\"\n}\nMessage received 2.24 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.24 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.24 seconds after request: {\n  \"content\": \"58\"\n}\nMessage received 2.24 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.24 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.24 seconds after request: {\n  \"content\": \"59\"\n}\nMessage received 2.24 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.24 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.24 seconds after request: {\n  \"content\": \"60\"\n}\nMessage received 2.24 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.24 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.24 seconds after request: {\n  \"content\": \"61\"\n}\nMessage received 2.24 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.24 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.24 seconds after request: {\n  \"content\": \"62\"\n}\nMessage received 2.24 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \"63\"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \"64\"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \"65\"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \"66\"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \"67\"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \"68\"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \"69\"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \"70\"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \"71\"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \"72\"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \"73\"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \"74\"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.25 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.26 seconds after request: {\n  \"content\": \"75\"\n}\nMessage received 2.26 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.26 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.27 seconds after request: {\n  \"content\": \"76\"\n}\nMessage received 2.28 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.29 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.29 seconds after request: {\n  \"content\": \"77\"\n}\nMessage received 2.31 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.32 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.32 seconds after request: {\n  \"content\": \"78\"\n}\nMessage received 2.33 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.35 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.35 seconds after request: {\n  \"content\": \"79\"\n}\nMessage received 2.36 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.37 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.38 seconds after request: {\n  \"content\": \"80\"\n}\nMessage received 2.39 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.40 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.41 seconds after request: {\n  \"content\": \"81\"\n}\nMessage received 2.42 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.43 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.44 seconds after request: {\n  \"content\": \"82\"\n}\nMessage received 2.45 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.46 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.47 seconds after request: {\n  \"content\": \"83\"\n}\nMessage received 2.48 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.49 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.50 seconds after request: {\n  \"content\": \"84\"\n}\nMessage received 2.51 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.52 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.53 seconds after request: {\n  \"content\": \"85\"\n}\nMessage received 2.54 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.55 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.56 seconds after request: {\n  \"content\": \"86\"\n}\nMessage received 2.57 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.58 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.59 seconds after request: {\n  \"content\": \"87\"\n}\nMessage received 2.60 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.60 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.62 seconds after request: {\n  \"content\": \"88\"\n}\nMessage received 2.63 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.63 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.64 seconds after request: {\n  \"content\": \"89\"\n}\nMessage received 2.66 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.66 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.68 seconds after request: {\n  \"content\": \"90\"\n}\nMessage received 2.68 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.69 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.70 seconds after request: {\n  \"content\": \"91\"\n}\nMessage received 2.71 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.72 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.73 seconds after request: {\n  \"content\": \"92\"\n}\nMessage received 2.74 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.75 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.76 seconds after request: {\n  \"content\": \"93\"\n}\nMessage received 2.77 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.78 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.79 seconds after request: {\n  \"content\": \"94\"\n}\nMessage received 2.80 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.81 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.81 seconds after request: {\n  \"content\": \"95\"\n}\nMessage received 2.82 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.83 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.84 seconds after request: {\n  \"content\": \"96\"\n}\nMessage received 2.85 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.86 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.87 seconds after request: {\n  \"content\": \"97\"\n}\nMessage received 2.88 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.88 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.89 seconds after request: {\n  \"content\": \"98\"\n}\nMessage received 2.90 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.91 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.92 seconds after request: {\n  \"content\": \"99\"\n}\nMessage received 2.93 seconds after request: {\n  \"content\": \",\"\n}\nMessage received 2.93 seconds after request: {\n  \"content\": \" \"\n}\nMessage received 2.94 seconds after request: {\n  \"content\": \"100\"\n}\nMessage received 2.95 seconds after request: {\n  \"content\": \".\"\n}\nMessage received 2.97 seconds after request: {}\nFull response received 2.97 seconds after request\nFull conversation received: \n\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100.\n</pre> In\u00a0[7]: Copied! <pre># Example of an OpenAI Completion request\n# https://beta.openai.com/docs/api-reference/completions/create\n\n# record the time before the request is sent\nstart_time = time.time()\n\n# send a Completion request to count to 100\nresponse = openai.Completion.create(\n    model='text-davinci-002',\n    prompt='1,2,3,',\n    max_tokens=193,\n    temperature=0,\n)\n\n# calculate the time it took to receive the response\nresponse_time = time.time() - start_time\n\n# extract the text from the response\ncompletion_text = response['choices'][0]['text']\n\n# print the time delay and text received\nprint(f\"Full response received {response_time:.2f} seconds after request\")\nprint(f\"Full text received: {completion_text}\")\n</pre> # Example of an OpenAI Completion request # https://beta.openai.com/docs/api-reference/completions/create  # record the time before the request is sent start_time = time.time()  # send a Completion request to count to 100 response = openai.Completion.create(     model='text-davinci-002',     prompt='1,2,3,',     max_tokens=193,     temperature=0, )  # calculate the time it took to receive the response response_time = time.time() - start_time  # extract the text from the response completion_text = response['choices'][0]['text']  # print the time delay and text received print(f\"Full response received {response_time:.2f} seconds after request\") print(f\"Full text received: {completion_text}\") <pre>Full response received 3.43 seconds after request\nFull text received: 4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100\n</pre> In\u00a0[9]: Copied! <pre># Example of an OpenAI Completion request, using the stream=True option\n# https://beta.openai.com/docs/api-reference/completions/create\n\n# record the time before the request is sent\nstart_time = time.time()\n\n# send a Completion request to count to 100\nresponse = openai.Completion.create(\n    model='text-davinci-002',\n    prompt='1,2,3,',\n    max_tokens=193,\n    temperature=0,\n    stream=True,  # this time, we set stream=True\n)\n\n# create variables to collect the stream of events\ncollected_events = []\ncompletion_text = ''\n# iterate through the stream of events\nfor event in response:\n    event_time = time.time() - start_time  # calculate the time delay of the event\n    collected_events.append(event)  # save the event response\n    event_text = event['choices'][0]['text']  # extract the text\n    completion_text += event_text  # append the text\n    print(f\"Text received: {event_text} ({event_time:.2f} seconds after request)\")  # print the delay and text\n\n# print the time delay and text received\nprint(f\"Full response received {event_time:.2f} seconds after request\")\nprint(f\"Full text received: {completion_text}\")\n</pre> # Example of an OpenAI Completion request, using the stream=True option # https://beta.openai.com/docs/api-reference/completions/create  # record the time before the request is sent start_time = time.time()  # send a Completion request to count to 100 response = openai.Completion.create(     model='text-davinci-002',     prompt='1,2,3,',     max_tokens=193,     temperature=0,     stream=True,  # this time, we set stream=True )  # create variables to collect the stream of events collected_events = [] completion_text = '' # iterate through the stream of events for event in response:     event_time = time.time() - start_time  # calculate the time delay of the event     collected_events.append(event)  # save the event response     event_text = event['choices'][0]['text']  # extract the text     completion_text += event_text  # append the text     print(f\"Text received: {event_text} ({event_time:.2f} seconds after request)\")  # print the delay and text  # print the time delay and text received print(f\"Full response received {event_time:.2f} seconds after request\") print(f\"Full text received: {completion_text}\") <pre>Text received: 4 (0.18 seconds after request)\nText received: , (0.19 seconds after request)\nText received: 5 (0.21 seconds after request)\nText received: , (0.23 seconds after request)\nText received: 6 (0.25 seconds after request)\nText received: , (0.26 seconds after request)\nText received: 7 (0.28 seconds after request)\nText received: , (0.29 seconds after request)\nText received: 8 (0.31 seconds after request)\nText received: , (0.33 seconds after request)\nText received: 9 (0.35 seconds after request)\nText received: , (0.36 seconds after request)\nText received: 10 (0.38 seconds after request)\nText received: , (0.39 seconds after request)\nText received: 11 (0.41 seconds after request)\nText received: , (0.42 seconds after request)\nText received: 12 (0.44 seconds after request)\nText received: , (0.45 seconds after request)\nText received: 13 (0.47 seconds after request)\nText received: , (0.48 seconds after request)\nText received: 14 (0.50 seconds after request)\nText received: , (0.51 seconds after request)\nText received: 15 (0.53 seconds after request)\nText received: , (0.54 seconds after request)\nText received: 16 (0.56 seconds after request)\nText received: , (0.57 seconds after request)\nText received: 17 (0.59 seconds after request)\nText received: , (0.62 seconds after request)\nText received: 18 (0.62 seconds after request)\nText received: , (0.63 seconds after request)\nText received: 19 (0.64 seconds after request)\nText received: , (0.66 seconds after request)\nText received: 20 (0.67 seconds after request)\nText received: , (0.68 seconds after request)\nText received: 21 (0.70 seconds after request)\nText received: , (0.71 seconds after request)\nText received: 22 (0.73 seconds after request)\nText received: , (0.74 seconds after request)\nText received: 23 (0.76 seconds after request)\nText received: , (0.77 seconds after request)\nText received: 24 (0.78 seconds after request)\nText received: , (0.80 seconds after request)\nText received: 25 (0.81 seconds after request)\nText received: , (0.82 seconds after request)\nText received: 26 (0.84 seconds after request)\nText received: , (0.85 seconds after request)\nText received: 27 (0.89 seconds after request)\nText received: , (0.90 seconds after request)\nText received: 28 (0.90 seconds after request)\nText received: , (0.91 seconds after request)\nText received: 29 (0.92 seconds after request)\nText received: , (0.94 seconds after request)\nText received: 30 (0.95 seconds after request)\nText received: , (0.96 seconds after request)\nText received: 31 (0.97 seconds after request)\nText received: , (0.99 seconds after request)\nText received: 32 (1.00 seconds after request)\nText received: , (1.01 seconds after request)\nText received: 33 (1.03 seconds after request)\nText received: , (1.04 seconds after request)\nText received: 34 (1.05 seconds after request)\nText received: , (1.07 seconds after request)\nText received: 35 (1.08 seconds after request)\nText received: , (1.10 seconds after request)\nText received: 36 (1.11 seconds after request)\nText received: , (1.12 seconds after request)\nText received: 37 (1.13 seconds after request)\nText received: , (1.15 seconds after request)\nText received: 38 (1.16 seconds after request)\nText received: , (1.18 seconds after request)\nText received: 39 (1.19 seconds after request)\nText received: , (1.20 seconds after request)\nText received: 40 (1.22 seconds after request)\nText received: , (1.24 seconds after request)\nText received: 41 (1.25 seconds after request)\nText received: , (1.26 seconds after request)\nText received: 42 (1.27 seconds after request)\nText received: , (1.29 seconds after request)\nText received: 43 (1.30 seconds after request)\nText received: , (1.31 seconds after request)\nText received: 44 (1.32 seconds after request)\nText received: , (1.34 seconds after request)\nText received: 45 (1.35 seconds after request)\nText received: , (1.36 seconds after request)\nText received: 46 (1.38 seconds after request)\nText received: , (1.39 seconds after request)\nText received: 47 (1.40 seconds after request)\nText received: , (1.42 seconds after request)\nText received: 48 (1.43 seconds after request)\nText received: , (1.45 seconds after request)\nText received: 49 (1.47 seconds after request)\nText received: , (1.47 seconds after request)\nText received: 50 (1.49 seconds after request)\nText received: , (1.50 seconds after request)\nText received: 51 (1.51 seconds after request)\nText received: , (1.53 seconds after request)\nText received: 52 (1.54 seconds after request)\nText received: , (1.55 seconds after request)\nText received: 53 (1.57 seconds after request)\nText received: , (1.58 seconds after request)\nText received: 54 (1.59 seconds after request)\nText received: , (1.61 seconds after request)\nText received: 55 (1.62 seconds after request)\nText received: , (1.64 seconds after request)\nText received: 56 (1.65 seconds after request)\nText received: , (1.66 seconds after request)\nText received: 57 (1.69 seconds after request)\nText received: , (1.69 seconds after request)\nText received: 58 (1.70 seconds after request)\nText received: , (1.72 seconds after request)\nText received: 59 (1.73 seconds after request)\nText received: , (1.74 seconds after request)\nText received: 60 (1.76 seconds after request)\nText received: , (1.77 seconds after request)\nText received: 61 (1.78 seconds after request)\nText received: , (1.80 seconds after request)\nText received: 62 (1.81 seconds after request)\nText received: , (1.83 seconds after request)\nText received: 63 (1.84 seconds after request)\nText received: , (1.85 seconds after request)\nText received: 64 (1.86 seconds after request)\nText received: , (1.88 seconds after request)\nText received: 65 (1.89 seconds after request)\nText received: , (1.90 seconds after request)\nText received: 66 (1.92 seconds after request)\nText received: , (1.93 seconds after request)\nText received: 67 (1.95 seconds after request)\nText received: , (1.96 seconds after request)\nText received: 68 (1.99 seconds after request)\nText received: , (1.99 seconds after request)\nText received: 69 (2.00 seconds after request)\nText received: , (2.01 seconds after request)\nText received: 70 (2.03 seconds after request)\nText received: , (2.04 seconds after request)\nText received: 71 (2.05 seconds after request)\nText received: , (2.07 seconds after request)\nText received: 72 (2.08 seconds after request)\nText received: , (2.09 seconds after request)\nText received: 73 (2.11 seconds after request)\nText received: , (2.12 seconds after request)\nText received: 74 (2.13 seconds after request)\nText received: , (2.15 seconds after request)\nText received: 75 (2.16 seconds after request)\nText received: , (2.17 seconds after request)\nText received: 76 (2.18 seconds after request)\nText received: , (2.20 seconds after request)\nText received: 77 (2.22 seconds after request)\nText received: , (2.23 seconds after request)\nText received: 78 (2.24 seconds after request)\nText received: , (2.25 seconds after request)\nText received: 79 (2.26 seconds after request)\nText received: , (2.28 seconds after request)\nText received: 80 (2.28 seconds after request)\nText received: , (2.29 seconds after request)\nText received: 81 (2.30 seconds after request)\nText received: , (2.31 seconds after request)\nText received: 82 (2.33 seconds after request)\nText received: , (2.34 seconds after request)\nText received: 83 (2.35 seconds after request)\nText received: , (2.36 seconds after request)\nText received: 84 (2.37 seconds after request)\nText received: , (2.39 seconds after request)\nText received: 85 (2.39 seconds after request)\nText received: , (2.40 seconds after request)\nText received: 86 (2.43 seconds after request)\nText received: , (2.43 seconds after request)\nText received: 87 (2.44 seconds after request)\nText received: , (2.45 seconds after request)\nText received: 88 (2.46 seconds after request)\nText received: , (2.47 seconds after request)\nText received: 89 (2.48 seconds after request)\nText received: , (2.49 seconds after request)\nText received: 90 (2.50 seconds after request)\nText received: , (2.51 seconds after request)\nText received: 91 (2.52 seconds after request)\nText received: , (2.54 seconds after request)\nText received: 92 (2.55 seconds after request)\nText received: , (2.57 seconds after request)\nText received: 93 (2.57 seconds after request)\nText received: , (2.58 seconds after request)\nText received: 94 (2.59 seconds after request)\nText received: , (2.60 seconds after request)\nText received: 95 (2.62 seconds after request)\nText received: , (2.62 seconds after request)\nText received: 96 (2.64 seconds after request)\nText received: , (2.65 seconds after request)\nText received: 97 (2.66 seconds after request)\nText received: , (2.67 seconds after request)\nText received: 98 (2.68 seconds after request)\nText received: , (2.69 seconds after request)\nText received: 99 (2.71 seconds after request)\nText received: , (2.72 seconds after request)\nText received: 100 (2.73 seconds after request)\nFull response received 2.73 seconds after request\nFull text received: 4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100\n</pre>"},{"location":"examples/How_to_stream_completions/#How-to-stream-completions","title":"How to stream completions\u00b6","text":"<p>By default, when you request a completion from the OpenAI, the entire completion is generated before being sent back in a single response.</p> <p>If you're generating long completions, waiting for the response can take many seconds.</p> <p>To get responses sooner, you can 'stream' the completion as it's being generated. This allows you to start printing or processing the beginning of the completion before the full completion is finished.</p> <p>To stream completions, set <code>stream=True</code> when calling the chat completions or completions endpoints. This will return an object that streams back the response as data-only server-sent events. Extract chunks from the <code>delta</code> field rather than the <code>message</code> field.</p>"},{"location":"examples/How_to_stream_completions/#Downsides","title":"Downsides\u00b6","text":"<p>Note that using <code>stream=True</code> in a production application makes it more difficult to moderate the content of the completions, as partial completions may be more difficult to evaluate. which has implications for approved usage.</p> <p>Another small drawback of streaming responses is that the response no longer includes the <code>usage</code> field to tell you how many tokens were consumed. After receiving and combining all of the responses, you can calculate this yourself using <code>tiktoken</code>.</p>"},{"location":"examples/How_to_stream_completions/#Example-code","title":"Example code\u00b6","text":"<p>Below, this notebook shows:</p> <ol> <li>What a typical chat completion response looks like</li> <li>What a streaming chat completion response looks like</li> <li>How much time is saved by streaming a chat completion</li> <li>How to stream non-chat completions (used by older models like <code>text-davinci-003</code>)</li> </ol>"},{"location":"examples/How_to_stream_completions/#1.-What-a-typical-chat-completion-response-looks-like","title":"1. What a typical chat completion response looks like\u00b6","text":"<p>With a typical ChatCompletions API call, the response is first computed and then returned all at once.</p>"},{"location":"examples/How_to_stream_completions/#2.-How-to-stream-a-chat-completion","title":"2. How to stream a chat completion\u00b6","text":"<p>With a streaming API call, the response is sent back incrementally in chunks via an event stream. In Python, you can iterate over these events with a <code>for</code> loop.</p> <p>Let's see what it looks like:</p>"},{"location":"examples/How_to_stream_completions/#3.-How-much-time-is-saved-by-streaming-a-chat-completion","title":"3. How much time is saved by streaming a chat completion\u00b6","text":"<p>Now let's ask <code>gpt-3.5-turbo</code> to count to 100 again, and see how long it takes.</p>"},{"location":"examples/How_to_stream_completions/#Time-comparison","title":"Time comparison\u00b6","text":"<p>In the example above, both requests took about 3 seconds to fully complete. Request times will vary depending on load and other stochastic factors.</p> <p>However, with the streaming request, we received the first token after 0.1 seconds, and subsequent tokens every ~0.01-0.02 seconds.</p>"},{"location":"examples/How_to_stream_completions/#4.-How-to-stream-non-chat-completions-(used-by-older-models-like-text-davinci-003)","title":"4. How to stream non-chat completions (used by older models like <code>text-davinci-003</code>)\u00b6","text":""},{"location":"examples/How_to_stream_completions/#A-typical-completion-request","title":"A typical completion request\u00b6","text":"<p>With a typical Completions API call, the text is first computed and then returned all at once.</p>"},{"location":"examples/How_to_stream_completions/#A-streaming-completion-request","title":"A streaming completion request\u00b6","text":"<p>With a streaming Completions API call, the text is sent back via a series of events. In Python, you can iterate over these events with a <code>for</code> loop.</p>"},{"location":"examples/How_to_stream_completions/#Time-comparison","title":"Time comparison\u00b6","text":"<p>In the example above, both requests took about 3 seconds to fully complete. Request times will vary depending on load and other stochastic factors.</p> <p>However, with the streaming request, we received the first token after 0.18 seconds, and subsequent tokens every ~0.01-0.02 seconds.</p>"},{"location":"examples/Multiclass_classification_for_transactions/","title":"Multiclass Classification for Transactions","text":"In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload\n</pre> %load_ext autoreload %autoreload  In\u00a0[311]: Copied! <pre>import openai\nimport pandas as pd\nimport numpy as np\nimport json\nimport os\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\nCOMPLETIONS_MODEL = \"text-davinci-002\"\n</pre> import openai import pandas as pd import numpy as np import json import os  openai.api_key = os.getenv(\"OPENAI_API_KEY\") COMPLETIONS_MODEL = \"text-davinci-002\" In\u00a0[312]: Copied! <pre>transactions = pd.read_csv('./data/25000_spend_dataset_current.csv', encoding= 'unicode_escape')\nlen(transactions)\n</pre> transactions = pd.read_csv('./data/25000_spend_dataset_current.csv', encoding= 'unicode_escape') len(transactions) Out[312]: <pre>359</pre> In\u00a0[313]: Copied! <pre>transactions.head()\n</pre> transactions.head() Out[313]: Date Supplier Description Transaction value (\u00a3) 0 21/04/2016 M &amp; J Ballantyne Ltd George IV Bridge Work 35098.0 1 26/04/2016 Private Sale Literary &amp; Archival Items 30000.0 2 30/04/2016 City Of Edinburgh Council Non Domestic Rates 40800.0 3 09/05/2016 Computacenter Uk Kelvin Hall 72835.0 4 09/05/2016 John Graham Construction Ltd Causewayside Refurbishment 64361.0 In\u00a0[335]: Copied! <pre>def request_completion(prompt):\n    \n    completion_response =   openai.Completion.create(\n                            prompt=prompt,\n                            temperature=0,\n                            max_tokens=5,\n                            top_p=1,\n                            frequency_penalty=0,\n                            presence_penalty=0,\n                            model=COMPLETIONS_MODEL\n                            )\n        \n    return completion_response\n\ndef classify_transaction(transaction,prompt):\n    \n    prompt = prompt.replace('SUPPLIER_NAME',transaction['Supplier'])\n    prompt = prompt.replace('DESCRIPTION_TEXT',transaction['Description'])\n    prompt = prompt.replace('TRANSACTION_VALUE',str(transaction['Transaction value (\u00a3)']))\n    \n    classification = request_completion(prompt)['choices'][0]['text'].replace('\\n','')\n    \n    return classification\n\n# This function takes your training and validation outputs from the prepare_data function of the Finetuning API, and\n# confirms that each have the same number of classes.\n# If they do not have the same number of classes the fine-tune will fail and return an error\n\ndef check_finetune_classes(train_file,valid_file):\n\n    train_classes = set()\n    valid_classes = set()\n    with open(train_file, 'r') as json_file:\n        json_list = list(json_file)\n        print(len(json_list))\n\n    for json_str in json_list:\n        result = json.loads(json_str)\n        train_classes.add(result['completion'])\n        #print(f\"result: {result['completion']}\")\n        #print(isinstance(result, dict))\n\n    with open(valid_file, 'r') as json_file:\n        json_list = list(json_file)\n        print(len(json_list))\n\n    for json_str in json_list:\n        result = json.loads(json_str)\n        valid_classes.add(result['completion'])\n        #print(f\"result: {result['completion']}\")\n        #print(isinstance(result, dict))\n        \n    if len(train_classes) == len(valid_classes):\n        print('All good')\n        \n    else:\n        print('Classes do not match, please prepare data again')\n</pre> def request_completion(prompt):          completion_response =   openai.Completion.create(                             prompt=prompt,                             temperature=0,                             max_tokens=5,                             top_p=1,                             frequency_penalty=0,                             presence_penalty=0,                             model=COMPLETIONS_MODEL                             )              return completion_response  def classify_transaction(transaction,prompt):          prompt = prompt.replace('SUPPLIER_NAME',transaction['Supplier'])     prompt = prompt.replace('DESCRIPTION_TEXT',transaction['Description'])     prompt = prompt.replace('TRANSACTION_VALUE',str(transaction['Transaction value (\u00a3)']))          classification = request_completion(prompt)['choices'][0]['text'].replace('\\n','')          return classification  # This function takes your training and validation outputs from the prepare_data function of the Finetuning API, and # confirms that each have the same number of classes. # If they do not have the same number of classes the fine-tune will fail and return an error  def check_finetune_classes(train_file,valid_file):      train_classes = set()     valid_classes = set()     with open(train_file, 'r') as json_file:         json_list = list(json_file)         print(len(json_list))      for json_str in json_list:         result = json.loads(json_str)         train_classes.add(result['completion'])         #print(f\"result: {result['completion']}\")         #print(isinstance(result, dict))      with open(valid_file, 'r') as json_file:         json_list = list(json_file)         print(len(json_list))      for json_str in json_list:         result = json.loads(json_str)         valid_classes.add(result['completion'])         #print(f\"result: {result['completion']}\")         #print(isinstance(result, dict))              if len(train_classes) == len(valid_classes):         print('All good')              else:         print('Classes do not match, please prepare data again') In\u00a0[277]: Copied! <pre>zero_shot_prompt = '''You are a data expert working for the National Library of Scotland. \nYou are analysing all transactions over \u00a325,000 in value and classifying them into one of five categories.\nThe five categories are Building Improvement, Literature &amp; Archive, Utility Bills, Professional Services and Software/IT.\nIf you can't tell what it is, say Could not classify\nTransaction:\nSupplier: SUPPLIER_NAME\nDescription: DESCRIPTION_TEXT\nValue: TRANSACTION_VALUE\nThe classification is:'''\n</pre> zero_shot_prompt = '''You are a data expert working for the National Library of Scotland.  You are analysing all transactions over \u00a325,000 in value and classifying them into one of five categories. The five categories are Building Improvement, Literature &amp; Archive, Utility Bills, Professional Services and Software/IT. If you can't tell what it is, say Could not classify                        Transaction:                        Supplier: SUPPLIER_NAME Description: DESCRIPTION_TEXT Value: TRANSACTION_VALUE                        The classification is:''' In\u00a0[315]: Copied! <pre># Get a test transaction\ntransaction = transactions.iloc[0]\n\n# Interpolate the values into the prompt\nprompt = zero_shot_prompt.replace('SUPPLIER_NAME',transaction['Supplier'])\nprompt = prompt.replace('DESCRIPTION_TEXT',transaction['Description'])\nprompt = prompt.replace('TRANSACTION_VALUE',str(transaction['Transaction value (\u00a3)']))\n\n# Use our completion function to return a prediction\ncompletion_response = request_completion(prompt)\nprint(completion_response['choices'][0]['text'])\n</pre> # Get a test transaction transaction = transactions.iloc[0]  # Interpolate the values into the prompt prompt = zero_shot_prompt.replace('SUPPLIER_NAME',transaction['Supplier']) prompt = prompt.replace('DESCRIPTION_TEXT',transaction['Description']) prompt = prompt.replace('TRANSACTION_VALUE',str(transaction['Transaction value (\u00a3)']))  # Use our completion function to return a prediction completion_response = request_completion(prompt) print(completion_response['choices'][0]['text']) <pre> Building Improvement\n</pre> <p>Our first attempt is correct, M &amp; J Ballantyne Ltd are a house builder and the work they performed is indeed Building Improvement.</p> <p>Lets expand the sample size to 25 and see how it performs, again with just a simple prompt to guide it</p> In\u00a0[291]: Copied! <pre>test_transactions = transactions.iloc[:25]\ntest_transactions['Classification'] = test_transactions.apply(lambda x: classify_transaction(x,zero_shot_prompt),axis=1)\n</pre> test_transactions = transactions.iloc[:25] test_transactions['Classification'] = test_transactions.apply(lambda x: classify_transaction(x,zero_shot_prompt),axis=1) <pre>/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \n</pre> In\u00a0[292]: Copied! <pre>test_transactions['Classification'].value_counts()\n</pre> test_transactions['Classification'].value_counts() Out[292]: <pre> Building Improvement    14\n Could not classify       5\n Literature &amp; Archive     3\n Software/IT              2\n Utility Bills            1\nName: Classification, dtype: int64</pre> In\u00a0[293]: Copied! <pre>test_transactions.head(25)\n</pre> test_transactions.head(25) Out[293]: Date Supplier Description Transaction value (\u00a3) Classification 0 21/04/2016 M &amp; J Ballantyne Ltd George IV Bridge Work 35098.0 Building Improvement 1 26/04/2016 Private Sale Literary &amp; Archival Items 30000.0 Literature &amp; Archive 2 30/04/2016 City Of Edinburgh Council Non Domestic Rates 40800.0 Utility Bills 3 09/05/2016 Computacenter Uk Kelvin Hall 72835.0 Software/IT 4 09/05/2016 John Graham Construction Ltd Causewayside Refurbishment 64361.0 Building Improvement 5 09/05/2016 A McGillivray Causewayside Refurbishment 53690.0 Building Improvement 6 16/05/2016 John Graham Construction Ltd Causewayside Refurbishment 365344.0 Building Improvement 7 23/05/2016 Computacenter Uk Kelvin Hall 26506.0 Software/IT 8 23/05/2016 ECG Facilities Service Facilities Management Charge 32777.0 Building Improvement 9 23/05/2016 ECG Facilities Service Facilities Management Charge 32777.0 Building Improvement 10 30/05/2016 ALDL ALDL Charges 32317.0 Could not classify 11 10/06/2016 Wavetek Ltd Kelvin Hall 87589.0 Could not classify 12 10/06/2016 John Graham Construction Ltd Causewayside Refurbishment 381803.0 Building Improvement 13 28/06/2016 ECG Facilities Service Facilities Management Charge 32832.0 Building Improvement 14 30/06/2016 Glasgow City Council Kelvin Hall 1700000.0 Building Improvement 15 11/07/2016 Wavetek Ltd Kelvin Hall 65692.0 Could not classify 16 11/07/2016 John Graham Construction Ltd Causewayside Refurbishment 139845.0 Building Improvement 17 15/07/2016 Sotheby'S Literary &amp; Archival Items 28500.0 Literature &amp; Archive 18 18/07/2016 Christies Literary &amp; Archival Items 33800.0 Literature &amp; Archive 19 25/07/2016 A McGillivray Causewayside Refurbishment 30113.0 Building Improvement 20 31/07/2016 ALDL ALDL Charges 32317.0 Could not classify 21 08/08/2016 ECG Facilities Service Facilities Management Charge 32795.0 Building Improvement 22 15/08/2016 Creative Video Productions Ltd Kelvin Hall 26866.0 Could not classify 23 15/08/2016 John Graham Construction Ltd Causewayside Refurbishment 196807.0 Building Improvement 24 24/08/2016 ECG Facilities Service Facilities Management Charge 32795.0 Building Improvement <p>Initial results are pretty good even with no labelled examples! The ones that it could not classify were tougher cases with few clues as to their topic, but maybe if we clean up the labelled dataset to give more examples we can get better performance.</p> In\u00a0[317]: Copied! <pre>df = pd.read_csv('./data/labelled_transactions.csv')\ndf.head()\n</pre> df = pd.read_csv('./data/labelled_transactions.csv') df.head() Out[317]: Date Supplier Description Transaction value (\u00a3) Classification 0 15/08/2016 Creative Video Productions Ltd Kelvin Hall 26866 Other 1 29/05/2017 John Graham Construction Ltd Causewayside Refurbishment 74806 Building Improvement 2 29/05/2017 Morris &amp; Spottiswood Ltd George IV Bridge Work 56448 Building Improvement 3 31/05/2017 John Graham Construction Ltd Causewayside Refurbishment 164691 Building Improvement 4 24/07/2017 John Graham Construction Ltd Causewayside Refurbishment 27926 Building Improvement In\u00a0[318]: Copied! <pre>df['combined'] = \"Supplier: \" + df['Supplier'].str.strip() + \"; Description: \" + df['Description'].str.strip() + \"; Value: \" + str(df['Transaction value (\u00a3)']).strip()\ndf.head(2)\n</pre> df['combined'] = \"Supplier: \" + df['Supplier'].str.strip() + \"; Description: \" + df['Description'].str.strip() + \"; Value: \" + str(df['Transaction value (\u00a3)']).strip() df.head(2) Out[318]: Date Supplier Description Transaction value (\u00a3) Classification combined 0 15/08/2016 Creative Video Productions Ltd Kelvin Hall 26866 Other Supplier: Creative Video Productions Ltd; Desc... 1 29/05/2017 John Graham Construction Ltd Causewayside Refurbishment 74806 Building Improvement Supplier: John Graham Construction Ltd; Descri... In\u00a0[319]: Copied! <pre>from transformers import GPT2TokenizerFast\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n\ndf['n_tokens'] = df.combined.apply(lambda x: len(tokenizer.encode(x)))\nlen(df)\n</pre> from transformers import GPT2TokenizerFast tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")  df['n_tokens'] = df.combined.apply(lambda x: len(tokenizer.encode(x))) len(df) Out[319]: <pre>101</pre> In\u00a0[320]: Copied! <pre>embedding_path = './data/transactions_with_embeddings_100.csv'\n</pre> embedding_path = './data/transactions_with_embeddings_100.csv' In\u00a0[321]: Copied! <pre>from openai.embeddings_utils import get_embedding\n\ndf['babbage_similarity'] = df.combined.apply(lambda x: get_embedding(x, engine='text-similarity-babbage-001'))\ndf['babbage_search'] = df.combined.apply(lambda x: get_embedding(x, engine='text-search-babbage-doc-001'))\ndf.to_csv(embedding_path)\n</pre> from openai.embeddings_utils import get_embedding  df['babbage_similarity'] = df.combined.apply(lambda x: get_embedding(x, engine='text-similarity-babbage-001')) df['babbage_search'] = df.combined.apply(lambda x: get_embedding(x, engine='text-search-babbage-doc-001')) df.to_csv(embedding_path) In\u00a0[309]: Copied! <pre>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\n\nfs_df = pd.read_csv(embedding_path)\nfs_df[\"babbage_similarity\"] = fs_df.babbage_similarity.apply(eval).apply(np.array)\nfs_df.head()\n</pre> from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report, accuracy_score  fs_df = pd.read_csv(embedding_path) fs_df[\"babbage_similarity\"] = fs_df.babbage_similarity.apply(eval).apply(np.array) fs_df.head() Out[309]: Unnamed: 0 Date Supplier Description Transaction value (\u00a3) Classification combined n_tokens babbage_similarity babbage_search 0 0 15/08/2016 Creative Video Productions Ltd Kelvin Hall 26866 Other Supplier: Creative Video Productions Ltd; Desc... 136 [-0.009802100248634815, 0.022551486268639565, ... [-0.00232666521333158, 0.019198870286345482, 0... 1 1 29/05/2017 John Graham Construction Ltd Causewayside Refurbishment 74806 Building Improvement Supplier: John Graham Construction Ltd; Descri... 140 [-0.009065819904208183, 0.012094118632376194, ... [0.005169447045773268, 0.00473341578617692, -0... 2 2 29/05/2017 Morris &amp; Spottiswood Ltd George IV Bridge Work 56448 Building Improvement Supplier: Morris &amp; Spottiswood Ltd; Descriptio... 141 [-0.009000026620924473, 0.02405017428100109, -... [0.0028343256562948227, 0.021166473627090454, ... 3 3 31/05/2017 John Graham Construction Ltd Causewayside Refurbishment 164691 Building Improvement Supplier: John Graham Construction Ltd; Descri... 140 [-0.009065819904208183, 0.012094118632376194, ... [0.005169447045773268, 0.00473341578617692, -0... 4 4 24/07/2017 John Graham Construction Ltd Causewayside Refurbishment 27926 Building Improvement Supplier: John Graham Construction Ltd; Descri... 140 [-0.009065819904208183, 0.012094118632376194, ... [0.005169447045773268, 0.00473341578617692, -0... In\u00a0[310]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(\n    list(fs_df.babbage_similarity.values), fs_df.Classification, test_size=0.2, random_state=42\n)\n\nclf = RandomForestClassifier(n_estimators=100)\nclf.fit(X_train, y_train)\npreds = clf.predict(X_test)\nprobas = clf.predict_proba(X_test)\n\nreport = classification_report(y_test, preds)\nprint(report)\n</pre> X_train, X_test, y_train, y_test = train_test_split(     list(fs_df.babbage_similarity.values), fs_df.Classification, test_size=0.2, random_state=42 )  clf = RandomForestClassifier(n_estimators=100) clf.fit(X_train, y_train) preds = clf.predict(X_test) probas = clf.predict_proba(X_test)  report = classification_report(y_test, preds) print(report) <pre>                      precision    recall  f1-score   support\n\nBuilding Improvement       0.92      1.00      0.96        11\nLiterature &amp; Archive       1.00      1.00      1.00         3\n               Other       0.00      0.00      0.00         1\n         Software/IT       1.00      1.00      1.00         1\n       Utility Bills       1.00      1.00      1.00         5\n\n            accuracy                           0.95        21\n           macro avg       0.78      0.80      0.79        21\n        weighted avg       0.91      0.95      0.93        21\n\n</pre> <pre>/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n</pre> <p>Performance for this model is pretty strong, so creating embeddings and using even a simpler classifier looks like an effective approach as well, with the zero-shot classifier helping us do the initial classification of the unlabelled dataset.</p> <p>Lets take it one step further and see if a fine-tuned model trained on this same labelled datasets gives us comparable results</p> In\u00a0[210]: Copied! <pre>ft_prep_df = fs_df.copy()\nlen(ft_prep_df)\n</pre> ft_prep_df = fs_df.copy() len(ft_prep_df) Out[210]: <pre>101</pre> In\u00a0[211]: Copied! <pre>ft_prep_df.head()\n</pre> ft_prep_df.head() Out[211]: Unnamed: 0 Date Supplier Description Transaction value (\u00a3) Classification combined n_tokens babbage_similarity babbage_search 0 0 15/08/2016 Creative Video Productions Ltd Kelvin Hall 26866 Other Supplier: Creative Video Productions Ltd; Desc... 12 [-0.009630300104618073, 0.009887108579277992, ... [-0.008217384107410908, 0.025170527398586273, ... 1 1 29/05/2017 John Graham Construction Ltd Causewayside Refurbishment 74806 Building Improvement Supplier: John Graham Construction Ltd; Descri... 16 [-0.006144719664007425, -0.0018709596479311585... [-0.007424891460686922, 0.008475713431835175, ... 2 2 29/05/2017 Morris &amp; Spottiswood Ltd George IV Bridge Work 56448 Building Improvement Supplier: Morris &amp; Spottiswood Ltd; Descriptio... 17 [-0.005225738976150751, 0.015156379900872707, ... [-0.007611643522977829, 0.030322374776005745, ... 3 3 31/05/2017 John Graham Construction Ltd Causewayside Refurbishment 164691 Building Improvement Supplier: John Graham Construction Ltd; Descri... 16 [-0.006144719664007425, -0.0018709596479311585... [-0.007424891460686922, 0.008475713431835175, ... 4 4 24/07/2017 John Graham Construction Ltd Causewayside Refurbishment 27926 Building Improvement Supplier: John Graham Construction Ltd; Descri... 16 [-0.006144719664007425, -0.0018709596479311585... [-0.007424891460686922, 0.008475713431835175, ... In\u00a0[212]: Copied! <pre>classes = list(set(ft_prep_df['Classification']))\nclass_df = pd.DataFrame(classes).reset_index()\nclass_df.columns = ['class_id','class']\nclass_df  , len(class_df)\n</pre> classes = list(set(ft_prep_df['Classification'])) class_df = pd.DataFrame(classes).reset_index() class_df.columns = ['class_id','class'] class_df  , len(class_df) Out[212]: <pre>(   class_id                 class\n 0         0  Literature &amp; Archive\n 1         1         Utility Bills\n 2         2  Building Improvement\n 3         3           Software/IT\n 4         4                 Other,\n 5)</pre> In\u00a0[215]: Copied! <pre>ft_df_with_class = ft_prep_df.merge(class_df,left_on='Classification',right_on='class',how='inner')\n\n# Adding a leading whitespace onto each completion to help the model\nft_df_with_class['class_id'] = ft_df_with_class.apply(lambda x: ' ' + str(x['class_id']),axis=1)\nft_df_with_class = ft_df_with_class.drop('class', axis=1)\n\n# Adding a common separator onto the end of each prompt so the model knows when a prompt is terminating\nft_df_with_class['prompt'] = ft_df_with_class.apply(lambda x: x['combined'] + '\\n\\n###\\n\\n',axis=1)\nft_df_with_class.head()\n</pre> ft_df_with_class = ft_prep_df.merge(class_df,left_on='Classification',right_on='class',how='inner')  # Adding a leading whitespace onto each completion to help the model ft_df_with_class['class_id'] = ft_df_with_class.apply(lambda x: ' ' + str(x['class_id']),axis=1) ft_df_with_class = ft_df_with_class.drop('class', axis=1)  # Adding a common separator onto the end of each prompt so the model knows when a prompt is terminating ft_df_with_class['prompt'] = ft_df_with_class.apply(lambda x: x['combined'] + '\\n\\n###\\n\\n',axis=1) ft_df_with_class.head() Out[215]: Unnamed: 0 Date Supplier Description Transaction value (\u00a3) Classification combined n_tokens babbage_similarity babbage_search class_id prompt 0 0 15/08/2016 Creative Video Productions Ltd Kelvin Hall 26866 Other Supplier: Creative Video Productions Ltd; Desc... 12 [-0.009630300104618073, 0.009887108579277992, ... [-0.008217384107410908, 0.025170527398586273, ... 4 Supplier: Creative Video Productions Ltd; Desc... 1 51 31/03/2017 NLS Foundation Grant Payment 177500 Other Supplier: NLS Foundation; Description: Grant P... 11 [-0.022305507212877274, 0.008543581701815128, ... [-0.020519884303212166, 0.01993306167423725, -... 4 Supplier: NLS Foundation; Description: Grant P... 2 70 26/06/2017 British Library Legal Deposit Services 50056 Other Supplier: British Library; Description: Legal ... 11 [-0.01019938476383686, 0.015277703292667866, -... [-0.01843327097594738, 0.03343546763062477, -0... 4 Supplier: British Library; Description: Legal ... 3 71 24/07/2017 ALDL Legal Deposit Services 27067 Other Supplier: ALDL; Description: Legal Deposit Ser... 11 [-0.008471488021314144, 0.004098685923963785, ... [-0.012966590002179146, 0.01299362163990736, 0... 4 Supplier: ALDL; Description: Legal Deposit Ser... 4 100 24/07/2017 AM Phillip Vehicle Purchase 26604 Other Supplier: AM Phillip; Description: Vehicle Pur... 10 [-0.003459023078903556, 0.004626389592885971, ... [-0.0010945454705506563, 0.008626140654087067,... 4 Supplier: AM Phillip; Description: Vehicle Pur... In\u00a0[236]: Copied! <pre># This step is unnecessary if you have a number of observations in each class\n# In our case we don't, so we shuffle the data to give us a better chance of getting equal classes in our train and validation sets\n# Our fine-tuned model will error if we have less classes in the validation set, so this is a necessary step\n\nimport random \n\nlabels = [x for x in ft_df_with_class['class_id']]\ntext = [x for x in ft_df_with_class['prompt']]\nft_df = pd.DataFrame(zip(text, labels), columns = ['prompt','class_id']) #[:300]\nft_df.columns = ['prompt','completion']\nft_df['ordering'] = ft_df.apply(lambda x: random.randint(0,len(ft_df)), axis = 1)\nft_df.set_index('ordering',inplace=True)\nft_df_sorted = ft_df.sort_index(ascending=True)\nft_df_sorted.head()\n</pre> # This step is unnecessary if you have a number of observations in each class # In our case we don't, so we shuffle the data to give us a better chance of getting equal classes in our train and validation sets # Our fine-tuned model will error if we have less classes in the validation set, so this is a necessary step  import random   labels = [x for x in ft_df_with_class['class_id']] text = [x for x in ft_df_with_class['prompt']] ft_df = pd.DataFrame(zip(text, labels), columns = ['prompt','class_id']) #[:300] ft_df.columns = ['prompt','completion'] ft_df['ordering'] = ft_df.apply(lambda x: random.randint(0,len(ft_df)), axis = 1) ft_df.set_index('ordering',inplace=True) ft_df_sorted = ft_df.sort_index(ascending=True) ft_df_sorted.head() Out[236]: prompt completion ordering 0 Supplier: Sothebys; Description: Literary &amp; Ar... 0 1 Supplier: Sotheby'S; Description: Literary &amp; A... 0 2 Supplier: City Of Edinburgh Council; Descripti... 1 2 Supplier: John Graham Construction Ltd; Descri... 2 3 Supplier: John Graham Construction Ltd; Descri... 2 In\u00a0[\u00a0]: Copied! <pre># This step is to remove any existing files if we've already produced training/validation sets for this classifier\n#!rm transactions_grouped*\n\n# We output our shuffled dataframe to a .jsonl file and run the prepare_data function to get us our input files\nft_df_sorted.to_json(\"transactions_grouped.jsonl\", orient='records', lines=True)\n!openai tools fine_tunes.prepare_data -f transactions_grouped.jsonl -q\n</pre> # This step is to remove any existing files if we've already produced training/validation sets for this classifier #!rm transactions_grouped*  # We output our shuffled dataframe to a .jsonl file and run the prepare_data function to get us our input files ft_df_sorted.to_json(\"transactions_grouped.jsonl\", orient='records', lines=True) !openai tools fine_tunes.prepare_data -f transactions_grouped.jsonl -q In\u00a0[322]: Copied! <pre># This functions checks that your classes all appear in both prepared files\n# If they don't, the fine-tuned model creation will fail\ncheck_classes('transactions_grouped_prepared_train.jsonl','transactions_grouped_prepared_valid.jsonl')\n</pre> # This functions checks that your classes all appear in both prepared files # If they don't, the fine-tuned model creation will fail check_classes('transactions_grouped_prepared_train.jsonl','transactions_grouped_prepared_valid.jsonl') <pre>31\n8\nAll good\n</pre> In\u00a0[\u00a0]: Copied! <pre># This step creates your model\n!openai api fine_tunes.create -t \"transactions_grouped_prepared_train.jsonl\" -v \"transactions_grouped_prepared_valid.jsonl\" --compute_classification_metrics --classification_n_classes 5 -m curie\n</pre> # This step creates your model !openai api fine_tunes.create -t \"transactions_grouped_prepared_train.jsonl\" -v \"transactions_grouped_prepared_valid.jsonl\" --compute_classification_metrics --classification_n_classes 5 -m curie In\u00a0[323]: Copied! <pre># Congrats, you've got a fine-tuned model!\n# Copy/paste the name provided into the variable below and we'll take it for a spin\nfine_tuned_model = 'curie:ft-personal-2022-10-20-10-42-56'\n</pre> # Congrats, you've got a fine-tuned model! # Copy/paste the name provided into the variable below and we'll take it for a spin fine_tuned_model = 'curie:ft-personal-2022-10-20-10-42-56' In\u00a0[324]: Copied! <pre>test_set = pd.read_json('transactions_grouped_prepared_valid.jsonl', lines=True)\ntest_set.head()\n</pre> test_set = pd.read_json('transactions_grouped_prepared_valid.jsonl', lines=True) test_set.head() Out[324]: prompt completion 0 Supplier: Wavetek Ltd; Description: Kelvin Hal... 2 1 Supplier: ECG Facilities Service; Description:... 1 2 Supplier: M &amp; J Ballantyne Ltd; Description: G... 2 3 Supplier: Private Sale; Description: Literary ... 0 4 Supplier: Ex Libris; Description: IT equipment... 3 In\u00a0[325]: Copied! <pre>test_set['predicted_class'] = test_set.apply(lambda x: openai.Completion.create(model=fine_tuned_model, prompt=x['prompt'], max_tokens=1, temperature=0, logprobs=5),axis=1)\ntest_set['pred'] = test_set.apply(lambda x : x['predicted_class']['choices'][0]['text'],axis=1)\n</pre> test_set['predicted_class'] = test_set.apply(lambda x: openai.Completion.create(model=fine_tuned_model, prompt=x['prompt'], max_tokens=1, temperature=0, logprobs=5),axis=1) test_set['pred'] = test_set.apply(lambda x : x['predicted_class']['choices'][0]['text'],axis=1) In\u00a0[326]: Copied! <pre>test_set['result'] = test_set.apply(lambda x: str(x['pred']).strip() == str(x['completion']).strip(), axis = 1)\n</pre> test_set['result'] = test_set.apply(lambda x: str(x['pred']).strip() == str(x['completion']).strip(), axis = 1) In\u00a0[327]: Copied! <pre>test_set['result'].value_counts()\n</pre> test_set['result'].value_counts() Out[327]: <pre>True     4\nFalse    4\nName: result, dtype: int64</pre> <p>Performance is not great - unfortunately this is expected. With only a few examples of each class, the above approach with embeddings and a traditional classifier worked better.</p> <p>A fine-tuned model works best with a great number of labelled observations. If we had a few hundred or thousand we may get better results, but lets do one last test on a holdout set to confirm that it doesn't generalise well to a new set of observations</p> In\u00a0[330]: Copied! <pre>holdout_df = transactions.copy().iloc[101:]\nholdout_df.head()\n</pre> holdout_df = transactions.copy().iloc[101:] holdout_df.head() Out[330]: Date Supplier Description Transaction value (\u00a3) 101 23/10/2017 City Building LLP Causewayside Refurbishment 53147.0 102 30/10/2017 ECG Facilities Service Facilities Management Charge 35758.0 103 30/10/2017 ECG Facilities Service Facilities Management Charge 35758.0 104 06/11/2017 John Graham Construction Ltd Causewayside Refurbishment 134208.0 105 06/11/2017 ALDL Legal Deposit Services 27067.0 In\u00a0[332]: Copied! <pre>holdout_df['combined'] = \"Supplier: \" + holdout_df['Supplier'].str.strip() + \"; Description: \" + holdout_df['Description'].str.strip() + '\\n\\n###\\n\\n' # + \"; Value: \" + str(df['Transaction value (\u00a3)']).strip()\nholdout_df['prediction_result'] = holdout_df.apply(lambda x: openai.Completion.create(model=fine_tuned_model, prompt=x['combined'], max_tokens=1, temperature=0, logprobs=5),axis=1)\nholdout_df['pred'] = holdout_df.apply(lambda x : x['prediction_result']['choices'][0]['text'],axis=1)\n</pre> holdout_df['combined'] = \"Supplier: \" + holdout_df['Supplier'].str.strip() + \"; Description: \" + holdout_df['Description'].str.strip() + '\\n\\n###\\n\\n' # + \"; Value: \" + str(df['Transaction value (\u00a3)']).strip() holdout_df['prediction_result'] = holdout_df.apply(lambda x: openai.Completion.create(model=fine_tuned_model, prompt=x['combined'], max_tokens=1, temperature=0, logprobs=5),axis=1) holdout_df['pred'] = holdout_df.apply(lambda x : x['prediction_result']['choices'][0]['text'],axis=1) In\u00a0[333]: Copied! <pre>holdout_df.head(10)\n</pre> holdout_df.head(10) Out[333]: Date Supplier Description Transaction value (\u00a3) combined prediction_result pred 101 23/10/2017 City Building LLP Causewayside Refurbishment 53147.0 Supplier: City Building LLP; Description: Caus... {'id': 'cmpl-63YDadbYLo8xKsGY2vReOFCMgTOvG', '... 2 102 30/10/2017 ECG Facilities Service Facilities Management Charge 35758.0 Supplier: ECG Facilities Service; Description:... {'id': 'cmpl-63YDbNK1D7UikDc3xi5ATihg5kQEt', '... 2 103 30/10/2017 ECG Facilities Service Facilities Management Charge 35758.0 Supplier: ECG Facilities Service; Description:... {'id': 'cmpl-63YDbwfiHjkjMWsfTKNt6naeqPzOe', '... 2 104 06/11/2017 John Graham Construction Ltd Causewayside Refurbishment 134208.0 Supplier: John Graham Construction Ltd; Descri... {'id': 'cmpl-63YDbWAndtsRqPTi2ZHZtPodZvOwr', '... 2 105 06/11/2017 ALDL Legal Deposit Services 27067.0 Supplier: ALDL; Description: Legal Deposit Ser... {'id': 'cmpl-63YDbDu7WM3svYWsRAMdDUKtSFDBu', '... 2 106 27/11/2017 Maggs Bros Ltd Literary &amp; Archival Items 26500.0 Supplier: Maggs Bros Ltd; Description: Literar... {'id': 'cmpl-63YDbxNNI8ZH5CJJNxQ0IF9Zf925C', '... 0 107 30/11/2017 Glasgow City Council Kelvin Hall 42345.0 Supplier: Glasgow City Council; Description: K... {'id': 'cmpl-63YDb8R1FWu4bjwM2xE775rouwneV', '... 2 108 11/12/2017 ECG Facilities Service Facilities Management Charge 35758.0 Supplier: ECG Facilities Service; Description:... {'id': 'cmpl-63YDcAPsp37WhbPs9kwfUX0kBk7Hv', '... 2 109 11/12/2017 John Graham Construction Ltd Causewayside Refurbishment 159275.0 Supplier: John Graham Construction Ltd; Descri... {'id': 'cmpl-63YDcML2welrC3wF0nuKgcNmVu1oQ', '... 2 110 08/01/2018 ECG Facilities Service Facilities Management Charge 35758.0 Supplier: ECG Facilities Service; Description:... {'id': 'cmpl-63YDc95SSdOHnIliFB2cjMEEm7Z2u', '... 2 In\u00a0[334]: Copied! <pre>holdout_df['pred'].value_counts()\n</pre> holdout_df['pred'].value_counts() Out[334]: <pre> 2    231\n 0     27\nName: pred, dtype: int64</pre> <p>Well those results were similarly underwhelming - so we've learned that with a dataset with a small number of labelled observations, either zero-shot classification or traditional classification with embeddings return better results than a fine-tuned model.</p> <p>A fine-tuned model is still a great tool, but is more effective when you have a larger number of labelled examples for each class that you're looking to classify</p>"},{"location":"examples/Multiclass_classification_for_transactions/#Multiclass-Classification-for-Transactions","title":"Multiclass Classification for Transactions\u00b6","text":"<p>For this notebook we will be looking to classify a public dataset of transactions into a number of categories that we have predefined. These approaches should be replicable to any multiclass classificaiton use case where we are trying to fit transactional data into predefined categories, and by the end of running through this you should have a few approaches for dealing with both labelled and unlabelled datasets.</p> <p>The different approaches we'll be taking in this notebook are:</p> <ul> <li>Zero-shot Classification: First we'll do zero shot classification to put transactions in one of five named buckets using only a prompt for guidance</li> <li>Classification with Embeddings: Following this we'll create embeddings on a labelled dataset, and then use a traditional classification model to test their effectiveness at identifying our categories</li> <li>Fine-tuned Classification: Lastly we'll produce a fine-tuned model trained on our labelled dataset to see how this compares to the zero-shot and few-shot classification approaches</li> </ul>"},{"location":"examples/Multiclass_classification_for_transactions/#Setup","title":"Setup\u00b6","text":""},{"location":"examples/Multiclass_classification_for_transactions/#Load-dataset","title":"Load dataset\u00b6","text":"<p>We're using a public transaction dataset of transactions over \u00a325k for the Library of Scotland. The dataset has three features that we'll be using:</p> <ul> <li>Supplier: The name of the supplier</li> <li>Description: A text description of the transaction</li> <li>Value: The value of the transaction in GBP</li> </ul> <p>Source:</p> <p>https://data.nls.uk/data/organisational-data/transactions-over-25k/</p>"},{"location":"examples/Multiclass_classification_for_transactions/#Zero-shot-Classification","title":"Zero-shot Classification\u00b6","text":"<p>We'll first assess the performance of the base models at classifying these transactions using a simple prompt. We'll provide the model with 5 categories and a catch-all of \"Could not classify\" for ones that it cannot place.</p>"},{"location":"examples/Multiclass_classification_for_transactions/#Classification-with-Embeddings","title":"Classification with Embeddings\u00b6","text":"<p>Lets create embeddings from the small set that we've classified so far - we've made a set of labelled examples by running the zero-shot classifier on 101 transactions from our dataset and manually correcting the 15 Could not classify results that we got</p>"},{"location":"examples/Multiclass_classification_for_transactions/#Create-embeddings","title":"Create embeddings\u00b6","text":"<p>This initial section reuses the approach from the Obtain_dataset Notebook to create embeddings from a combined field concatenating all of our features</p>"},{"location":"examples/Multiclass_classification_for_transactions/#Use-embeddings-for-classification","title":"Use embeddings for classification\u00b6","text":"<p>Now that we have our embeddings, let see if classifying these into the categories we've named gives us any more success.</p> <p>For this we'll use a template from the Classification_using_embeddings notebook</p>"},{"location":"examples/Multiclass_classification_for_transactions/#Fine-tuned-Transaction-Classification","title":"Fine-tuned Transaction Classification\u00b6","text":"<p>For this use case we're going to try to improve on the few-shot classification from above by training a fine-tuned model on the same labelled set of 101 transactions and applying this fine-tuned model on group of unseen transactions</p>"},{"location":"examples/Multiclass_classification_for_transactions/#Building-Fine-tuned-Classifier","title":"Building Fine-tuned Classifier\u00b6","text":"<p>We'll need to do some data prep first to get our data ready. This will take the following steps:</p> <ul> <li>First we'll list out our classes and replace them with numeric identifiers. Making the model predict a single token rather than multiple consecutive ones like 'Building Improvement' should give us better results</li> <li>We also need to add a common prefix and suffix to each example to aid the model in making predictions - in our case our text is already started with 'Supplier' and we'll add a suffix of '\\n\\n###\\n\\n'</li> <li>Lastly we'll aid a leading whitespace onto each of our target classes for classification, again to aid the model</li> </ul>"},{"location":"examples/Multiclass_classification_for_transactions/#Applying-Fine-tuned-Classifier","title":"Applying Fine-tuned Classifier\u00b6","text":"<p>Now we'll apply our classifier to see how it performs. We only had 31 unique observations in our training set and 8 in our validation set, so lets see how the performance is</p>"},{"location":"examples/Obtain_dataset/","title":"Obtain dataset","text":"<p>To run this notebook, you will need to install: pandas, openai, transformers, plotly, matplotlib, scikit-learn, torch (transformer dep), torchvision, and scipy.</p> In\u00a0[6]: Copied! <pre># imports\nimport pandas as pd\nimport tiktoken\n\nfrom openai.embeddings_utils import get_embedding\n</pre> # imports import pandas as pd import tiktoken  from openai.embeddings_utils import get_embedding  In\u00a0[7]: Copied! <pre># embedding model parameters\nembedding_model = \"text-embedding-ada-002\"\nembedding_encoding = \"cl100k_base\"  # this the encoding for text-embedding-ada-002\nmax_tokens = 8000  # the maximum for text-embedding-ada-002 is 8191\n</pre> # embedding model parameters embedding_model = \"text-embedding-ada-002\" embedding_encoding = \"cl100k_base\"  # this the encoding for text-embedding-ada-002 max_tokens = 8000  # the maximum for text-embedding-ada-002 is 8191  In\u00a0[8]: Copied! <pre># load &amp; inspect dataset\ninput_datapath = \"data/fine_food_reviews_1k.csv\"  # to save space, we provide a pre-filtered dataset\ndf = pd.read_csv(input_datapath, index_col=0)\ndf = df[[\"Time\", \"ProductId\", \"UserId\", \"Score\", \"Summary\", \"Text\"]]\ndf = df.dropna()\ndf[\"combined\"] = (\n    \"Title: \" + df.Summary.str.strip() + \"; Content: \" + df.Text.str.strip()\n)\ndf.head(2)\n</pre> # load &amp; inspect dataset input_datapath = \"data/fine_food_reviews_1k.csv\"  # to save space, we provide a pre-filtered dataset df = pd.read_csv(input_datapath, index_col=0) df = df[[\"Time\", \"ProductId\", \"UserId\", \"Score\", \"Summary\", \"Text\"]] df = df.dropna() df[\"combined\"] = (     \"Title: \" + df.Summary.str.strip() + \"; Content: \" + df.Text.str.strip() ) df.head(2)  Out[8]: Time ProductId UserId Score Summary Text combined 0 1351123200 B003XPF9BO A3R7JR3FMEBXQB 5 where does one  start...and stop... with a tre... Wanted to save some to bring to my Chicago fam... Title: where does one  start...and stop... wit... 1 1351123200 B003JK537S A3JBPC3WFUT5ZP 1 Arrived in pieces Not pleased at all. When I opened the box, mos... Title: Arrived in pieces; Content: Not pleased... In\u00a0[9]: Copied! <pre># subsample to 1k most recent reviews and remove samples that are too long\ntop_n = 1000\ndf = df.sort_values(\"Time\").tail(top_n * 2)  # first cut to first 2k entries, assuming less than half will be filtered out\ndf.drop(\"Time\", axis=1, inplace=True)\n\nencoding = tiktoken.get_encoding(embedding_encoding)\n\n# omit reviews that are too long to embed\ndf[\"n_tokens\"] = df.combined.apply(lambda x: len(encoding.encode(x)))\ndf = df[df.n_tokens &lt;= max_tokens].tail(top_n)\nlen(df)\n</pre> # subsample to 1k most recent reviews and remove samples that are too long top_n = 1000 df = df.sort_values(\"Time\").tail(top_n * 2)  # first cut to first 2k entries, assuming less than half will be filtered out df.drop(\"Time\", axis=1, inplace=True)  encoding = tiktoken.get_encoding(embedding_encoding)  # omit reviews that are too long to embed df[\"n_tokens\"] = df.combined.apply(lambda x: len(encoding.encode(x))) df = df[df.n_tokens &lt;= max_tokens].tail(top_n) len(df)  Out[9]: <pre>1000</pre> In\u00a0[10]: Copied! <pre># Ensure you have your API key set in your environment per the README: https://github.com/openai/openai-python#usage\n\n# This may take a few minutes\ndf[\"embedding\"] = df.combined.apply(lambda x: get_embedding(x, engine=embedding_model))\ndf.to_csv(\"data/fine_food_reviews_with_embeddings_1k.csv\")\n</pre> # Ensure you have your API key set in your environment per the README: https://github.com/openai/openai-python#usage  # This may take a few minutes df[\"embedding\"] = df.combined.apply(lambda x: get_embedding(x, engine=embedding_model)) df.to_csv(\"data/fine_food_reviews_with_embeddings_1k.csv\")"},{"location":"examples/Obtain_dataset/#1.-Load-the-dataset","title":"1. Load the dataset\u00b6","text":"<p>The dataset used in this example is fine-food reviews from Amazon. The dataset contains a total of 568,454 food reviews Amazon users left up to October 2012. We will use a subset of this dataset, consisting of 1,000 most recent reviews for illustration purposes. The reviews are in English and tend to be positive or negative. Each review has a ProductId, UserId, Score, review title (Summary) and review body (Text).</p> <p>We will combine the review summary and review text into a single combined text. The model will encode this combined text and it will output a single vector embedding.</p>"},{"location":"examples/Obtain_dataset/#2.-Get-embeddings-and-save-them-for-future-reuse","title":"2. Get embeddings and save them for future reuse\u00b6","text":""},{"location":"examples/Question_answering_using_embeddings/","title":"Question Answering using Embeddings","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport openai\nimport pandas as pd\nimport pickle\nimport tiktoken\n\nCOMPLETIONS_MODEL = \"text-davinci-003\"\nEMBEDDING_MODEL = \"text-embedding-ada-002\"\n</pre> import numpy as np import openai import pandas as pd import pickle import tiktoken  COMPLETIONS_MODEL = \"text-davinci-003\" EMBEDDING_MODEL = \"text-embedding-ada-002\" <p>By default, GPT-3 isn't an expert on the 2020 Olympics:</p> In\u00a0[2]: Copied! <pre>prompt = \"Who won the 2020 Summer Olympics men's high jump?\"\n\nopenai.Completion.create(\n    prompt=prompt,\n    temperature=0,\n    max_tokens=300,\n    model=COMPLETIONS_MODEL\n)[\"choices\"][0][\"text\"].strip(\" \\n\")\n</pre> prompt = \"Who won the 2020 Summer Olympics men's high jump?\"  openai.Completion.create(     prompt=prompt,     temperature=0,     max_tokens=300,     model=COMPLETIONS_MODEL )[\"choices\"][0][\"text\"].strip(\" \\n\") Out[2]: <pre>\"Marcelo Chierighini of Brazil won the gold medal in the men's high jump at the 2020 Summer Olympics.\"</pre> <p>Marcelo is a gold medalist swimmer, and, we assume, not much of a high jumper! Evidently GPT-3 needs some assistance here.</p> <p>The first issue to tackle is that the model is hallucinating an answer rather than telling us \"I don't know\". This is bad because it makes it hard to trust the answer that the model gives us!</p> In\u00a0[3]: Copied! <pre>prompt = \"\"\"Answer the question as truthfully as possible, and if you're unsure of the answer, say \"Sorry, I don't know\".\n\nQ: Who won the 2020 Summer Olympics men's high jump?\nA:\"\"\"\n\nopenai.Completion.create(\n    prompt=prompt,\n    temperature=0,\n    max_tokens=300,\n    model=COMPLETIONS_MODEL\n)[\"choices\"][0][\"text\"].strip(\" \\n\")\n</pre> prompt = \"\"\"Answer the question as truthfully as possible, and if you're unsure of the answer, say \"Sorry, I don't know\".  Q: Who won the 2020 Summer Olympics men's high jump? A:\"\"\"  openai.Completion.create(     prompt=prompt,     temperature=0,     max_tokens=300,     model=COMPLETIONS_MODEL )[\"choices\"][0][\"text\"].strip(\" \\n\") Out[3]: <pre>\"Sorry, I don't know.\"</pre> <p>To help the model answer the question, we provide extra contextual information in the prompt. When the total required context is short, we can include it in the prompt directly. For example we can use this information taken from Wikipedia. We update the initial prompt to tell the model to explicitly make use of the provided text.</p> In\u00a0[4]: Copied! <pre>prompt = \"\"\"Answer the question as truthfully as possible using the provided text, and if the answer is not contained within the text below, say \"I don't know\"\n\nContext:\nThe men's high jump event at the 2020 Summer Olympics took place between 30 July and 1 August 2021 at the Olympic Stadium.\n33 athletes from 24 nations competed; the total possible number depended on how many nations would use universality places \nto enter athletes in addition to the 32 qualifying through mark or ranking (no universality places were used in 2021).\nItalian athlete Gianmarco Tamberi along with Qatari athlete Mutaz Essa Barshim emerged as joint winners of the event following\na tie between both of them as they cleared 2.37m. Both Tamberi and Barshim agreed to share the gold medal in a rare instance\nwhere the athletes of different nations had agreed to share the same medal in the history of Olympics. \nBarshim in particular was heard to ask a competition official \"Can we have two golds?\" in response to being offered a \n'jump off'. Maksim Nedasekau of Belarus took bronze. The medals were the first ever in the men's high jump for Italy and \nBelarus, the first gold in the men's high jump for Italy and Qatar, and the third consecutive medal in the men's high jump\nfor Qatar (all by Barshim). Barshim became only the second man to earn three medals in high jump, joining Patrik Sj\u00f6berg\nof Sweden (1984 to 1992).\n\nQ: Who won the 2020 Summer Olympics men's high jump?\nA:\"\"\"\n\nopenai.Completion.create(\n    prompt=prompt,\n    temperature=0,\n    max_tokens=300,\n    top_p=1,\n    frequency_penalty=0,\n    presence_penalty=0,\n    model=COMPLETIONS_MODEL\n)[\"choices\"][0][\"text\"].strip(\" \\n\")\n</pre> prompt = \"\"\"Answer the question as truthfully as possible using the provided text, and if the answer is not contained within the text below, say \"I don't know\"  Context: The men's high jump event at the 2020 Summer Olympics took place between 30 July and 1 August 2021 at the Olympic Stadium. 33 athletes from 24 nations competed; the total possible number depended on how many nations would use universality places  to enter athletes in addition to the 32 qualifying through mark or ranking (no universality places were used in 2021). Italian athlete Gianmarco Tamberi along with Qatari athlete Mutaz Essa Barshim emerged as joint winners of the event following a tie between both of them as they cleared 2.37m. Both Tamberi and Barshim agreed to share the gold medal in a rare instance where the athletes of different nations had agreed to share the same medal in the history of Olympics.  Barshim in particular was heard to ask a competition official \"Can we have two golds?\" in response to being offered a  'jump off'. Maksim Nedasekau of Belarus took bronze. The medals were the first ever in the men's high jump for Italy and  Belarus, the first gold in the men's high jump for Italy and Qatar, and the third consecutive medal in the men's high jump for Qatar (all by Barshim). Barshim became only the second man to earn three medals in high jump, joining Patrik Sj\u00f6berg of Sweden (1984 to 1992).  Q: Who won the 2020 Summer Olympics men's high jump? A:\"\"\"  openai.Completion.create(     prompt=prompt,     temperature=0,     max_tokens=300,     top_p=1,     frequency_penalty=0,     presence_penalty=0,     model=COMPLETIONS_MODEL )[\"choices\"][0][\"text\"].strip(\" \\n\") Out[4]: <pre>'Gianmarco Tamberi and Mutaz Essa Barshim emerged as joint winners of the event.'</pre> <p>Adding extra information into the prompt only works when the dataset of extra content that the model may need to know is small enough to fit in a single prompt. What do we do when we need the model to choose relevant contextual information from within a large body of information?</p> <p>In the remainder of this notebook, we will demonstrate a method for augmenting GPT-3 with a large body of additional contextual information by using document embeddings and retrieval. This method answers queries in two steps: first it retrieves the information relevant to the query, then it writes an answer tailored to the question based on the retrieved information. The first step uses the Embeddings API, the second step uses the Completions API.</p> <p>The steps are:</p> <ul> <li>Preprocess the contextual information by splitting it into chunks and create an embedding vector for each chunk.</li> <li>On receiving a query, embed the query in the same vector space as the context chunks and find the context embeddings which are most similar to the query.</li> <li>Prepend the most relevant context embeddings to the query prompt.</li> <li>Submit the question along with the most relevant context to GPT, and receive an answer which makes use of the provided contextual information.</li> </ul> In\u00a0[5]: Copied! <pre># We have hosted the processed dataset, so you can download it directly without having to recreate it.\n# This dataset has already been split into sections, one row for each section of the Wikipedia page.\n\ndf = pd.read_csv('https://cdn.openai.com/API/examples/data/olympics_sections_text.csv')\ndf = df.set_index([\"title\", \"heading\"])\nprint(f\"{len(df)} rows in the data.\")\ndf.sample(5)\n</pre> # We have hosted the processed dataset, so you can download it directly without having to recreate it. # This dataset has already been split into sections, one row for each section of the Wikipedia page.  df = pd.read_csv('https://cdn.openai.com/API/examples/data/olympics_sections_text.csv') df = df.set_index([\"title\", \"heading\"]) print(f\"{len(df)} rows in the data.\") df.sample(5) <pre>3964 rows in the data.\n</pre> Out[5]: content tokens title heading Jamaica at the 2020 Summer Olympics Swimming Jamaican swimmers further achieved qualifying ... 51 Archery at the 2020 Summer Olympics \u2013 Women's individual Background This is the 13th consecutive appearance of the... 136 Germany at the 2020 Summer Olympics Sport climbing Germany entered two sport climbers into the Ol... 98 Cycling at the 2020 Summer Olympics \u2013 Women's BMX racing Competition format The competition was a three-round tournament, ... 215 Volleyball at the 2020 Summer Olympics \u2013 Men's tournament Format The preliminary round was a competition betwee... 104 <p>We preprocess the document sections by creating an embedding vector for each section. An embedding is a vector of numbers that helps us understand how semantically similar or different the texts are. The closer two embeddings are to each other, the more similar are their contents. See the documentation on OpenAI embeddings for more information.</p> <p>This indexing stage can be executed offline and only runs once to precompute the indexes for the dataset so that each piece of content can be retrieved later. Since this is a small example, we will store and search the embeddings locally. If you have a larger dataset, consider using a vector search engine like Pinecone, Weaviate or Qdrant to power the search.</p> In\u00a0[6]: Copied! <pre>def get_embedding(text: str, model: str=EMBEDDING_MODEL) -&gt; list[float]:\n    result = openai.Embedding.create(\n      model=model,\n      input=text\n    )\n    return result[\"data\"][0][\"embedding\"]\n\ndef compute_doc_embeddings(df: pd.DataFrame) -&gt; dict[tuple[str, str], list[float]]:\n\"\"\"\n    Create an embedding for each row in the dataframe using the OpenAI Embeddings API.\n    Return a dictionary that maps between each embedding vector and the index of the row that it corresponds to.\n    \"\"\"\n    return {\n        idx: get_embedding(r.content) for idx, r in df.iterrows()\n    }\n</pre> def get_embedding(text: str, model: str=EMBEDDING_MODEL) -&gt; list[float]:     result = openai.Embedding.create(       model=model,       input=text     )     return result[\"data\"][0][\"embedding\"]  def compute_doc_embeddings(df: pd.DataFrame) -&gt; dict[tuple[str, str], list[float]]:     \"\"\"     Create an embedding for each row in the dataframe using the OpenAI Embeddings API.          Return a dictionary that maps between each embedding vector and the index of the row that it corresponds to.     \"\"\"     return {         idx: get_embedding(r.content) for idx, r in df.iterrows()     } In\u00a0[7]: Copied! <pre>def load_embeddings(fname: str) -&gt; dict[tuple[str, str], list[float]]:\n\"\"\"\n    Read the document embeddings and their keys from a CSV.\n    fname is the path to a CSV with exactly these named columns: \n        \"title\", \"heading\", \"0\", \"1\", ... up to the length of the embedding vectors.\n    \"\"\"\n    \n    df = pd.read_csv(fname, header=0)\n    max_dim = max([int(c) for c in df.columns if c != \"title\" and c != \"heading\"])\n    return {\n           (r.title, r.heading): [r[str(i)] for i in range(max_dim + 1)] for _, r in df.iterrows()\n    }\n</pre> def load_embeddings(fname: str) -&gt; dict[tuple[str, str], list[float]]:     \"\"\"     Read the document embeddings and their keys from a CSV.          fname is the path to a CSV with exactly these named columns:          \"title\", \"heading\", \"0\", \"1\", ... up to the length of the embedding vectors.     \"\"\"          df = pd.read_csv(fname, header=0)     max_dim = max([int(c) for c in df.columns if c != \"title\" and c != \"heading\"])     return {            (r.title, r.heading): [r[str(i)] for i in range(max_dim + 1)] for _, r in df.iterrows()     } <p>Again, we have hosted the embeddings for you so you don't have to re-calculate them from scratch.</p> In\u00a0[8]: Copied! <pre>document_embeddings = load_embeddings(\"https://cdn.openai.com/API/examples/data/olympics_sections_document_embeddings.csv\")\n\n# ===== OR, uncomment the below line to recaculate the embeddings from scratch. ========\n\n# document_embeddings = compute_doc_embeddings(df)\n</pre> document_embeddings = load_embeddings(\"https://cdn.openai.com/API/examples/data/olympics_sections_document_embeddings.csv\")  # ===== OR, uncomment the below line to recaculate the embeddings from scratch. ========  # document_embeddings = compute_doc_embeddings(df) In\u00a0[9]: Copied! <pre># An example embedding:\nexample_entry = list(document_embeddings.items())[0]\nprint(f\"{example_entry[0]} : {example_entry[1][:5]}... ({len(example_entry[1])} entries)\")\n</pre> # An example embedding: example_entry = list(document_embeddings.items())[0] print(f\"{example_entry[0]} : {example_entry[1][:5]}... ({len(example_entry[1])} entries)\") <pre>('2020 Summer Olympics', 'Summary') : [0.0037565305829048, -0.0061981128528714, -0.0087078781798481, -0.0071364338509738, -0.0025227521546185]... (1536 entries)\n</pre> <p>So we have split our document library into sections, and encoded them by creating embedding vectors that represent each chunk. Next we will use these embeddings to answer our users' questions.</p> In\u00a0[10]: Copied! <pre>def vector_similarity(x: list[float], y: list[float]) -&gt; float:\n\"\"\"\n    Returns the similarity between two vectors.\n    Because OpenAI Embeddings are normalized to length 1, the cosine similarity is the same as the dot product.\n    \"\"\"\n    return np.dot(np.array(x), np.array(y))\n\ndef order_document_sections_by_query_similarity(query: str, contexts: dict[(str, str), np.array]) -&gt; list[(float, (str, str))]:\n\"\"\"\n    Find the query embedding for the supplied query, and compare it against all of the pre-calculated document embeddings\n    to find the most relevant sections. \n    Return the list of document sections, sorted by relevance in descending order.\n    \"\"\"\n    query_embedding = get_embedding(query)\n    \n    document_similarities = sorted([\n        (vector_similarity(query_embedding, doc_embedding), doc_index) for doc_index, doc_embedding in contexts.items()\n    ], reverse=True)\n    \n    return document_similarities\n</pre> def vector_similarity(x: list[float], y: list[float]) -&gt; float:     \"\"\"     Returns the similarity between two vectors.          Because OpenAI Embeddings are normalized to length 1, the cosine similarity is the same as the dot product.     \"\"\"     return np.dot(np.array(x), np.array(y))  def order_document_sections_by_query_similarity(query: str, contexts: dict[(str, str), np.array]) -&gt; list[(float, (str, str))]:     \"\"\"     Find the query embedding for the supplied query, and compare it against all of the pre-calculated document embeddings     to find the most relevant sections.           Return the list of document sections, sorted by relevance in descending order.     \"\"\"     query_embedding = get_embedding(query)          document_similarities = sorted([         (vector_similarity(query_embedding, doc_embedding), doc_index) for doc_index, doc_embedding in contexts.items()     ], reverse=True)          return document_similarities In\u00a0[11]: Copied! <pre>order_document_sections_by_query_similarity(\"Who won the men's high jump?\", document_embeddings)[:5]\n</pre> order_document_sections_by_query_similarity(\"Who won the men's high jump?\", document_embeddings)[:5] Out[11]: <pre>[(0.884864308450606,\n  (\"Athletics at the 2020 Summer Olympics \u2013 Men's high jump\", 'Summary')),\n (0.8633938355935518,\n  (\"Athletics at the 2020 Summer Olympics \u2013 Men's pole vault\", 'Summary')),\n (0.861639730583851,\n  (\"Athletics at the 2020 Summer Olympics \u2013 Men's long jump\", 'Summary')),\n (0.8560523857031264,\n  (\"Athletics at the 2020 Summer Olympics \u2013 Men's triple jump\", 'Summary')),\n (0.8469039130441247,\n  (\"Athletics at the 2020 Summer Olympics \u2013 Men's 110 metres hurdles\",\n   'Summary'))]</pre> In\u00a0[12]: Copied! <pre>order_document_sections_by_query_similarity(\"Who won the women's high jump?\", document_embeddings)[:5]\n</pre> order_document_sections_by_query_similarity(\"Who won the women's high jump?\", document_embeddings)[:5] Out[12]: <pre>[(0.8726165220223294,\n  (\"Athletics at the 2020 Summer Olympics \u2013 Women's long jump\", 'Summary')),\n (0.8682196158313358,\n  (\"Athletics at the 2020 Summer Olympics \u2013 Women's high jump\", 'Summary')),\n (0.863191526370672,\n  (\"Athletics at the 2020 Summer Olympics \u2013 Women's pole vault\", 'Summary')),\n (0.8609374262115406,\n  (\"Athletics at the 2020 Summer Olympics \u2013 Women's triple jump\", 'Summary')),\n (0.8581515607285688,\n  (\"Athletics at the 2020 Summer Olympics \u2013 Women's 100 metres hurdles\",\n   'Summary'))]</pre> <p>We can see that the most relevant document sections for each question include the summaries for the Men's and Women's high jump competitions - which is exactly what we would expect.</p> In\u00a0[13]: Copied! <pre>MAX_SECTION_LEN = 500\nSEPARATOR = \"\\n* \"\nENCODING = \"gpt2\"  # encoding for text-davinci-003\n\nencoding = tiktoken.get_encoding(ENCODING)\nseparator_len = len(encoding.encode(SEPARATOR))\n\nf\"Context separator contains {separator_len} tokens\"\n</pre> MAX_SECTION_LEN = 500 SEPARATOR = \"\\n* \" ENCODING = \"gpt2\"  # encoding for text-davinci-003  encoding = tiktoken.get_encoding(ENCODING) separator_len = len(encoding.encode(SEPARATOR))  f\"Context separator contains {separator_len} tokens\" Out[13]: <pre>'Context separator contains 3 tokens'</pre> In\u00a0[14]: Copied! <pre>def construct_prompt(question: str, context_embeddings: dict, df: pd.DataFrame) -&gt; str:\n\"\"\"\n    Fetch relevant \n    \"\"\"\n    most_relevant_document_sections = order_document_sections_by_query_similarity(question, context_embeddings)\n    \n    chosen_sections = []\n    chosen_sections_len = 0\n    chosen_sections_indexes = []\n     \n    for _, section_index in most_relevant_document_sections:\n        # Add contexts until we run out of space.        \n        document_section = df.loc[section_index]\n        \n        chosen_sections_len += document_section.tokens + separator_len\n        if chosen_sections_len &gt; MAX_SECTION_LEN:\n            break\n            \n        chosen_sections.append(SEPARATOR + document_section.content.replace(\"\\n\", \" \"))\n        chosen_sections_indexes.append(str(section_index))\n            \n    # Useful diagnostic information\n    print(f\"Selected {len(chosen_sections)} document sections:\")\n    print(\"\\n\".join(chosen_sections_indexes))\n    \n    header = \"\"\"Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don't know.\"\\n\\nContext:\\n\"\"\"\n    \n    return header + \"\".join(chosen_sections) + \"\\n\\n Q: \" + question + \"\\n A:\"\n</pre> def construct_prompt(question: str, context_embeddings: dict, df: pd.DataFrame) -&gt; str:     \"\"\"     Fetch relevant      \"\"\"     most_relevant_document_sections = order_document_sections_by_query_similarity(question, context_embeddings)          chosen_sections = []     chosen_sections_len = 0     chosen_sections_indexes = []           for _, section_index in most_relevant_document_sections:         # Add contexts until we run out of space.                 document_section = df.loc[section_index]                  chosen_sections_len += document_section.tokens + separator_len         if chosen_sections_len &gt; MAX_SECTION_LEN:             break                      chosen_sections.append(SEPARATOR + document_section.content.replace(\"\\n\", \" \"))         chosen_sections_indexes.append(str(section_index))                  # Useful diagnostic information     print(f\"Selected {len(chosen_sections)} document sections:\")     print(\"\\n\".join(chosen_sections_indexes))          header = \"\"\"Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don't know.\"\\n\\nContext:\\n\"\"\"          return header + \"\".join(chosen_sections) + \"\\n\\n Q: \" + question + \"\\n A:\" In\u00a0[15]: Copied! <pre>prompt = construct_prompt(\n    \"Who won the 2020 Summer Olympics men's high jump?\",\n    document_embeddings,\n    df\n)\n\nprint(\"===\\n\", prompt)\n</pre> prompt = construct_prompt(     \"Who won the 2020 Summer Olympics men's high jump?\",     document_embeddings,     df )  print(\"===\\n\", prompt) <pre>Selected 2 document sections:\n(\"Athletics at the 2020 Summer Olympics \u2013 Men's high jump\", 'Summary')\n(\"Athletics at the 2020 Summer Olympics \u2013 Men's long jump\", 'Summary')\n===\n Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don't know.\"\n\nContext:\n\n* The men's high jump event at the 2020 Summer Olympics took place between 30 July and 1 August 2021 at the Olympic Stadium. 33 athletes from 24 nations competed; the total possible number depended on how many nations would use universality places to enter athletes in addition to the 32 qualifying through mark or ranking (no universality places were used in 2021). Italian athlete Gianmarco Tamberi along with Qatari athlete Mutaz Essa Barshim emerged as joint winners of the event following a tie between both of them as they cleared 2.37m. Both Tamberi and Barshim agreed to share the gold medal in a rare instance where the athletes of different nations had agreed to share the same medal in the history of Olympics. Barshim in particular was heard to ask a competition official \"Can we have two golds?\" in response to being offered a 'jump off'. Maksim Nedasekau of Belarus took bronze. The medals were the first ever in the men's high jump for Italy and Belarus, the first gold in the men's high jump for Italy and Qatar, and the third consecutive medal in the men's high jump for Qatar (all by Barshim). Barshim became only the second man to earn three medals in high jump, joining Patrik Sj\u00f6berg of Sweden (1984 to 1992).\n* The men's long jump event at the 2020 Summer Olympics took place between 31 July and 2 August 2021 at the Japan National Stadium. Approximately 35 athletes were expected to compete; the exact number was dependent on how many nations use universality places to enter athletes in addition to the 32 qualifying through time or ranking (1 universality place was used in 2016). 31 athletes from 20 nations competed. Miltiadis Tentoglou won the gold medal, Greece's first medal in the men's long jump. Cuban athletes Juan Miguel Echevarr\u00eda and Maykel Mass\u00f3 earned silver and bronze, respectively, the nation's first medals in the event since 2008.\n\n Q: Who won the 2020 Summer Olympics men's high jump?\n A:\n</pre> <p>We have now obtained the document sections that are most relevant to the question. As a final step, let's put it all together to get an answer to the question.</p> In\u00a0[16]: Copied! <pre>COMPLETIONS_API_PARAMS = {\n    # We use temperature of 0.0 because it gives the most predictable, factual answer.\n    \"temperature\": 0.0,\n    \"max_tokens\": 300,\n    \"model\": COMPLETIONS_MODEL,\n}\n</pre> COMPLETIONS_API_PARAMS = {     # We use temperature of 0.0 because it gives the most predictable, factual answer.     \"temperature\": 0.0,     \"max_tokens\": 300,     \"model\": COMPLETIONS_MODEL, } In\u00a0[17]: Copied! <pre>def answer_query_with_context(\n    query: str,\n    df: pd.DataFrame,\n    document_embeddings: dict[(str, str), np.array],\n    show_prompt: bool = False\n) -&gt; str:\n    prompt = construct_prompt(\n        query,\n        document_embeddings,\n        df\n    )\n    \n    if show_prompt:\n        print(prompt)\n\n    response = openai.Completion.create(\n                prompt=prompt,\n                **COMPLETIONS_API_PARAMS\n            )\n\n    return response[\"choices\"][0][\"text\"].strip(\" \\n\")\n</pre> def answer_query_with_context(     query: str,     df: pd.DataFrame,     document_embeddings: dict[(str, str), np.array],     show_prompt: bool = False ) -&gt; str:     prompt = construct_prompt(         query,         document_embeddings,         df     )          if show_prompt:         print(prompt)      response = openai.Completion.create(                 prompt=prompt,                 **COMPLETIONS_API_PARAMS             )      return response[\"choices\"][0][\"text\"].strip(\" \\n\") In\u00a0[18]: Copied! <pre>answer_query_with_context(\"Who won the 2020 Summer Olympics men's high jump?\", df, document_embeddings)\n</pre> answer_query_with_context(\"Who won the 2020 Summer Olympics men's high jump?\", df, document_embeddings) <pre>Selected 2 document sections:\n(\"Athletics at the 2020 Summer Olympics \u2013 Men's high jump\", 'Summary')\n(\"Athletics at the 2020 Summer Olympics \u2013 Men's long jump\", 'Summary')\n</pre> Out[18]: <pre>'Gianmarco Tamberi and Mutaz Essa Barshim emerged as joint winners of the event following a tie between both of them as they cleared 2.37m. Both Tamberi and Barshim agreed to share the gold medal.'</pre> <p>By combining the Embeddings and Completions APIs, we have created a question-answering model which can answer questions using a large base of additional knowledge. It also understands when it doesn't know the answer!</p> <p>For this example we have used a dataset of Wikipedia articles, but that dataset could be replaced with books, articles, documentation, service manuals, or much much more. We can't wait to see what you create with GPT-3!</p> In\u00a0[19]: Copied! <pre>query = \"Why was the 2020 Summer Olympics originally postponed?\"\nanswer = answer_query_with_context(query, df, document_embeddings)\n\nprint(f\"\\nQ: {query}\\nA: {answer}\")\n</pre> query = \"Why was the 2020 Summer Olympics originally postponed?\" answer = answer_query_with_context(query, df, document_embeddings)  print(f\"\\nQ: {query}\\nA: {answer}\") <pre>Selected 1 document sections:\n('Concerns and controversies at the 2020 Summer Olympics', 'Summary')\n\nQ: Why was the 2020 Summer Olympics originally postponed?\nA: The 2020 Summer Olympics were originally postponed due to the COVID-19 pandemic.\n</pre> In\u00a0[20]: Copied! <pre>query = \"In the 2020 Summer Olympics, how many gold medals did the country which won the most medals win?\"\nanswer = answer_query_with_context(query, df, document_embeddings)\n\nprint(f\"\\nQ: {query}\\nA: {answer}\")\n</pre> query = \"In the 2020 Summer Olympics, how many gold medals did the country which won the most medals win?\" answer = answer_query_with_context(query, df, document_embeddings)  print(f\"\\nQ: {query}\\nA: {answer}\") <pre>Selected 2 document sections:\n('2020 Summer Olympics medal table', 'Summary')\n('List of 2020 Summer Olympics medal winners', 'Summary')\n\nQ: In the 2020 Summer Olympics, how many gold medals did the country which won the most medals win?\nA: The United States won the most medals overall, with 113, and the most gold medals, with 39.\n</pre> In\u00a0[21]: Copied! <pre>query = \"What was unusual about the men\u2019s shotput competition?\"\nanswer = answer_query_with_context(query, df, document_embeddings)\n\nprint(f\"\\nQ: {query}\\nA: {answer}\")\n</pre> query = \"What was unusual about the men\u2019s shotput competition?\" answer = answer_query_with_context(query, df, document_embeddings)  print(f\"\\nQ: {query}\\nA: {answer}\") <pre>Selected 2 document sections:\n(\"Athletics at the 2020 Summer Olympics \u2013 Men's shot put\", 'Summary')\n(\"Athletics at the 2020 Summer Olympics \u2013 Men's discus throw\", 'Summary')\n\nQ: What was unusual about the men\u2019s shotput competition?\nA: The same three competitors received the same medals in back-to-back editions of the same individual event.\n</pre> In\u00a0[22]: Copied! <pre>query = \"In the 2020 Summer Olympics, how many silver medals did Italy win?\"\nanswer = answer_query_with_context(query, df, document_embeddings)\n\nprint(f\"\\nQ: {query}\\nA: {answer}\")\n</pre> query = \"In the 2020 Summer Olympics, how many silver medals did Italy win?\" answer = answer_query_with_context(query, df, document_embeddings)  print(f\"\\nQ: {query}\\nA: {answer}\") <pre>Selected 2 document sections:\n('Italy at the 2020 Summer Olympics', 'Summary')\n('San Marino at the 2020 Summer Olympics', 'Summary')\n\nQ: In the 2020 Summer Olympics, how many silver medals did Italy win?\nA: 10 silver medals.\n</pre> <p>Our Q&amp;A model is less prone to hallucinating answers, and has a better sense of what it does or doesn't know. This works when the information isn't contained in the context; when the question is nonsensical; or when the question is theoretically answerable but beyond GPT-3's powers!</p> In\u00a0[23]: Copied! <pre>query = \"What is the total number of medals won by France, multiplied by the number of Taekwondo medals given out to all countries?\"\nanswer = answer_query_with_context(query, df, document_embeddings)\n\nprint(f\"\\nQ: {query}\\nA: {answer}\")\n</pre> query = \"What is the total number of medals won by France, multiplied by the number of Taekwondo medals given out to all countries?\" answer = answer_query_with_context(query, df, document_embeddings)  print(f\"\\nQ: {query}\\nA: {answer}\") <pre>Selected 4 document sections:\n('France at the 2020 Summer Olympics', 'Taekwondo')\n('Taekwondo at the 2020 Summer Olympics \u2013 Qualification', 'Qualification summary')\n('2020 Summer Olympics medal table', 'Medal count')\n(\"Taekwondo at the 2020 Summer Olympics \u2013 Men's 80 kg\", 'Competition format')\n\nQ: What is the total number of medals won by France, multiplied by the number of Taekwondo medals given out to all countries?\nA: I don't know.\n</pre> In\u00a0[24]: Copied! <pre>query = \"What is the tallest mountain in the world?\"\nanswer = answer_query_with_context(query, df, document_embeddings)\n\nprint(f\"\\nQ: {query}\\nA: {answer}\")\n</pre> query = \"What is the tallest mountain in the world?\" answer = answer_query_with_context(query, df, document_embeddings)  print(f\"\\nQ: {query}\\nA: {answer}\") <pre>Selected 3 document sections:\n(\"Sport climbing at the 2020 Summer Olympics \u2013 Men's combined\", 'Route-setting')\n(\"Ski mountaineering at the 2020 Winter Youth Olympics \u2013 Boys' individual\", 'Summary')\n(\"Ski mountaineering at the 2020 Winter Youth Olympics \u2013 Girls' individual\", 'Summary')\n\nQ: What is the tallest mountain in the world?\nA: I don't know.\n</pre> In\u00a0[25]: Copied! <pre>query = \"Who won the grimblesplatch competition at the 2020 Summer Olympic games?\"\nanswer = answer_query_with_context(query, df, document_embeddings)\n\nprint(f\"\\nQ: {query}\\nA: {answer}\")\n</pre> query = \"Who won the grimblesplatch competition at the 2020 Summer Olympic games?\" answer = answer_query_with_context(query, df, document_embeddings)  print(f\"\\nQ: {query}\\nA: {answer}\") <pre>Selected 2 document sections:\n(\"Gymnastics at the 2020 Summer Olympics \u2013 Women's trampoline\", 'Summary')\n('Equestrian at the 2020 Summer Olympics \u2013 Team jumping', 'Summary')\n\nQ: Who won the grimblesplatch competition at the 2020 Summer Olympic games?\nA: I don't know.\n</pre>"},{"location":"examples/Question_answering_using_embeddings/#Question-Answering-using-Embeddings","title":"Question Answering using Embeddings\u00b6","text":"<p>Many use cases require GPT-3 to respond to user questions with insightful answers. For example, a customer support chatbot may need to provide answers to common questions. The GPT models have picked up a lot of general knowledge in training, but we often need to ingest and use a large library of more specific information.</p> <p>In this notebook we will demonstrate a method for enabling GPT-3 to answer questions using a library of text as a reference, by using document embeddings and retrieval. We'll be using a dataset of Wikipedia articles about the 2020 Summer Olympic Games. Please see this notebook to follow the data gathering process.</p>"},{"location":"examples/Question_answering_using_embeddings/#0)-Preventing-hallucination-with-prompt-engineering","title":"0) Preventing hallucination with prompt engineering\u00b6","text":"<p>We can address this hallucination issue by being more explicit with our prompt:</p>"},{"location":"examples/Question_answering_using_embeddings/#1)-Preprocess-the-document-library","title":"1) Preprocess the document library\u00b6","text":"<p>We plan to use document embeddings to fetch the most relevant part of parts of our document library and insert them into the prompt that we provide to GPT-3. We therefore need to break up the document library into \"sections\" of context, which can be searched and retrieved separately.</p> <p>Sections should be large enough to contain enough information to answer a question; but small enough to fit one or several into the GPT-3 prompt. We find that approximately a paragraph of text is usually a good length, but you should experiment for your particular use case. In this example, Wikipedia articles are already grouped into semantically related headers, so we will use these to define our sections. This preprocessing has already been done in this notebook, so we will load the results and use them.</p>"},{"location":"examples/Question_answering_using_embeddings/#2)-Find-the-most-similar-document-embeddings-to-the-question-embedding","title":"2) Find the most similar document embeddings to the question embedding\u00b6","text":"<p>At the time of question-answering, to answer the user's query we compute the query embedding of the question and use it to find the most similar document sections. Since this is a small example, we store and search the embeddings locally. If you have a larger dataset, consider using a vector search engine like Pinecone, Weaviate or Qdrant to power the search.</p>"},{"location":"examples/Question_answering_using_embeddings/#3)-Add-the-most-relevant-document-sections-to-the-query-prompt","title":"3) Add the most relevant document sections to the query prompt\u00b6","text":"<p>Once we've calculated the most relevant pieces of context, we construct a prompt by simply prepending them to the supplied query. It is helpful to use a query separator to help the model distinguish between separate pieces of text.</p>"},{"location":"examples/Question_answering_using_embeddings/#4)-Answer-the-user's-question-based-on-the-context.","title":"4) Answer the user's question based on the context.\u00b6","text":"<p>Now that we've retrieved the relevant context and constructed our prompt, we can finally use the Completions API to answer the user's query.</p>"},{"location":"examples/Question_answering_using_embeddings/#More-Examples","title":"More Examples\u00b6","text":"<p>Let's have some fun and try some more examples.</p>"},{"location":"examples/Recommendation_using_embeddings/","title":"Recommendation using embeddings and nearest neighbor search","text":"In\u00a0[1]: Copied! <pre># imports\nimport pandas as pd\nimport pickle\n\nfrom openai.embeddings_utils import (\n    get_embedding,\n    distances_from_embeddings,\n    tsne_components_from_embeddings,\n    chart_from_components,\n    indices_of_nearest_neighbors_from_distances,\n)\n\n# constants\nEMBEDDING_MODEL = \"text-embedding-ada-002\"\n</pre> # imports import pandas as pd import pickle  from openai.embeddings_utils import (     get_embedding,     distances_from_embeddings,     tsne_components_from_embeddings,     chart_from_components,     indices_of_nearest_neighbors_from_distances, )  # constants EMBEDDING_MODEL = \"text-embedding-ada-002\" In\u00a0[2]: Copied! <pre># load data (full dataset available at http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html)\ndataset_path = \"data/AG_news_samples.csv\"\ndf = pd.read_csv(dataset_path)\n\n# print dataframe\nn_examples = 5\ndf.head(n_examples)\n</pre> # load data (full dataset available at http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) dataset_path = \"data/AG_news_samples.csv\" df = pd.read_csv(dataset_path)  # print dataframe n_examples = 5 df.head(n_examples) Out[2]: title description label_int label 0 World Briefings BRITAIN: BLAIR WARNS OF CLIMATE THREAT Prime M... 1 World 1 Nvidia Puts a Firewall on a Motherboard (PC Wo... PC World - Upcoming chip set will include buil... 4 Sci/Tech 2 Olympic joy in Greek, Chinese press Newspapers in Greece reflect a mixture of exhi... 2 Sports 3 U2 Can iPod with Pictures SAN JOSE, Calif. -- Apple Computer (Quote, Cha... 4 Sci/Tech 4 The Dream Factory Any product, any shape, any size -- manufactur... 4 Sci/Tech <p>Let's take a look at those same examples, but not truncated by ellipses.</p> In\u00a0[3]: Copied! <pre># print the title, description, and label of each example\nfor idx, row in df.head(n_examples).iterrows():\n    print(\"\")\n    print(f\"Title: {row['title']}\")\n    print(f\"Description: {row['description']}\")\n    print(f\"Label: {row['label']}\")\n</pre> # print the title, description, and label of each example for idx, row in df.head(n_examples).iterrows():     print(\"\")     print(f\"Title: {row['title']}\")     print(f\"Description: {row['description']}\")     print(f\"Label: {row['label']}\") <pre>\nTitle: World Briefings\nDescription: BRITAIN: BLAIR WARNS OF CLIMATE THREAT Prime Minister Tony Blair urged the international community to consider global warming a dire threat and agree on a plan of action to curb the  quot;alarming quot; growth of greenhouse gases.\nLabel: World\n\nTitle: Nvidia Puts a Firewall on a Motherboard (PC World)\nDescription: PC World - Upcoming chip set will include built-in security features for your PC.\nLabel: Sci/Tech\n\nTitle: Olympic joy in Greek, Chinese press\nDescription: Newspapers in Greece reflect a mixture of exhilaration that the Athens Olympics proved successful, and relief that they passed off without any major setback.\nLabel: Sports\n\nTitle: U2 Can iPod with Pictures\nDescription: SAN JOSE, Calif. -- Apple Computer (Quote, Chart) unveiled a batch of new iPods, iTunes software and promos designed to keep it atop the heap of digital music players.\nLabel: Sci/Tech\n\nTitle: The Dream Factory\nDescription: Any product, any shape, any size -- manufactured on your desktop! The future is the fabricator. By Bruce Sterling from Wired magazine.\nLabel: Sci/Tech\n</pre> In\u00a0[4]: Copied! <pre># establish a cache of embeddings to avoid recomputing\n# cache is a dict of tuples (text, model) -&gt; embedding, saved as a pickle file\n\n# set path to embedding cache\nembedding_cache_path = \"data/recommendations_embeddings_cache.pkl\"\n\n# load the cache if it exists, and save a copy to disk\ntry:\n    embedding_cache = pd.read_pickle(embedding_cache_path)\nexcept FileNotFoundError:\n    embedding_cache = {}\nwith open(embedding_cache_path, \"wb\") as embedding_cache_file:\n    pickle.dump(embedding_cache, embedding_cache_file)\n\n# define a function to retrieve embeddings from the cache if present, and otherwise request via the API\ndef embedding_from_string(\n    string: str,\n    model: str = EMBEDDING_MODEL,\n    embedding_cache=embedding_cache\n) -&gt; list:\n\"\"\"Return embedding of given string, using a cache to avoid recomputing.\"\"\"\n    if (string, model) not in embedding_cache.keys():\n        embedding_cache[(string, model)] = get_embedding(string, model)\n        with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n            pickle.dump(embedding_cache, embedding_cache_file)\n    return embedding_cache[(string, model)]\n</pre> # establish a cache of embeddings to avoid recomputing # cache is a dict of tuples (text, model) -&gt; embedding, saved as a pickle file  # set path to embedding cache embedding_cache_path = \"data/recommendations_embeddings_cache.pkl\"  # load the cache if it exists, and save a copy to disk try:     embedding_cache = pd.read_pickle(embedding_cache_path) except FileNotFoundError:     embedding_cache = {} with open(embedding_cache_path, \"wb\") as embedding_cache_file:     pickle.dump(embedding_cache, embedding_cache_file)  # define a function to retrieve embeddings from the cache if present, and otherwise request via the API def embedding_from_string(     string: str,     model: str = EMBEDDING_MODEL,     embedding_cache=embedding_cache ) -&gt; list:     \"\"\"Return embedding of given string, using a cache to avoid recomputing.\"\"\"     if (string, model) not in embedding_cache.keys():         embedding_cache[(string, model)] = get_embedding(string, model)         with open(embedding_cache_path, \"wb\") as embedding_cache_file:             pickle.dump(embedding_cache, embedding_cache_file)     return embedding_cache[(string, model)] <p>Let's check that it works by getting an embedding.</p> In\u00a0[5]: Copied! <pre># as an example, take the first description from the dataset\nexample_string = df[\"description\"].values[0]\nprint(f\"\\nExample string: {example_string}\")\n\n# print the first 10 dimensions of the embedding\nexample_embedding = embedding_from_string(example_string)\nprint(f\"\\nExample embedding: {example_embedding[:10]}...\")\n</pre> # as an example, take the first description from the dataset example_string = df[\"description\"].values[0] print(f\"\\nExample string: {example_string}\")  # print the first 10 dimensions of the embedding example_embedding = embedding_from_string(example_string) print(f\"\\nExample embedding: {example_embedding[:10]}...\") <pre>\nExample string: BRITAIN: BLAIR WARNS OF CLIMATE THREAT Prime Minister Tony Blair urged the international community to consider global warming a dire threat and agree on a plan of action to curb the  quot;alarming quot; growth of greenhouse gases.\n\nExample embedding: [-0.01071077398955822, -0.022362446412444115, -0.00883542187511921, -0.0254171434789896, 0.031423427164554596, 0.010723662562668324, -0.016717055812478065, 0.004195375367999077, -0.008074969984591007, -0.02142154797911644]...\n</pre> In\u00a0[6]: Copied! <pre>def print_recommendations_from_strings(\n    strings: list[str],\n    index_of_source_string: int,\n    k_nearest_neighbors: int = 1,\n    model=EMBEDDING_MODEL,\n) -&gt; list[int]:\n\"\"\"Print out the k nearest neighbors of a given string.\"\"\"\n    # get embeddings for all strings\n    embeddings = [embedding_from_string(string, model=model) for string in strings]\n    # get the embedding of the source string\n    query_embedding = embeddings[index_of_source_string]\n    # get distances between the source embedding and other embeddings (function from embeddings_utils.py)\n    distances = distances_from_embeddings(query_embedding, embeddings, distance_metric=\"cosine\")\n    # get indices of nearest neighbors (function from embeddings_utils.py)\n    indices_of_nearest_neighbors = indices_of_nearest_neighbors_from_distances(distances)\n\n    # print out source string\n    query_string = strings[index_of_source_string]\n    print(f\"Source string: {query_string}\")\n    # print out its k nearest neighbors\n    k_counter = 0\n    for i in indices_of_nearest_neighbors:\n        # skip any strings that are identical matches to the starting string\n        if query_string == strings[i]:\n            continue\n        # stop after printing out k articles\n        if k_counter &gt;= k_nearest_neighbors:\n            break\n        k_counter += 1\n\n        # print out the similar strings and their distances\n        print(\n            f\"\"\"\n        --- Recommendation #{k_counter} (nearest neighbor {k_counter} of {k_nearest_neighbors}) ---\n        String: {strings[i]}\n        Distance: {distances[i]:0.3f}\"\"\"\n        )\n\n    return indices_of_nearest_neighbors\n</pre> def print_recommendations_from_strings(     strings: list[str],     index_of_source_string: int,     k_nearest_neighbors: int = 1,     model=EMBEDDING_MODEL, ) -&gt; list[int]:     \"\"\"Print out the k nearest neighbors of a given string.\"\"\"     # get embeddings for all strings     embeddings = [embedding_from_string(string, model=model) for string in strings]     # get the embedding of the source string     query_embedding = embeddings[index_of_source_string]     # get distances between the source embedding and other embeddings (function from embeddings_utils.py)     distances = distances_from_embeddings(query_embedding, embeddings, distance_metric=\"cosine\")     # get indices of nearest neighbors (function from embeddings_utils.py)     indices_of_nearest_neighbors = indices_of_nearest_neighbors_from_distances(distances)      # print out source string     query_string = strings[index_of_source_string]     print(f\"Source string: {query_string}\")     # print out its k nearest neighbors     k_counter = 0     for i in indices_of_nearest_neighbors:         # skip any strings that are identical matches to the starting string         if query_string == strings[i]:             continue         # stop after printing out k articles         if k_counter &gt;= k_nearest_neighbors:             break         k_counter += 1          # print out the similar strings and their distances         print(             f\"\"\"         --- Recommendation #{k_counter} (nearest neighbor {k_counter} of {k_nearest_neighbors}) ---         String: {strings[i]}         Distance: {distances[i]:0.3f}\"\"\"         )      return indices_of_nearest_neighbors In\u00a0[7]: Copied! <pre>article_descriptions = df[\"description\"].tolist()\n\ntony_blair_articles = print_recommendations_from_strings(\n    strings=article_descriptions,  # let's base similarity off of the article description\n    index_of_source_string=0,  # let's look at articles similar to the first one about Tony Blair\n    k_nearest_neighbors=5,  # let's look at the 5 most similar articles\n)\n</pre> article_descriptions = df[\"description\"].tolist()  tony_blair_articles = print_recommendations_from_strings(     strings=article_descriptions,  # let's base similarity off of the article description     index_of_source_string=0,  # let's look at articles similar to the first one about Tony Blair     k_nearest_neighbors=5,  # let's look at the 5 most similar articles ) <pre>Source string: BRITAIN: BLAIR WARNS OF CLIMATE THREAT Prime Minister Tony Blair urged the international community to consider global warming a dire threat and agree on a plan of action to curb the  quot;alarming quot; growth of greenhouse gases.\n\n        --- Recommendation #1 (nearest neighbor 1 of 5) ---\n        String: THE re-election of British Prime Minister Tony Blair would be seen as an endorsement of the military action in Iraq, Prime Minister John Howard said today.\n        Distance: 0.153\n\n        --- Recommendation #2 (nearest neighbor 2 of 5) ---\n        String: LONDON, England -- A US scientist is reported to have observed a surprising jump in the amount of carbon dioxide, the main greenhouse gas.\n        Distance: 0.160\n\n        --- Recommendation #3 (nearest neighbor 3 of 5) ---\n        String: The anguish of hostage Kenneth Bigley in Iraq hangs over Prime Minister Tony Blair today as he faces the twin test of a local election and a debate by his Labour Party about the divisive war.\n        Distance: 0.160\n\n        --- Recommendation #4 (nearest neighbor 4 of 5) ---\n        String: Israel is prepared to back a Middle East conference convened by Tony Blair early next year despite having expressed fears that the British plans were over-ambitious and designed \n        Distance: 0.171\n\n        --- Recommendation #5 (nearest neighbor 5 of 5) ---\n        String: AFP - A battle group of British troops rolled out of southern Iraq on a US-requested mission to deadlier areas near Baghdad, in a major political gamble for British Prime Minister Tony Blair.\n        Distance: 0.173\n</pre> <p>Pretty good! 4 of the 5 recommendations explicitly mention Tony Blair and the fifth is an article from London about climate change, topics that might be often associated with Tony Blair.</p> <p>Let's see how our recommender does on the second example article about NVIDIA's new chipset with more security.</p> In\u00a0[8]: Copied! <pre>chipset_security_articles = print_recommendations_from_strings(\n    strings=article_descriptions,  # let's base similarity off of the article description\n    index_of_source_string=1,  # let's look at articles similar to the second one about a more secure chipset\n    k_nearest_neighbors=5,  # let's look at the 5 most similar articles\n)\n</pre> chipset_security_articles = print_recommendations_from_strings(     strings=article_descriptions,  # let's base similarity off of the article description     index_of_source_string=1,  # let's look at articles similar to the second one about a more secure chipset     k_nearest_neighbors=5,  # let's look at the 5 most similar articles ) <pre>Source string: PC World - Upcoming chip set will include built-in security features for your PC.\n\n        --- Recommendation #1 (nearest neighbor 1 of 5) ---\n        String: PC World - Updated antivirus software for businesses adds intrusion prevention features.\n        Distance: 0.112\n\n        --- Recommendation #2 (nearest neighbor 2 of 5) ---\n        String: PC World - The one-time World Class Product of the Year PDA gets a much-needed upgrade.\n        Distance: 0.145\n\n        --- Recommendation #3 (nearest neighbor 3 of 5) ---\n        String: PC World - Send your video throughout your house--wirelessly--with new gateways and media adapters.\n        Distance: 0.153\n\n        --- Recommendation #4 (nearest neighbor 4 of 5) ---\n        String: PC World - Symantec, McAfee hope raising virus-definition fees will move users to\\  suites.\n        Distance: 0.157\n\n        --- Recommendation #5 (nearest neighbor 5 of 5) ---\n        String: Gateway computers will be more widely available at Office Depot, in the PC maker #39;s latest move to broaden distribution at retail stores since acquiring rival eMachines this year. \n        Distance: 0.168\n</pre> <p>From the printed distances, you can see that the #1 recommendation is much closer than all the others (0.11 vs 0.14+). And the #1 recommendation looks very similar to the starting article - it's another article from PC World about increasing computer security. Pretty good!</p> <p>To get a sense of what our nearest neighbor recommender is doing, let's visualize the article embeddings. Although we can't plot the 2048 dimensions of each embedding vector, we can use techniques like t-SNE or PCA to compress the embeddings down into 2 or 3 dimensions, which we can chart.</p> <p>Before visualizing the nearest neighbors, let's visualize all of the article descriptions using t-SNE. Note that t-SNE is not deterministic, meaning that results may vary from run to run.</p> In\u00a0[9]: Copied! <pre># get embeddings for all article descriptions\nembeddings = [embedding_from_string(string) for string in article_descriptions]\n# compress the 2048-dimensional embeddings into 2 dimensions using t-SNE\ntsne_components = tsne_components_from_embeddings(embeddings)\n# get the article labels for coloring the chart\nlabels = df[\"label\"].tolist()\n\nchart_from_components(\n    components=tsne_components,\n    labels=labels,\n    strings=article_descriptions,\n    width=600,\n    height=500,\n    title=\"t-SNE components of article descriptions\",\n)\n</pre> # get embeddings for all article descriptions embeddings = [embedding_from_string(string) for string in article_descriptions] # compress the 2048-dimensional embeddings into 2 dimensions using t-SNE tsne_components = tsne_components_from_embeddings(embeddings) # get the article labels for coloring the chart labels = df[\"label\"].tolist()  chart_from_components(     components=tsne_components,     labels=labels,     strings=article_descriptions,     width=600,     height=500,     title=\"t-SNE components of article descriptions\", ) <p>As you can see in the chart above, even the highly compressed embeddings do a good job of clustering article descriptions by category. And it's worth emphasizing: this clustering is done with no knowledge of the labels themselves!</p> <p>Also, if you look closely at the most egregious outliers, they are often due to mislabeling rather than poor embedding. For example, the majority of the blue World points in the green Sports cluster appear to be Sports stories.</p> <p>Next, let's recolor the points by whether they are a source article, its nearest neighbors, or other.</p> In\u00a0[10]: Copied! <pre># create labels for the recommended articles\ndef nearest_neighbor_labels(\n    list_of_indices: list[int],\n    k_nearest_neighbors: int = 5\n) -&gt; list[str]:\n\"\"\"Return a list of labels to color the k nearest neighbors.\"\"\"\n    labels = [\"Other\" for _ in list_of_indices]\n    source_index = list_of_indices[0]\n    labels[source_index] = \"Source\"\n    for i in range(k_nearest_neighbors):\n        nearest_neighbor_index = list_of_indices[i + 1]\n        labels[nearest_neighbor_index] = f\"Nearest neighbor (top {k_nearest_neighbors})\"\n    return labels\n\n\ntony_blair_labels = nearest_neighbor_labels(tony_blair_articles, k_nearest_neighbors=5)\nchipset_security_labels = nearest_neighbor_labels(chipset_security_articles, k_nearest_neighbors=5\n)\n</pre> # create labels for the recommended articles def nearest_neighbor_labels(     list_of_indices: list[int],     k_nearest_neighbors: int = 5 ) -&gt; list[str]:     \"\"\"Return a list of labels to color the k nearest neighbors.\"\"\"     labels = [\"Other\" for _ in list_of_indices]     source_index = list_of_indices[0]     labels[source_index] = \"Source\"     for i in range(k_nearest_neighbors):         nearest_neighbor_index = list_of_indices[i + 1]         labels[nearest_neighbor_index] = f\"Nearest neighbor (top {k_nearest_neighbors})\"     return labels   tony_blair_labels = nearest_neighbor_labels(tony_blair_articles, k_nearest_neighbors=5) chipset_security_labels = nearest_neighbor_labels(chipset_security_articles, k_nearest_neighbors=5 ) In\u00a0[11]: Copied! <pre># a 2D chart of nearest neighbors of the Tony Blair article\nchart_from_components(\n    components=tsne_components,\n    labels=tony_blair_labels,\n    strings=article_descriptions,\n    width=600,\n    height=500,\n    title=\"Nearest neighbors of the Tony Blair article\",\n    category_orders={\"label\": [\"Other\", \"Nearest neighbor (top 5)\", \"Source\"]},\n)\n</pre> # a 2D chart of nearest neighbors of the Tony Blair article chart_from_components(     components=tsne_components,     labels=tony_blair_labels,     strings=article_descriptions,     width=600,     height=500,     title=\"Nearest neighbors of the Tony Blair article\",     category_orders={\"label\": [\"Other\", \"Nearest neighbor (top 5)\", \"Source\"]}, ) <p>Looking at the 2D chart above, we can see that the articles about Tony Blair are somewhat close together inside of the World news cluster. Interestingly, although the 5 nearest neighbors (red) were closest in high dimensional space, they are not the closest points in this compressed 2D space. Compressing the embeddings down to 2 dimensions discards much of their information, and the nearest neighbors in the 2D space don't seem to be as relevant as those in the full embedding space.</p> In\u00a0[12]: Copied! <pre># a 2D chart of nearest neighbors of the chipset security article\nchart_from_components(\n    components=tsne_components,\n    labels=chipset_security_labels,\n    strings=article_descriptions,\n    width=600,\n    height=500,\n    title=\"Nearest neighbors of the chipset security article\",\n    category_orders={\"label\": [\"Other\", \"Nearest neighbor (top 5)\", \"Source\"]},\n)\n</pre> # a 2D chart of nearest neighbors of the chipset security article chart_from_components(     components=tsne_components,     labels=chipset_security_labels,     strings=article_descriptions,     width=600,     height=500,     title=\"Nearest neighbors of the chipset security article\",     category_orders={\"label\": [\"Other\", \"Nearest neighbor (top 5)\", \"Source\"]}, ) <p>For the chipset security example, the 4 closest nearest neighbors in the full embedding space remain nearest neighbors in this compressed 2D visualization. The fifth is displayed as more distant, despite being closer in the full embedding space.</p> <p>Should you want to, you can also make an interactive 3D plot of the embeddings with the function <code>chart_from_components_3D</code>. (Doing so will require recomputing the t-SNE components with <code>n_components=3</code>.)</p>"},{"location":"examples/Recommendation_using_embeddings/#Recommendation-using-embeddings-and-nearest-neighbor-search","title":"Recommendation using embeddings and nearest neighbor search\u00b6","text":"<p>Recommendations are widespread across the web.</p> <ul> <li>'Bought that item? Try these similar items.'</li> <li>'Enjoy that book? Try these similar titles.'</li> <li>'Not the help page you were looking for? Try these similar pages.'</li> </ul> <p>This notebook demonstrates how to use embeddings to find similar items to recommend. In particular, we use AG's corpus of news articles as our dataset.</p> <p>Our model will answer the question: given an article, what other articles are most similar to it?</p>"},{"location":"examples/Recommendation_using_embeddings/#1.-Imports","title":"1. Imports\u00b6","text":"<p>First, let's import the packages and functions we'll need for later. If you don't have these, you'll need to install them. You can install them via your terminal by running <code>pip install {package_name}</code>, e.g. <code>pip install pandas</code>.</p>"},{"location":"examples/Recommendation_using_embeddings/#2.-Load-data","title":"2. Load data\u00b6","text":"<p>Next, let's load the AG news data and see what it looks like.</p>"},{"location":"examples/Recommendation_using_embeddings/#3.-Build-cache-to-save-embeddings","title":"3. Build cache to save embeddings\u00b6","text":"<p>Before getting embeddings for these articles, let's set up a cache to save the embeddings we generate. In general, it's a good idea to save your embeddings so you can re-use them later. If you don't save them, you'll pay again each time you compute them again.</p> <p>The cache is a dictionary that maps tuples of <code>(text, model)</code> to an embedding, which is a list of floats. The cache is saved as a Python pickle file.</p>"},{"location":"examples/Recommendation_using_embeddings/#4.-Recommend-similar-articles-based-on-embeddings","title":"4. Recommend similar articles based on embeddings\u00b6","text":"<p>To find similar articles, let's follow a three-step plan:</p> <ol> <li>Get the similarity embeddings of all the article descriptions</li> <li>Calculate the distance between a source title and all other articles</li> <li>Print out the other articles closest to the source title</li> </ol>"},{"location":"examples/Recommendation_using_embeddings/#5.-Example-recommendations","title":"5. Example recommendations\u00b6","text":"<p>Let's look for articles similar to first one, which was about Tony Blair.</p>"},{"location":"examples/Recommendation_using_embeddings/#Appendix:-Using-embeddings-in-more-sophisticated-recommenders","title":"Appendix: Using embeddings in more sophisticated recommenders\u00b6","text":"<p>A more sophisticated way to build a recommender system is to train a machine learning model that takes in tens or hundreds of signals, such as item popularity or user click data. Even in this system, embeddings can be a very useful signal into the recommender, especially for items that are being 'cold started' with no user data yet (e.g., a brand new product added to the catalog without any clicks yet).</p>"},{"location":"examples/Recommendation_using_embeddings/#Appendix:-Using-embeddings-to-visualize-similar-articles","title":"Appendix: Using embeddings to visualize similar articles\u00b6","text":""},{"location":"examples/Regression_using_embeddings/","title":"Regression using embeddings","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\ndatafile_path = \"data/fine_food_reviews_with_embeddings_1k.csv\"\n\ndf = pd.read_csv(datafile_path)\ndf[\"embedding\"] = df.embedding.apply(eval).apply(np.array)\n\nX_train, X_test, y_train, y_test = train_test_split(list(df.embedding.values), df.Score, test_size=0.2, random_state=42)\n\nrfr = RandomForestRegressor(n_estimators=100)\nrfr.fit(X_train, y_train)\npreds = rfr.predict(X_test)\n\nmse = mean_squared_error(y_test, preds)\nmae = mean_absolute_error(y_test, preds)\n\nprint(f\"ada-002 embedding performance on 1k Amazon reviews: mse={mse:.2f}, mae={mae:.2f}\")\n</pre> import pandas as pd import numpy as np  from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error, mean_absolute_error  datafile_path = \"data/fine_food_reviews_with_embeddings_1k.csv\"  df = pd.read_csv(datafile_path) df[\"embedding\"] = df.embedding.apply(eval).apply(np.array)  X_train, X_test, y_train, y_test = train_test_split(list(df.embedding.values), df.Score, test_size=0.2, random_state=42)  rfr = RandomForestRegressor(n_estimators=100) rfr.fit(X_train, y_train) preds = rfr.predict(X_test)  mse = mean_squared_error(y_test, preds) mae = mean_absolute_error(y_test, preds)  print(f\"ada-002 embedding performance on 1k Amazon reviews: mse={mse:.2f}, mae={mae:.2f}\")  <pre>ada-002 embedding performance on 1k Amazon reviews: mse=0.62, mae=0.53\n</pre> In\u00a0[2]: Copied! <pre>bmse = mean_squared_error(y_test, np.repeat(y_test.mean(), len(y_test)))\nbmae = mean_absolute_error(y_test, np.repeat(y_test.mean(), len(y_test)))\nprint(\n    f\"Dummy mean prediction performance on Amazon reviews: mse={bmse:.2f}, mae={bmae:.2f}\"\n)\n</pre> bmse = mean_squared_error(y_test, np.repeat(y_test.mean(), len(y_test))) bmae = mean_absolute_error(y_test, np.repeat(y_test.mean(), len(y_test))) print(     f\"Dummy mean prediction performance on Amazon reviews: mse={bmse:.2f}, mae={bmae:.2f}\" )  <pre>Dummy mean prediction performance on Amazon reviews: mse=1.73, mae=1.03\n</pre> <p>We can see that the embeddings are able to predict the scores with an average error of 0.53 per score prediction. This is roughly equivalent to predicting half of reviews perfectly, and half off by one star.</p> <p>You could also train a classifier to predict the label, or use the embeddings within an existing ML model to encode free text features.</p>"},{"location":"examples/Regression_using_embeddings/#Regression-using-the-embeddings","title":"Regression using the embeddings\u00b6","text":"<p>Regression means predicting a number, rather than one of the categories. We will predict the score based on the embedding of the review's text. We split the dataset into a training and a testing set for all of the following tasks, so we can realistically evaluate performance on unseen data. The dataset is created in the Obtain_dataset Notebook.</p> <p>We're predicting the score of the review, which is a number between 1 and 5 (1-star being negative and 5-star positive).</p>"},{"location":"examples/Semantic_text_search_using_embeddings/","title":"Semantic text search using embeddings","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\n\ndatafile_path = \"data/fine_food_reviews_with_embeddings_1k.csv\"\n\ndf = pd.read_csv(datafile_path)\ndf[\"embedding\"] = df.embedding.apply(eval).apply(np.array)\n</pre> import pandas as pd import numpy as np  datafile_path = \"data/fine_food_reviews_with_embeddings_1k.csv\"  df = pd.read_csv(datafile_path) df[\"embedding\"] = df.embedding.apply(eval).apply(np.array)  <p>Remember to use the documents embedding engine for documents (in this case reviews), and query embedding engine for queries. Note that here we just compare the cosine similarity of the embeddings of the query and the documents, and show top_n best matches.</p> In\u00a0[2]: Copied! <pre>from openai.embeddings_utils import get_embedding, cosine_similarity\n\n# search through the reviews for a specific product\ndef search_reviews(df, product_description, n=3, pprint=True):\n    product_embedding = get_embedding(\n        product_description,\n        engine=\"text-embedding-ada-002\"\n    )\n    df[\"similarity\"] = df.embedding.apply(lambda x: cosine_similarity(x, product_embedding))\n\n    results = (\n        df.sort_values(\"similarity\", ascending=False)\n        .head(n)\n        .combined.str.replace(\"Title: \", \"\")\n        .str.replace(\"; Content:\", \": \")\n    )\n    if pprint:\n        for r in results:\n            print(r[:200])\n            print()\n    return results\n\n\nresults = search_reviews(df, \"delicious beans\", n=3)\n</pre> from openai.embeddings_utils import get_embedding, cosine_similarity  # search through the reviews for a specific product def search_reviews(df, product_description, n=3, pprint=True):     product_embedding = get_embedding(         product_description,         engine=\"text-embedding-ada-002\"     )     df[\"similarity\"] = df.embedding.apply(lambda x: cosine_similarity(x, product_embedding))      results = (         df.sort_values(\"similarity\", ascending=False)         .head(n)         .combined.str.replace(\"Title: \", \"\")         .str.replace(\"; Content:\", \": \")     )     if pprint:         for r in results:             print(r[:200])             print()     return results   results = search_reviews(df, \"delicious beans\", n=3)  <pre>Good Buy:  I liked the beans. They were vacuum sealed, plump and moist. Would recommend them for any use. I personally split and stuck them in some vodka to make vanilla extract. Yum!\n\nJamaican Blue beans:  Excellent coffee bean for roasting. Our family just purchased another 5 pounds for more roasting. Plenty of flavor and mild on acidity when roasted to a dark brown bean and befor\n\nDelicious!:  I enjoy this white beans seasoning, it gives a rich flavor to the beans I just love it, my mother in law didn't know about this Zatarain's brand and now she is traying different seasoning\n\n</pre> In\u00a0[3]: Copied! <pre>results = search_reviews(df, \"whole wheat pasta\", n=3)\n</pre> results = search_reviews(df, \"whole wheat pasta\", n=3) <pre>Tasty and Quick Pasta:  Barilla Whole Grain Fusilli with Vegetable Marinara is tasty and has an excellent chunky vegetable marinara.  I just wish there was more of it.  If you aren't starving or on a \n\nsooo good:  tastes so good. Worth the money. My boyfriend hates wheat pasta and LOVES this. cooks fast tastes great.I love this brand and started buying more of their pastas. Bulk is best.\n\nHandy:  Love the idea of ready in a minute pasta and for that alone this product gets praise.  The pasta is whole grain so that's a big plus and it actually comes out al dente.  The vegetable marinara\n\n</pre> <p>We can search through these reviews easily. To speed up computation, we can use a special algorithm, aimed at faster search through embeddings.</p> In\u00a0[4]: Copied! <pre>results = search_reviews(df, \"bad delivery\", n=1)\n</pre> results = search_reviews(df, \"bad delivery\", n=1) <pre>great product, poor delivery:  The coffee is excellent and I am a repeat buyer.  Problem this time was with the UPS delivery.  They left the box in front of my garage door in the middle of the drivewa\n\n</pre> <p>As we can see, this can immediately deliver a lot of value. In this example we show being able to quickly find the examples of delivery failures.</p> In\u00a0[5]: Copied! <pre>results = search_reviews(df, \"spoilt\", n=1)\n</pre> results = search_reviews(df, \"spoilt\", n=1) <pre>Extremely dissapointed:  Hi,&lt;br /&gt;I am very disappointed with the past shipment I received of the ONE coconut water. 3 of the boxes were leaking and the coconut water was spoiled.&lt;br /&gt;&lt;br /&gt;Thanks.&lt;b\n\n</pre> In\u00a0[6]: Copied! <pre>results = search_reviews(df, \"pet food\", n=2)\n</pre> results = search_reviews(df, \"pet food\", n=2) <pre>Good food:  The only dry food my queen cat will eat. Helps prevent hair balls. Good packaging. Arrives promptly. Recommended by a friend who sells pet food.\n\nThe cats like it:  My 7 cats like this food but it is a little yucky for the human. Pieces of mackerel swimming in a dark broth. It is billed as a \"complete\" food and contains carrots, peas and pasta.\n\n</pre>"},{"location":"examples/Semantic_text_search_using_embeddings/#Semantic-text-search-using-embeddings","title":"Semantic text search using embeddings\u00b6","text":"<p>We can search through all our reviews semantically in a very efficient manner and at very low cost, by simply embedding our search query, and then finding the most similar reviews. The dataset is created in the Obtain_dataset Notebook.</p>"},{"location":"examples/Unit_test_writing_using_a_multi-step_prompt/","title":"Unit test writing using a multi-step prompt","text":"In\u00a0[1]: Copied! <pre># imports needed to run the code in this notebook\nimport ast  # used for detecting whether generated Python code is valid\nimport openai  # used for calling the OpenAI API\n\n# example of a function that uses a multi-step prompt to write unit tests\ndef unit_test_from_function(\n    function_to_test: str,  # Python function to test, as a string\n    unit_test_package: str = \"pytest\",  # unit testing package; use the name as it appears in the import statement\n    approx_min_cases_to_cover: int = 7,  # minimum number of test case categories to cover (approximate)\n    print_text: bool = False,  # optionally prints text; helpful for understanding the function &amp; debugging\n    text_model: str = \"text-davinci-002\",  # model used to generate text plans in steps 1, 2, and 2b\n    code_model: str = \"code-davinci-002\",  # if you don't have access to code models, you can use text models here instead\n    max_tokens: int = 1000,  # can set this high, as generations should be stopped earlier by stop sequences\n    temperature: float = 0.4,  # temperature = 0 can sometimes get stuck in repetitive loops, so we use 0.4\n    reruns_if_fail: int = 1,  # if the output code cannot be parsed, this will re-run the function up to N times\n) -&gt; str:\n\"\"\"Outputs a unit test for a given Python function, using a 3-step GPT-3 prompt.\"\"\"\n\n    # Step 1: Generate an explanation of the function\n\n    # create a markdown-formatted prompt that asks GPT-3 to complete an explanation of the function, formatted as a bullet list\n    prompt_to_explain_the_function = f\"\"\"# How to write great unit tests with {unit_test_package}\n\nIn this advanced tutorial for experts, we'll use Python 3.9 and `{unit_test_package}` to write a suite of unit tests to verify the behavior of the following function.\n```python\n{function_to_test}\n```\n\nBefore writing any unit tests, let's review what each element of the function is doing exactly and what the author's intentions may have been.\n- First,\"\"\"\n    if print_text:\n        text_color_prefix = \"\\033[30m\"  # black; if you read against a dark background \\033[97m is white\n        print(text_color_prefix + prompt_to_explain_the_function, end=\"\")  # end='' prevents a newline from being printed\n\n    # send the prompt to the API, using \\n\\n as a stop sequence to stop at the end of the bullet list\n    explanation_response = openai.Completion.create(\n        model=text_model,\n        prompt=prompt_to_explain_the_function,\n        stop=[\"\\n\\n\", \"\\n\\t\\n\", \"\\n    \\n\"],\n        max_tokens=max_tokens,\n        temperature=temperature,\n        stream=True,\n    )\n    explanation_completion = \"\"\n    if print_text:\n        completion_color_prefix = \"\\033[92m\"  # green\n        print(completion_color_prefix, end=\"\")\n    for event in explanation_response:\n        event_text = event[\"choices\"][0][\"text\"]\n        explanation_completion += event_text\n        if print_text:\n            print(event_text, end=\"\")\n\n    # Step 2: Generate a plan to write a unit test\n\n    # create a markdown-formatted prompt that asks GPT-3 to complete a plan for writing unit tests, formatted as a bullet list\n    prompt_to_explain_a_plan = f\"\"\"\nA good unit test suite should aim to:\n- Test the function's behavior for a wide range of possible inputs\n- Test edge cases that the author may not have foreseen\n- Take advantage of the features of `{unit_test_package}` to make the tests easy to write and maintain\n- Be easy to read and understand, with clean code and descriptive names\n- Be deterministic, so that the tests always pass or fail in the same way\n\n`{unit_test_package}` has many convenient features that make it easy to write and maintain unit tests. We'll use them to write unit tests for the function above.\n\nFor this particular function, we'll want our unit tests to handle the following diverse scenarios (and under each scenario, we include a few examples as sub-bullets):\n-\"\"\"\n    if print_text:\n        print(text_color_prefix + prompt_to_explain_a_plan, end=\"\")\n\n    # append this planning prompt to the results from step 1\n    prior_text = prompt_to_explain_the_function + explanation_completion\n    full_plan_prompt = prior_text + prompt_to_explain_a_plan\n\n    # send the prompt to the API, using \\n\\n as a stop sequence to stop at the end of the bullet list\n    plan_response = openai.Completion.create(\n        model=text_model,\n        prompt=full_plan_prompt,\n        stop=[\"\\n\\n\", \"\\n\\t\\n\", \"\\n    \\n\"],\n        max_tokens=max_tokens,\n        temperature=temperature,\n        stream=True,\n    )\n    plan_completion = \"\"\n    if print_text:\n        print(completion_color_prefix, end=\"\")\n    for event in plan_response:\n        event_text = event[\"choices\"][0][\"text\"]\n        plan_completion += event_text\n        if print_text:\n            print(event_text, end=\"\")\n\n    # Step 2b: If the plan is short, ask GPT-3 to elaborate further\n    # this counts top-level bullets (e.g., categories), but not sub-bullets (e.g., test cases)\n    elaboration_needed = plan_completion.count(\"\\n-\") +1 &lt; approx_min_cases_to_cover  # adds 1 because the first bullet is not counted\n    if elaboration_needed:\n        prompt_to_elaborate_on_the_plan = f\"\"\"\n\nIn addition to the scenarios above, we'll also want to make sure we don't forget to test rare or unexpected edge cases (and under each edge case, we include a few examples as sub-bullets):\n-\"\"\"\n        if print_text:\n            print(text_color_prefix + prompt_to_elaborate_on_the_plan, end=\"\")\n\n        # append this elaboration prompt to the results from step 2\n        prior_text = full_plan_prompt + plan_completion\n        full_elaboration_prompt = prior_text + prompt_to_elaborate_on_the_plan\n\n        # send the prompt to the API, using \\n\\n as a stop sequence to stop at the end of the bullet list\n        elaboration_response = openai.Completion.create(\n            model=text_model,\n            prompt=full_elaboration_prompt,\n            stop=[\"\\n\\n\", \"\\n\\t\\n\", \"\\n    \\n\"],\n            max_tokens=max_tokens,\n            temperature=temperature,\n            stream=True,\n        )\n        elaboration_completion = \"\"\n        if print_text:\n            print(completion_color_prefix, end=\"\")\n        for event in elaboration_response:\n            event_text = event[\"choices\"][0][\"text\"]\n            elaboration_completion += event_text\n            if print_text:\n                print(event_text, end=\"\")\n\n    # Step 3: Generate the unit test\n\n    # create a markdown-formatted prompt that asks GPT-3 to complete a unit test\n    starter_comment = \"\"\n    if unit_test_package == \"pytest\":\n        starter_comment = \"Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator\"\n    prompt_to_generate_the_unit_test = f\"\"\"\n\nBefore going into the individual tests, let's first look at the complete suite of unit tests as a cohesive whole. We've added helpful comments to explain what each line does.\n```python\nimport {unit_test_package}  # used for our unit tests\n\n{function_to_test}\n\n#{starter_comment}\"\"\"\n    if print_text:\n        print(text_color_prefix + prompt_to_generate_the_unit_test, end=\"\")\n\n    # append this unit test prompt to the results from step 3\n    if elaboration_needed:\n        prior_text = full_elaboration_prompt + elaboration_completion\n    else:\n        prior_text = full_plan_prompt + plan_completion\n    full_unit_test_prompt = prior_text + prompt_to_generate_the_unit_test\n\n    # send the prompt to the API, using ``` as a stop sequence to stop at the end of the code block\n    unit_test_response = openai.Completion.create(\n        model=code_model,\n        prompt=full_unit_test_prompt,\n        stop=\"```\",\n        max_tokens=max_tokens,\n        temperature=temperature,\n        stream=True\n    )\n    unit_test_completion = \"\"\n    if print_text:\n        print(completion_color_prefix, end=\"\")\n    for event in unit_test_response:\n        event_text = event[\"choices\"][0][\"text\"]\n        unit_test_completion += event_text\n        if print_text:\n            print(event_text, end=\"\")\n\n    # check the output for errors\n    code_start_index = prompt_to_generate_the_unit_test.find(\"```python\\n\") + len(\"```python\\n\")\n    code_output = prompt_to_generate_the_unit_test[code_start_index:] + unit_test_completion\n    try:\n        ast.parse(code_output)\n    except SyntaxError as e:\n        print(f\"Syntax error in generated code: {e}\")\n        if reruns_if_fail &gt; 0:\n            print(\"Rerunning...\")\n            return unit_test_from_function(\n                function_to_test=function_to_test,\n                unit_test_package=unit_test_package,\n                approx_min_cases_to_cover=approx_min_cases_to_cover,\n                print_text=print_text,\n                text_model=text_model,\n                code_model=code_model,\n                max_tokens=max_tokens,\n                temperature=temperature,\n                reruns_if_fail=reruns_if_fail-1,  # decrement rerun counter when calling again\n            )\n\n    # return the unit test as a string\n    return unit_test_completion\n</pre> # imports needed to run the code in this notebook import ast  # used for detecting whether generated Python code is valid import openai  # used for calling the OpenAI API  # example of a function that uses a multi-step prompt to write unit tests def unit_test_from_function(     function_to_test: str,  # Python function to test, as a string     unit_test_package: str = \"pytest\",  # unit testing package; use the name as it appears in the import statement     approx_min_cases_to_cover: int = 7,  # minimum number of test case categories to cover (approximate)     print_text: bool = False,  # optionally prints text; helpful for understanding the function &amp; debugging     text_model: str = \"text-davinci-002\",  # model used to generate text plans in steps 1, 2, and 2b     code_model: str = \"code-davinci-002\",  # if you don't have access to code models, you can use text models here instead     max_tokens: int = 1000,  # can set this high, as generations should be stopped earlier by stop sequences     temperature: float = 0.4,  # temperature = 0 can sometimes get stuck in repetitive loops, so we use 0.4     reruns_if_fail: int = 1,  # if the output code cannot be parsed, this will re-run the function up to N times ) -&gt; str:     \"\"\"Outputs a unit test for a given Python function, using a 3-step GPT-3 prompt.\"\"\"      # Step 1: Generate an explanation of the function      # create a markdown-formatted prompt that asks GPT-3 to complete an explanation of the function, formatted as a bullet list     prompt_to_explain_the_function = f\"\"\"# How to write great unit tests with {unit_test_package}  In this advanced tutorial for experts, we'll use Python 3.9 and `{unit_test_package}` to write a suite of unit tests to verify the behavior of the following function. ```python {function_to_test} ```  Before writing any unit tests, let's review what each element of the function is doing exactly and what the author's intentions may have been. - First,\"\"\"     if print_text:         text_color_prefix = \"\\033[30m\"  # black; if you read against a dark background \\033[97m is white         print(text_color_prefix + prompt_to_explain_the_function, end=\"\")  # end='' prevents a newline from being printed      # send the prompt to the API, using \\n\\n as a stop sequence to stop at the end of the bullet list     explanation_response = openai.Completion.create(         model=text_model,         prompt=prompt_to_explain_the_function,         stop=[\"\\n\\n\", \"\\n\\t\\n\", \"\\n    \\n\"],         max_tokens=max_tokens,         temperature=temperature,         stream=True,     )     explanation_completion = \"\"     if print_text:         completion_color_prefix = \"\\033[92m\"  # green         print(completion_color_prefix, end=\"\")     for event in explanation_response:         event_text = event[\"choices\"][0][\"text\"]         explanation_completion += event_text         if print_text:             print(event_text, end=\"\")      # Step 2: Generate a plan to write a unit test      # create a markdown-formatted prompt that asks GPT-3 to complete a plan for writing unit tests, formatted as a bullet list     prompt_to_explain_a_plan = f\"\"\"      A good unit test suite should aim to: - Test the function's behavior for a wide range of possible inputs - Test edge cases that the author may not have foreseen - Take advantage of the features of `{unit_test_package}` to make the tests easy to write and maintain - Be easy to read and understand, with clean code and descriptive names - Be deterministic, so that the tests always pass or fail in the same way  `{unit_test_package}` has many convenient features that make it easy to write and maintain unit tests. We'll use them to write unit tests for the function above.  For this particular function, we'll want our unit tests to handle the following diverse scenarios (and under each scenario, we include a few examples as sub-bullets): -\"\"\"     if print_text:         print(text_color_prefix + prompt_to_explain_a_plan, end=\"\")      # append this planning prompt to the results from step 1     prior_text = prompt_to_explain_the_function + explanation_completion     full_plan_prompt = prior_text + prompt_to_explain_a_plan      # send the prompt to the API, using \\n\\n as a stop sequence to stop at the end of the bullet list     plan_response = openai.Completion.create(         model=text_model,         prompt=full_plan_prompt,         stop=[\"\\n\\n\", \"\\n\\t\\n\", \"\\n    \\n\"],         max_tokens=max_tokens,         temperature=temperature,         stream=True,     )     plan_completion = \"\"     if print_text:         print(completion_color_prefix, end=\"\")     for event in plan_response:         event_text = event[\"choices\"][0][\"text\"]         plan_completion += event_text         if print_text:             print(event_text, end=\"\")      # Step 2b: If the plan is short, ask GPT-3 to elaborate further     # this counts top-level bullets (e.g., categories), but not sub-bullets (e.g., test cases)     elaboration_needed = plan_completion.count(\"\\n-\") +1 &lt; approx_min_cases_to_cover  # adds 1 because the first bullet is not counted     if elaboration_needed:         prompt_to_elaborate_on_the_plan = f\"\"\"  In addition to the scenarios above, we'll also want to make sure we don't forget to test rare or unexpected edge cases (and under each edge case, we include a few examples as sub-bullets): -\"\"\"         if print_text:             print(text_color_prefix + prompt_to_elaborate_on_the_plan, end=\"\")          # append this elaboration prompt to the results from step 2         prior_text = full_plan_prompt + plan_completion         full_elaboration_prompt = prior_text + prompt_to_elaborate_on_the_plan          # send the prompt to the API, using \\n\\n as a stop sequence to stop at the end of the bullet list         elaboration_response = openai.Completion.create(             model=text_model,             prompt=full_elaboration_prompt,             stop=[\"\\n\\n\", \"\\n\\t\\n\", \"\\n    \\n\"],             max_tokens=max_tokens,             temperature=temperature,             stream=True,         )         elaboration_completion = \"\"         if print_text:             print(completion_color_prefix, end=\"\")         for event in elaboration_response:             event_text = event[\"choices\"][0][\"text\"]             elaboration_completion += event_text             if print_text:                 print(event_text, end=\"\")      # Step 3: Generate the unit test      # create a markdown-formatted prompt that asks GPT-3 to complete a unit test     starter_comment = \"\"     if unit_test_package == \"pytest\":         starter_comment = \"Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator\"     prompt_to_generate_the_unit_test = f\"\"\"  Before going into the individual tests, let's first look at the complete suite of unit tests as a cohesive whole. We've added helpful comments to explain what each line does. ```python import {unit_test_package}  # used for our unit tests  {function_to_test}  #{starter_comment}\"\"\"     if print_text:         print(text_color_prefix + prompt_to_generate_the_unit_test, end=\"\")      # append this unit test prompt to the results from step 3     if elaboration_needed:         prior_text = full_elaboration_prompt + elaboration_completion     else:         prior_text = full_plan_prompt + plan_completion     full_unit_test_prompt = prior_text + prompt_to_generate_the_unit_test      # send the prompt to the API, using ``` as a stop sequence to stop at the end of the code block     unit_test_response = openai.Completion.create(         model=code_model,         prompt=full_unit_test_prompt,         stop=\"```\",         max_tokens=max_tokens,         temperature=temperature,         stream=True     )     unit_test_completion = \"\"     if print_text:         print(completion_color_prefix, end=\"\")     for event in unit_test_response:         event_text = event[\"choices\"][0][\"text\"]         unit_test_completion += event_text         if print_text:             print(event_text, end=\"\")      # check the output for errors     code_start_index = prompt_to_generate_the_unit_test.find(\"```python\\n\") + len(\"```python\\n\")     code_output = prompt_to_generate_the_unit_test[code_start_index:] + unit_test_completion     try:         ast.parse(code_output)     except SyntaxError as e:         print(f\"Syntax error in generated code: {e}\")         if reruns_if_fail &gt; 0:             print(\"Rerunning...\")             return unit_test_from_function(                 function_to_test=function_to_test,                 unit_test_package=unit_test_package,                 approx_min_cases_to_cover=approx_min_cases_to_cover,                 print_text=print_text,                 text_model=text_model,                 code_model=code_model,                 max_tokens=max_tokens,                 temperature=temperature,                 reruns_if_fail=reruns_if_fail-1,  # decrement rerun counter when calling again             )      # return the unit test as a string     return unit_test_completion  In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[2]: Copied! <pre>example_function = \"\"\"def is_palindrome(s):\n    return s == s[::-1]\"\"\"\n\nunit_test_from_function(example_function, print_text=True)\n</pre> example_function = \"\"\"def is_palindrome(s):     return s == s[::-1]\"\"\"  unit_test_from_function(example_function, print_text=True) <pre># How to write great unit tests with pytest\n\nIn this advanced tutorial for experts, we'll use Python 3.9 and `pytest` to write a suite of unit tests to verify the behavior of the following function.\n```python\ndef is_palindrome(s):\n    return s == s[::-1]\n```\n\nBefore writing any unit tests, let's review what each element of the function is doing exactly and what the author's intentions may have been.\n- First, we have a function definition. This is where we give the function a name, `is_palindrome`, and specify the arguments that the function accepts. In this case, the function accepts a single string argument, `s`.\n- Next, we have a return statement. This is where we specify the value that the function returns. In this case, the function returns `s == s[::-1]`.\n- Finally, we have a function call. This is where we actually call the function with a specific set of arguments. In this case, we're calling the function with the string `\"racecar\"`.\n    \nA good unit test suite should aim to:\n- Test the function's behavior for a wide range of possible inputs\n- Test edge cases that the author may not have foreseen\n- Take advantage of the features of `pytest` to make the tests easy to write and maintain\n- Be easy to read and understand, with clean code and descriptive names\n- Be deterministic, so that the tests always pass or fail in the same way\n\n`pytest` has many convenient features that make it easy to write and maintain unit tests. We'll use them to write unit tests for the function above.\n\nFor this particular function, we'll want our unit tests to handle the following diverse scenarios (and under each scenario, we include a few examples as sub-bullets):\n- The input is a palindrome\n    - `\"racecar\"`\n    - `\"madam\"`\n    - `\"anna\"`\n- The input is not a palindrome\n    - `\"python\"`\n    - `\"test\"`\n    - `\"1234\"`\n- The input is an empty string\n    - `\"\"`\n- The input is `None`\n- The input is not a string\n    - `1`\n    - `1.0`\n    - `True`\n    - `False`\n    - `[]`\n    - `{}`\n\nIn addition to the scenarios above, we'll also want to make sure we don't forget to test rare or unexpected edge cases (and under each edge case, we include a few examples as sub-bullets):\n- The input is a palindrome with spaces\n    - `\"race car\"`\n    - `\" madam \"`\n    - `\" anna \"`\n- The input is not a palindrome with spaces\n    - `\" python \"`\n    - `\" test \"`\n    - `\" 1234 \"`\n- The input is a palindrome with punctuation\n    - `\"racecar!\"`\n    - `\"Madam, I'm Adam.\"`\n    - `\"Anna's\"`\n- The input is not a palindrome with punctuation\n    - `\"python!\"`\n    - `\"test.\"`\n    - `\"1234!\"`\n- The input is a palindrome with mixed case\n    - `\"Racecar\"`\n    - `\"Madam\"`\n    - `\"Anna\"`\n- The input is not a palindrome with mixed case\n    - `\"Python\"`\n    - `\"Test\"`\n    - `\"1234\"`\n\nBefore going into the individual tests, let's first look at the complete suite of unit tests as a cohesive whole. We've added helpful comments to explain what each line does.\n```python\nimport pytest  # used for our unit tests\n\ndef is_palindrome(s):\n    return s == s[::-1]\n\n#Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator.\n#The first element of the tuple is a name for the test case, and the second element is a list of arguments for the test case.\n#The @pytest.mark.parametrize decorator will generate a separate test function for each test case.\n#The generated test function will be named test_is_palindrome_&lt;name&gt; where &lt;name&gt; is the name of the test case.\n#The generated test function will be given the arguments specified in the list of arguments for the test case.\n#The generated test function will be given the fixture specified in the decorator, in this case the function itself.\n#The generated test function will call the function with the arguments and assert that the result is equal to the expected value.\n@pytest.mark.parametrize(\n    \"name,args,expected\",\n    [\n        # Test the function's behavior for a wide range of possible inputs\n        (\"palindrome\", [\"racecar\"], True),\n        (\"palindrome\", [\"madam\"], True),\n        (\"palindrome\", [\"anna\"], True),\n        (\"non-palindrome\", [\"python\"], False),\n        (\"non-palindrome\", [\"test\"], False),\n        (\"non-palindrome\", [\"1234\"], False),\n        (\"empty string\", [\"\"], True),\n        (\"None\", [None], False),\n        (\"non-string\", [1], False),\n        (\"non-string\", [1.0], False),\n        (\"non-string\", [True], False),\n        (\"non-string\", [False], False),\n        (\"non-string\", [[]], False),\n        (\"non-string\", [{}], False),\n        # Test edge cases that the author may not have foreseen\n        (\"palindrome with spaces\", [\"race car\"], True),\n        (\"palindrome with spaces\", [\" madam \"], True),\n        (\"palindrome with spaces\", [\" anna \"], True),\n        (\"non-palindrome with spaces\", [\" python \"], False),\n        (\"non-palindrome with spaces\", [\" test \"], False),\n        (\"non-palindrome with spaces\", [\" 1234 \"], False),\n        (\"palindrome with punctuation\", [\"racecar!\"], True),\n        (\"palindrome with punctuation\", [\"Madam, I'm Adam.\"], True),\n        (\"palindrome with punctuation\", [\"Anna's\"], True),\n        (\"non-palindrome with punctuation\", [\"python!\"], False),\n        (\"non-palindrome with punctuation\", [\"test.\"], False),\n        (\"non-palindrome with punctuation\", [\"1234!\"], False),\n        (\"palindrome with mixed case\", [\"Racecar\"], True),\n        (\"palindrome with mixed case\", [\"Madam\"], True),\n        (\"palindrome with mixed case\", [\"Anna\"], True),\n        (\"non-palindrome with mixed case\", [\"Python\"], False),\n        (\"non-palindrome with mixed case\", [\"Test\"], False),\n        (\"non-palindrome with mixed case\", [\"1234\"], False),\n    ],\n)\ndef test_is_palindrome(is_palindrome, args, expected):\n    assert is_palindrome(*args) == expected\n</pre> Out[2]: <pre>'.\\n#The first element of the tuple is a name for the test case, and the second element is a list of arguments for the test case.\\n#The @pytest.mark.parametrize decorator will generate a separate test function for each test case.\\n#The generated test function will be named test_is_palindrome_&lt;name&gt; where &lt;name&gt; is the name of the test case.\\n#The generated test function will be given the arguments specified in the list of arguments for the test case.\\n#The generated test function will be given the fixture specified in the decorator, in this case the function itself.\\n#The generated test function will call the function with the arguments and assert that the result is equal to the expected value.\\n@pytest.mark.parametrize(\\n    \"name,args,expected\",\\n    [\\n        # Test the function\\'s behavior for a wide range of possible inputs\\n        (\"palindrome\", [\"racecar\"], True),\\n        (\"palindrome\", [\"madam\"], True),\\n        (\"palindrome\", [\"anna\"], True),\\n        (\"non-palindrome\", [\"python\"], False),\\n        (\"non-palindrome\", [\"test\"], False),\\n        (\"non-palindrome\", [\"1234\"], False),\\n        (\"empty string\", [\"\"], True),\\n        (\"None\", [None], False),\\n        (\"non-string\", [1], False),\\n        (\"non-string\", [1.0], False),\\n        (\"non-string\", [True], False),\\n        (\"non-string\", [False], False),\\n        (\"non-string\", [[]], False),\\n        (\"non-string\", [{}], False),\\n        # Test edge cases that the author may not have foreseen\\n        (\"palindrome with spaces\", [\"race car\"], True),\\n        (\"palindrome with spaces\", [\" madam \"], True),\\n        (\"palindrome with spaces\", [\" anna \"], True),\\n        (\"non-palindrome with spaces\", [\" python \"], False),\\n        (\"non-palindrome with spaces\", [\" test \"], False),\\n        (\"non-palindrome with spaces\", [\" 1234 \"], False),\\n        (\"palindrome with punctuation\", [\"racecar!\"], True),\\n        (\"palindrome with punctuation\", [\"Madam, I\\'m Adam.\"], True),\\n        (\"palindrome with punctuation\", [\"Anna\\'s\"], True),\\n        (\"non-palindrome with punctuation\", [\"python!\"], False),\\n        (\"non-palindrome with punctuation\", [\"test.\"], False),\\n        (\"non-palindrome with punctuation\", [\"1234!\"], False),\\n        (\"palindrome with mixed case\", [\"Racecar\"], True),\\n        (\"palindrome with mixed case\", [\"Madam\"], True),\\n        (\"palindrome with mixed case\", [\"Anna\"], True),\\n        (\"non-palindrome with mixed case\", [\"Python\"], False),\\n        (\"non-palindrome with mixed case\", [\"Test\"], False),\\n        (\"non-palindrome with mixed case\", [\"1234\"], False),\\n    ],\\n)\\ndef test_is_palindrome(is_palindrome, args, expected):\\n    assert is_palindrome(*args) == expected\\n'</pre>"},{"location":"examples/Unit_test_writing_using_a_multi-step_prompt/#Unit-test-writing-using-a-multi-step-prompt","title":"Unit test writing using a multi-step prompt\u00b6","text":"<p>Complex tasks, such as writing unit tests, can benefit from multi-step prompts. In contrast to a single prompt, a multi-step prompt generates text from GPT-3 and then feeds that text back into subsequent prompts. This can help in cases where you want GPT-3 to explain its reasoning before answering, or brainstorm a plan before executing it.</p> <p>In this notebook, we use a 3-step prompt to write unit tests in Python using the following steps:</p> <ol> <li>Given a Python function, we first prompt GPT-3 to explain what the function is doing.</li> <li>Second, we prompt GPT-3 to plan a set of unit tests for the function.<ul> <li>If the plan is too short, we ask GPT-3 to elaborate with more ideas for unit tests.</li> </ul> </li> <li>Finally, we prompt GPT-3 to write the unit tests.</li> </ol> <p>The code example illustrates a few optional embellishments on the chained, multi-step prompt:</p> <ul> <li>Conditional branching (e.g., only asking for elaboration if the first plan is too short)</li> <li>Different models for different steps (e.g., <code>text-davinci-002</code> for the text planning steps and <code>code-davinci-002</code> for the code writing step)</li> <li>A check that re-runs the function if the output is unsatisfactory (e.g., if the output code cannot be parsed by Python's <code>ast</code> module)</li> <li>Streaming output so that you can start reading the output before it's fully generated (useful for long, multi-step outputs)</li> </ul> <p>The full 3-step prompt looks like this (using as an example <code>pytest</code> for the unit test framework and <code>is_palindrome</code> as the function):</p> <pre><code># How to write great unit tests with pytest\n\nIn this advanced tutorial for experts, we'll use Python 3.9 and `pytest` to write a suite of unit tests to verify the behavior of the following function.\n```python\ndef is_palindrome(s):\n    return s == s[::-1]\n```\n\nBefore writing any unit tests, let's review what each element of the function is doing exactly and what the author's intentions may have been.\n- First,{GENERATED IN STEP 1}\n\nA good unit test suite should aim to:\n- Test the function's behavior for a wide range of possible inputs\n- Test edge cases that the author may not have foreseen\n- Take advantage of the features of `pytest` to make the tests easy to write and maintain\n- Be easy to read and understand, with clean code and descriptive names\n- Be deterministic, so that the tests always pass or fail in the same way\n\n`pytest` has many convenient features that make it easy to write and maintain unit tests. We'll use them to write unit tests for the function above.\n\nFor this particular function, we'll want our unit tests to handle the following diverse scenarios (and under each scenario, we include a few examples as sub-bullets):\n-{GENERATED IN STEP 2}\n\n[OPTIONALLY APPENDED]In addition to the scenarios above, we'll also want to make sure we don't forget to test rare or unexpected edge cases (and under each edge case, we include a few examples as sub-bullets):\n-{GENERATED IN STEP 2B}\n\nBefore going into the individual tests, let's first look at the complete suite of unit tests as a cohesive whole. We've added helpful comments to explain what each line does.\n```python\nimport pytest  # used for our unit tests\n\ndef is_palindrome(s):\n    return s == s[::-1]\n\n#Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator\n{GENERATED IN STEP 3}\n</code></pre>"},{"location":"examples/User_and_product_embeddings/","title":"User and product embeddings","text":"In\u00a0[2]: Copied! <pre>import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\ndf = pd.read_csv('output/embedded_babbage_similarity_50k.csv', index_col=0)  # note that you will need to generate this file to run the code below\ndf['babbage_similarity'] = df.babbage_similarity.apply(eval).apply(np.array)\nX_train, X_test, y_train, y_test = train_test_split(df, df.Score, test_size = 0.2, random_state=42)\n\nuser_embeddings = X_train.groupby('UserId').babbage_similarity.apply(np.mean)\nprod_embeddings = X_train.groupby('ProductId').babbage_similarity.apply(np.mean)\nlen(user_embeddings), len(prod_embeddings)\n</pre> import pandas as pd import numpy as np from sklearn.model_selection import train_test_split  df = pd.read_csv('output/embedded_babbage_similarity_50k.csv', index_col=0)  # note that you will need to generate this file to run the code below df['babbage_similarity'] = df.babbage_similarity.apply(eval).apply(np.array) X_train, X_test, y_train, y_test = train_test_split(df, df.Score, test_size = 0.2, random_state=42)  user_embeddings = X_train.groupby('UserId').babbage_similarity.apply(np.mean) prod_embeddings = X_train.groupby('ProductId').babbage_similarity.apply(np.mean) len(user_embeddings), len(prod_embeddings)  Out[2]: <pre>(24502, 19035)</pre> <p>We can see that most of the users and products appear within the 50k examples only once.</p> In\u00a0[3]: Copied! <pre>from openai.embeddings_utils import cosine_similarity\n\n# evaluate embeddings as recommendations on X_test\ndef evaluate_single_match(row):\n    user_id = row.UserId\n    product_id = row.ProductId\n    try:\n        user_embedding = user_embeddings[user_id]\n        product_embedding = prod_embeddings[product_id]\n        similarity = cosine_similarity(user_embedding, product_embedding)\n        return similarity\n    except Exception as e:\n        return np.nan\n\nX_test['cosine_similarity'] = X_test.apply(evaluate_single_match, axis=1)\nX_test['percentile_cosine_similarity'] = X_test.cosine_similarity.rank(pct=True)\n</pre> from openai.embeddings_utils import cosine_similarity  # evaluate embeddings as recommendations on X_test def evaluate_single_match(row):     user_id = row.UserId     product_id = row.ProductId     try:         user_embedding = user_embeddings[user_id]         product_embedding = prod_embeddings[product_id]         similarity = cosine_similarity(user_embedding, product_embedding)         return similarity     except Exception as e:         return np.nan  X_test['cosine_similarity'] = X_test.apply(evaluate_single_match, axis=1) X_test['percentile_cosine_similarity'] = X_test.cosine_similarity.rank(pct=True)  In\u00a0[18]: Copied! <pre>import matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n\ncorrelation = X_test[['percentile_cosine_similarity', 'Score']].corr().values[0,1]\nprint('Correlation between user &amp; vector similarity percentile metric and review number of stars (score): %.2f%%' % (100*correlation))\n\n# boxplot of cosine similarity for each score\nX_test.boxplot(column='percentile_cosine_similarity', by='Score')\nplt.title('')\nplt.show()\nplt.close()\n</pre> import matplotlib.pyplot as plt import statsmodels.api as sm   correlation = X_test[['percentile_cosine_similarity', 'Score']].corr().values[0,1] print('Correlation between user &amp; vector similarity percentile metric and review number of stars (score): %.2f%%' % (100*correlation))  # boxplot of cosine similarity for each score X_test.boxplot(column='percentile_cosine_similarity', by='Score') plt.title('') plt.show() plt.close() <pre>Correlation between user&amp;vector similarity percentile metric and review number of stars (score): 22.11%\n</pre> <p>We can observe a weak trend, showing that the higher the similarity score between the user and the product embedding, the higher the review score. Therefore, the user and product embeddings can weakly predict the review score - even before the user receives the product!</p> <p>Because this signal works in a different way than the more commonly used collaborative filtering, it can act as an additional feature to slightly improve the performance on existing problems.</p>"},{"location":"examples/User_and_product_embeddings/#User-and-product-embeddings","title":"User and product embeddings\u00b6","text":"<p>We calculate user and product embeddings based on the training set, and evaluate the results on the unseen test set. We will evaluate the results by plotting the user and product similarity versus the review score. The dataset is created in the Obtain_dataset Notebook.</p>"},{"location":"examples/User_and_product_embeddings/#1.-Calculate-user-and-product-embeddings","title":"1. Calculate user and product embeddings\u00b6","text":"<p>We calculate these embeddings simply by averaging all the reviews about the same product or written by the same user within the training set.</p>"},{"location":"examples/User_and_product_embeddings/#2.-Evaluate-the-embeddings","title":"2. Evaluate the embeddings\u00b6","text":"<p>To evaluate the recommendations, we look at the similarity of the user and product embeddings amongst the reviews in the unseen test set. We calculate the cosine distance between the user and product embeddings, which gives us a similarity score between 0 and 1. We then normalize the scores to be evenly split between 0 and 1, by calculating the percentile of the similarity score amongst all predicted scores.</p>"},{"location":"examples/User_and_product_embeddings/#2.1-Visualize-cosine-similarity-by-review-score","title":"2.1 Visualize cosine similarity by review score\u00b6","text":"<p>We group the cosine similarity scores by the review score, and plot the distribution of cosine similarity scores for each review score.</p>"},{"location":"examples/Visualizing_embeddings_in_2D/","title":"Visualizing embeddings in 2D","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nfrom sklearn.manifold import TSNE\nimport numpy as np\n\n# Load the embeddings\ndatafile_path = \"data/fine_food_reviews_with_embeddings_1k.csv\"\ndf = pd.read_csv(datafile_path)\n\n# Convert to a list of lists of floats\nmatrix = np.array(df.embedding.apply(eval).to_list())\n\n# Create a t-SNE model and transform the data\ntsne = TSNE(n_components=2, perplexity=15, random_state=42, init='random', learning_rate=200)\nvis_dims = tsne.fit_transform(matrix)\nvis_dims.shape\n</pre> import pandas as pd from sklearn.manifold import TSNE import numpy as np  # Load the embeddings datafile_path = \"data/fine_food_reviews_with_embeddings_1k.csv\" df = pd.read_csv(datafile_path)  # Convert to a list of lists of floats matrix = np.array(df.embedding.apply(eval).to_list())  # Create a t-SNE model and transform the data tsne = TSNE(n_components=2, perplexity=15, random_state=42, init='random', learning_rate=200) vis_dims = tsne.fit_transform(matrix) vis_dims.shape Out[1]: <pre>(1000, 2)</pre> <p>We can observe a decent data separation even in the reduced 2 dimensions.</p> In\u00a0[2]: Copied! <pre>import matplotlib.pyplot as plt\nimport matplotlib\nimport numpy as np\n\ncolors = [\"red\", \"darkorange\", \"gold\", \"turquoise\", \"darkgreen\"]\nx = [x for x,y in vis_dims]\ny = [y for x,y in vis_dims]\ncolor_indices = df.Score.values - 1\n\ncolormap = matplotlib.colors.ListedColormap(colors)\nplt.scatter(x, y, c=color_indices, cmap=colormap, alpha=0.3)\nfor score in [0,1,2,3,4]:\n    avg_x = np.array(x)[df.Score-1==score].mean()\n    avg_y = np.array(y)[df.Score-1==score].mean()\n    color = colors[score]\n    plt.scatter(avg_x, avg_y, marker='x', color=color, s=100)\n\nplt.title(\"Amazon ratings visualized in language using t-SNE\")\n</pre> import matplotlib.pyplot as plt import matplotlib import numpy as np  colors = [\"red\", \"darkorange\", \"gold\", \"turquoise\", \"darkgreen\"] x = [x for x,y in vis_dims] y = [y for x,y in vis_dims] color_indices = df.Score.values - 1  colormap = matplotlib.colors.ListedColormap(colors) plt.scatter(x, y, c=color_indices, cmap=colormap, alpha=0.3) for score in [0,1,2,3,4]:     avg_x = np.array(x)[df.Score-1==score].mean()     avg_y = np.array(y)[df.Score-1==score].mean()     color = colors[score]     plt.scatter(avg_x, avg_y, marker='x', color=color, s=100)  plt.title(\"Amazon ratings visualized in language using t-SNE\") Out[2]: <pre>Text(0.5, 1.0, 'Amazon ratings visualized in language using t-SNE')</pre>"},{"location":"examples/Visualizing_embeddings_in_2D/#Visualizing-the-embeddings-in-2D","title":"Visualizing the embeddings in 2D\u00b6","text":"<p>We will use t-SNE to reduce the dimensionality of the embeddings from 1536 to 2. Once the embeddings are reduced to two dimensions, we can plot them in a 2D scatter plot. The dataset is created in the Obtain_dataset Notebook.</p>"},{"location":"examples/Visualizing_embeddings_in_2D/#1.-Reduce-dimensionality","title":"1. Reduce dimensionality\u00b6","text":"<p>We reduce the dimensionality to 2 dimensions using t-SNE decomposition.</p>"},{"location":"examples/Visualizing_embeddings_in_2D/#2.-Plotting-the-embeddings","title":"2. Plotting the embeddings\u00b6","text":"<p>We colour each review by its star rating, ranging from red to green.</p>"},{"location":"examples/Visualizing_embeddings_in_3D/","title":"Visualizing embeddings in 3D","text":"<p>The example uses PCA to reduce the dimensionality fo the embeddings from 1536 to 3. Then we can visualize the data points in a 3D plot. The small dataset <code>dbpedia_samples.jsonl</code> is curated by randomly sampling 200 samples from DBpedia validation dataset.</p> In\u00a0[1]: Copied! <pre>import pandas as pd\nsamples = pd.read_json(\"data/dbpedia_samples.jsonl\", lines=True)\ncategories = sorted(samples[\"category\"].unique())\nprint(\"Categories of DBpedia samples:\", samples[\"category\"].value_counts())\nsamples.head()\n</pre> import pandas as pd samples = pd.read_json(\"data/dbpedia_samples.jsonl\", lines=True) categories = sorted(samples[\"category\"].unique()) print(\"Categories of DBpedia samples:\", samples[\"category\"].value_counts()) samples.head() <pre>Categories of DBpedia samples: Artist                    21\nFilm                      19\nPlant                     19\nOfficeHolder              18\nCompany                   17\nNaturalPlace              16\nAthlete                   16\nVillage                   12\nWrittenWork               11\nBuilding                  11\nAlbum                     11\nAnimal                    11\nEducationalInstitution    10\nMeanOfTransportation       8\nName: category, dtype: int64\n</pre> Out[1]: text category 0 Morada Limited is a textile company based in ... Company 1 The Armenian Mirror-Spectator is a newspaper ... WrittenWork 2 Mt. Kinka (\u91d1\u83ef\u5c71 Kinka-zan) also known as Kinka... NaturalPlace 3 Planning the Play of a Bridge Hand is a book ... WrittenWork 4 Wang Yuanping (born 8 December 1976) is a ret... Athlete In\u00a0[2]: Copied! <pre>from openai.embeddings_utils import get_embeddings\n# NOTE: The following code will send a query of batch size 200 to /embeddings\nmatrix = get_embeddings(samples[\"text\"].to_list(), engine=\"text-embedding-ada-002\")\n</pre> from openai.embeddings_utils import get_embeddings # NOTE: The following code will send a query of batch size 200 to /embeddings matrix = get_embeddings(samples[\"text\"].to_list(), engine=\"text-embedding-ada-002\") In\u00a0[3]: Copied! <pre>from sklearn.decomposition import PCA\npca = PCA(n_components=3)\nvis_dims = pca.fit_transform(matrix)\nsamples[\"embed_vis\"] = vis_dims.tolist()\n</pre> from sklearn.decomposition import PCA pca = PCA(n_components=3) vis_dims = pca.fit_transform(matrix) samples[\"embed_vis\"] = vis_dims.tolist() In\u00a0[4]: Copied! <pre>%matplotlib widget\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig = plt.figure(figsize=(10, 5))\nax = fig.add_subplot(projection='3d')\ncmap = plt.get_cmap(\"tab20\")\n\n# Plot each sample category individually such that we can set label name.\nfor i, cat in enumerate(categories):\n    sub_matrix = np.array(samples[samples[\"category\"] == cat][\"embed_vis\"].to_list())\n    x=sub_matrix[:, 0]\n    y=sub_matrix[:, 1]\n    z=sub_matrix[:, 2]\n    colors = [cmap(i/len(categories))] * len(sub_matrix)\n    ax.scatter(x, y, zs=z, zdir='z', c=colors, label=cat)\n\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\nax.legend(bbox_to_anchor=(1.1, 1))\n</pre> %matplotlib widget import matplotlib.pyplot as plt import numpy as np  fig = plt.figure(figsize=(10, 5)) ax = fig.add_subplot(projection='3d') cmap = plt.get_cmap(\"tab20\")  # Plot each sample category individually such that we can set label name. for i, cat in enumerate(categories):     sub_matrix = np.array(samples[samples[\"category\"] == cat][\"embed_vis\"].to_list())     x=sub_matrix[:, 0]     y=sub_matrix[:, 1]     z=sub_matrix[:, 2]     colors = [cmap(i/len(categories))] * len(sub_matrix)     ax.scatter(x, y, zs=z, zdir='z', c=colors, label=cat)  ax.set_xlabel('x') ax.set_ylabel('y') ax.set_zlabel('z') ax.legend(bbox_to_anchor=(1.1, 1)) Out[4]: <pre>&lt;matplotlib.legend.Legend at 0x1622180a0&gt;</pre>                      Figure"},{"location":"examples/Visualizing_embeddings_in_3D/#Visualizing-embeddings-in-3D","title":"Visualizing embeddings in 3D\u00b6","text":""},{"location":"examples/Visualizing_embeddings_in_3D/#1.-Load-the-dataset-and-query-embeddings","title":"1. Load the dataset and query embeddings\u00b6","text":""},{"location":"examples/Visualizing_embeddings_in_3D/#2.-Reduce-the-embedding-dimensionality","title":"2. Reduce the embedding dimensionality\u00b6","text":""},{"location":"examples/Visualizing_embeddings_in_3D/#3.-Plot-the-embeddings-of-lower-dimensionality","title":"3. Plot the embeddings of lower dimensionality\u00b6","text":""},{"location":"examples/Visualizing_embeddings_in_W%26B/","title":"Visualizing embeddings in W&B","text":"In\u00a0[2]: Copied! <pre>import pandas as pd\nfrom sklearn.manifold import TSNE\nimport numpy as np\n\n# Load the embeddings\ndatafile_path = \"data/fine_food_reviews_with_embeddings_1k.csv\"\ndf = pd.read_csv(datafile_path)\n\n# Convert to a list of lists of floats\nmatrix = np.array(df.embedding.apply(eval).to_list())\n</pre> import pandas as pd from sklearn.manifold import TSNE import numpy as np  # Load the embeddings datafile_path = \"data/fine_food_reviews_with_embeddings_1k.csv\" df = pd.read_csv(datafile_path)  # Convert to a list of lists of floats matrix = np.array(df.embedding.apply(eval).to_list()) In\u00a0[\u00a0]: Copied! <pre>import wandb\n\noriginal_cols = df.columns[1:-1].tolist()\nembedding_cols = ['emb_'+str(idx) for idx in range(len(matrix[0]))]\ntable_cols = original_cols + embedding_cols\n\nwith wandb.init(project='openai_embeddings'):\n    table = wandb.Table(columns=table_cols)\n    for i, row in enumerate(df.to_dict(orient=\"records\")):\n        original_data = [row[col_name] for col_name in original_cols]\n        embedding_data = matrix[i].tolist()\n        table.add_data(*(original_data + embedding_data))\n    wandb.log({'openai_embedding_table': table})\n</pre> import wandb  original_cols = df.columns[1:-1].tolist() embedding_cols = ['emb_'+str(idx) for idx in range(len(matrix[0]))] table_cols = original_cols + embedding_cols  with wandb.init(project='openai_embeddings'):     table = wandb.Table(columns=table_cols)     for i, row in enumerate(df.to_dict(orient=\"records\")):         original_data = [row[col_name] for col_name in original_cols]         embedding_data = matrix[i].tolist()         table.add_data(*(original_data + embedding_data))     wandb.log({'openai_embedding_table': table}) <p>After navigating to the W&amp;B run link, we click the \u2699\ufe0f icon in the top right of the Table and change \"Render As:\" to \"Combined 2D Projection\".</p> <p>Example: http://wandb.me/openai_embeddings</p>"},{"location":"examples/Visualizing_embeddings_in_W%26B/#Visualizing-the-embeddings-in-W&B","title":"Visualizing the embeddings in W&amp;B\u00b6","text":"<p>We will upload the data to Weights &amp; Biases and use an Embedding Projector to visualize the embeddings using common dimension reduction algorithms like PCA, UMAP, and t-SNE. The dataset is created in the Obtain_dataset Notebook.</p>"},{"location":"examples/Visualizing_embeddings_in_W%26B/#What-is-Weights-&-Biases?","title":"What is Weights &amp; Biases?\u00b6","text":"<p>Weights &amp; Biases is a machine learning platform used by OpenAI and other ML teams to build better models faster. They use it to quickly track experiments, evaluate model performance, reproduce models, visualize results, and share findings with colleagues.</p>"},{"location":"examples/Visualizing_embeddings_in_W%26B/#1.-Log-the-data-to-W&B","title":"1. Log the data to W&amp;B\u00b6","text":"<p>We create a W&amp;B Table with the original data and the embeddings. Each review is a new row and the 1536 embedding floats are given their own column named <code>emb_{i}</code>.</p>"},{"location":"examples/Visualizing_embeddings_in_W%26B/#2.-Render-as-2D-Projection","title":"2. Render as 2D Projection\u00b6","text":""},{"location":"examples/Zero-shot_classification_with_embeddings/","title":"Zero shot classification with embeddings","text":"In\u00a0[1]: Copied! <pre># imports\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.metrics import classification_report\n\n# parameters\nEMBEDDING_MODEL = \"text-embedding-ada-002\"\n\n# load data\ndatafile_path = \"data/fine_food_reviews_with_embeddings_1k.csv\"\n\ndf = pd.read_csv(datafile_path)\ndf[\"embedding\"] = df.embedding.apply(eval).apply(np.array)\n\n# convert 5-star rating to binary sentiment\ndf = df[df.Score != 3]\ndf[\"sentiment\"] = df.Score.replace({1: \"negative\", 2: \"negative\", 4: \"positive\", 5: \"positive\"})\n</pre> # imports import pandas as pd import numpy as np  from sklearn.metrics import classification_report  # parameters EMBEDDING_MODEL = \"text-embedding-ada-002\"  # load data datafile_path = \"data/fine_food_reviews_with_embeddings_1k.csv\"  df = pd.read_csv(datafile_path) df[\"embedding\"] = df.embedding.apply(eval).apply(np.array)  # convert 5-star rating to binary sentiment df = df[df.Score != 3] df[\"sentiment\"] = df.Score.replace({1: \"negative\", 2: \"negative\", 4: \"positive\", 5: \"positive\"})  In\u00a0[2]: Copied! <pre>from openai.embeddings_utils import cosine_similarity, get_embedding\nfrom sklearn.metrics import PrecisionRecallDisplay\n\ndef evaluate_embeddings_approach(\n    labels = ['negative', 'positive'], \n    model = EMBEDDING_MODEL,\n):\n    label_embeddings = [get_embedding(label, engine=model) for label in labels]\n\n    def label_score(review_embedding, label_embeddings):\n        return cosine_similarity(review_embedding, label_embeddings[1]) - cosine_similarity(review_embedding, label_embeddings[0])\n\n    probas = df[\"embedding\"].apply(lambda x: label_score(x, label_embeddings))\n    preds = probas.apply(lambda x: 'positive' if x&gt;0 else 'negative')\n\n    report = classification_report(df.sentiment, preds)\n    print(report)\n\n    display = PrecisionRecallDisplay.from_predictions(df.sentiment, probas, pos_label='positive')\n    _ = display.ax_.set_title(\"2-class Precision-Recall curve\")\n\nevaluate_embeddings_approach(labels=['negative', 'positive'], model=EMBEDDING_MODEL)\n</pre> from openai.embeddings_utils import cosine_similarity, get_embedding from sklearn.metrics import PrecisionRecallDisplay  def evaluate_embeddings_approach(     labels = ['negative', 'positive'],      model = EMBEDDING_MODEL, ):     label_embeddings = [get_embedding(label, engine=model) for label in labels]      def label_score(review_embedding, label_embeddings):         return cosine_similarity(review_embedding, label_embeddings[1]) - cosine_similarity(review_embedding, label_embeddings[0])      probas = df[\"embedding\"].apply(lambda x: label_score(x, label_embeddings))     preds = probas.apply(lambda x: 'positive' if x&gt;0 else 'negative')      report = classification_report(df.sentiment, preds)     print(report)      display = PrecisionRecallDisplay.from_predictions(df.sentiment, probas, pos_label='positive')     _ = display.ax_.set_title(\"2-class Precision-Recall curve\")  evaluate_embeddings_approach(labels=['negative', 'positive'], model=EMBEDDING_MODEL) <pre>              precision    recall  f1-score   support\n\n    negative       0.61      0.88      0.72       136\n    positive       0.98      0.90      0.94       789\n\n    accuracy                           0.90       925\n   macro avg       0.79      0.89      0.83       925\nweighted avg       0.92      0.90      0.91       925\n\n</pre> <p>We can see that this classifier already performs extremely well. We used similarity embeddings, and the simplest possible label name. Let's try to improve on this by using more descriptive label names, and search embeddings.</p> In\u00a0[3]: Copied! <pre>evaluate_embeddings_approach(labels=['An Amazon review with a negative sentiment.', 'An Amazon review with a positive sentiment.'])\n</pre> evaluate_embeddings_approach(labels=['An Amazon review with a negative sentiment.', 'An Amazon review with a positive sentiment.']) <pre>              precision    recall  f1-score   support\n\n    negative       0.98      0.73      0.84       136\n    positive       0.96      1.00      0.98       789\n\n    accuracy                           0.96       925\n   macro avg       0.97      0.86      0.91       925\nweighted avg       0.96      0.96      0.96       925\n\n</pre> <p>Using the search embeddings and descriptive names leads to an additional improvement in performance.</p> In\u00a0[4]: Copied! <pre>evaluate_embeddings_approach(labels=['An Amazon review with a negative sentiment.', 'An Amazon review with a positive sentiment.'])\n</pre> evaluate_embeddings_approach(labels=['An Amazon review with a negative sentiment.', 'An Amazon review with a positive sentiment.']) <pre>              precision    recall  f1-score   support\n\n    negative       0.98      0.73      0.84       136\n    positive       0.96      1.00      0.98       789\n\n    accuracy                           0.96       925\n   macro avg       0.97      0.86      0.91       925\nweighted avg       0.96      0.96      0.96       925\n\n</pre> <p>As shown above, zero-shot classification with embeddings can lead to great results, especially when the labels are more descriptive than just simple words.</p>"},{"location":"examples/Zero-shot_classification_with_embeddings/#Zero-shot-classification-with-embeddings","title":"Zero-shot classification with embeddings\u00b6","text":"<p>In this notebook we will classify the sentiment of reviews using embeddings and zero labeled data! The dataset is created in the Obtain_dataset Notebook.</p> <p>We'll define positive sentiment to be 4- and 5-star reviews, and negative sentiment to be 1- and 2-star reviews. 3-star reviews are considered neutral and we won't use them for this example.</p> <p>We will perform zero-shot classification by embedding descriptions of each class and then comparing new samples to those class embeddings.</p>"},{"location":"examples/Zero-shot_classification_with_embeddings/#Zero-Shot-Classification","title":"Zero-Shot Classification\u00b6","text":"<p>To perform zero shot classification, we want to predict labels for our samples without any training. To do this, we can simply embed short descriptions of each label, such as positive and negative, and then compare the cosine distance between embeddings of samples and label descriptions.</p> <p>The highest similarity label to the sample input is the predicted label. We can also define a prediction score to be the difference between the cosine distance to the positive and to the negative label. This score can be used for plotting a precision-recall curve, which can be used to select a different tradeoff between precision and recall, by selecting a different threshold.</p>"},{"location":"examples/api_request_parallel_processor/","title":"Api request parallel processor","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nAPI REQUEST PARALLEL PROCESSOR\n\nUsing the OpenAI API to process lots of text quickly takes some care.\nIf you trickle in a million API requests one by one, they'll take days to complete.\nIf you flood a million API requests in parallel, they'll exceed the rate limits and fail with errors.\nTo maximize throughput, parallel requests need to be throttled to stay under rate limits.\n\nThis script parallelizes requests to the OpenAI API while throttling to stay under rate limits.\n\nFeatures:\n- Streams requests from file, to avoid running out of memory for giant jobs\n- Makes requests concurrently, to maximize throughput\n- Throttles request and token usage, to stay under rate limits\n- Retries failed requests up to {max_attempts} times, to avoid missing data\n- Logs errors, to diagnose problems with requests\n\nExample command to call script:\n```\npython examples/api_request_parallel_processor.py \\\n  --requests_filepath examples/data/example_requests_to_parallel_process.jsonl \\\n  --save_filepath examples/data/example_requests_to_parallel_process_results.jsonl \\\n  --request_url https://api.openai.com/v1/embeddings \\\n  --max_requests_per_minute 1500 \\\n  --max_tokens_per_minute 6250000 \\\n  --token_encoding_name cl100k_base \\\n  --max_attempts 5 \\\n  --logging_level 20\n```\n\nInputs:\n- requests_filepath : str\n    - path to the file containing the requests to be processed\n    - file should be a jsonl file, where each line is a json object with API parameters\n    - e.g., {\"model\": \"text-embedding-ada-002\", \"input\": \"embed me\"}\n    - as with all jsonl files, take care that newlines in the content are properly escaped (json.dumps does this automatically)\n    - an example file is provided at examples/data/example_requests_to_parallel_process.jsonl\n    - the code to generate the example file is appended to the bottom of this script\n- save_filepath : str, optional\n    - path to the file where the results will be saved\n    - file will be a jsonl file, where each line is an array with the original request plus the API response\n    - e.g., [{\"model\": \"text-embedding-ada-002\", \"input\": \"embed me\"}, {...}]\n    - if omitted, results will be saved to {requests_filename}_results.jsonl\n- request_url : str, optional\n    - URL of the API endpoint to call\n    - if omitted, will default to \"https://api.openai.com/v1/embeddings\"\n- api_key : str, optional\n    - API key to use\n    - if omitted, the script will attempt to read it from an environment variable {os.getenv(\"OPENAI_API_KEY\")}\n- max_requests_per_minute : float, optional\n    - target number of requests to make per minute (will make less if limited by tokens)\n    - leave headroom by setting this to 50% or 75% of your limit\n    - if requests are limiting you, try batching multiple embeddings or completions into one request\n    - if omitted, will default to 1,500\n- max_tokens_per_minute : float, optional\n    - target number of tokens to use per minute (will use less if limited by requests)\n    - leave headroom by setting this to 50% or 75% of your limit\n    - if omitted, will default to 125,000\n- token_encoding_name : str, optional\n    - name of the token encoding used, as defined in the `tiktoken` package\n    - if omitted, will default to \"cl100k_base\" (used by `text-embedding-ada-002`)\n- max_attempts : int, optional\n    - number of times to retry a failed request before giving up\n    - if omitted, will default to 5\n- logging_level : int, optional\n    - level of logging to use; higher numbers will log fewer messages\n    - 40 = ERROR; will log only when requests fail after all retries\n    - 30 = WARNING; will log when requests his rate limits or other errors\n    - 20 = INFO; will log when requests start and the status at finish\n    - 10 = DEBUG; will log various things as the loop runs to see when they occur\n    - if omitted, will default to 20 (INFO).\n\nThe script is structured as follows:\n    - Imports\n    - Define main()\n        - Initialize things\n        - In main loop:\n            - Get next request if one is not already waiting for capacity\n            - Update available token &amp; request capacity\n            - If enough capacity available, call API\n            - The loop pauses if a rate limit error is hit\n            - The loop breaks when no tasks remain\n    - Define dataclasses\n        - StatusTracker (stores script metadata counters; only one instance is created)\n        - APIRequest (stores API inputs, outputs, metadata; one method to call API)\n    - Define functions\n        - api_endpoint_from_url (extracts API endpoint from request URL)\n        - append_to_jsonl (writes to results file)\n        - num_tokens_consumed_from_request (bigger function to infer token usage from request)\n        - task_id_generator_function (yields 1, 2, 3, ...)\n    - Run main()\n\"\"\"\n</pre> \"\"\" API REQUEST PARALLEL PROCESSOR  Using the OpenAI API to process lots of text quickly takes some care. If you trickle in a million API requests one by one, they'll take days to complete. If you flood a million API requests in parallel, they'll exceed the rate limits and fail with errors. To maximize throughput, parallel requests need to be throttled to stay under rate limits.  This script parallelizes requests to the OpenAI API while throttling to stay under rate limits.  Features: - Streams requests from file, to avoid running out of memory for giant jobs - Makes requests concurrently, to maximize throughput - Throttles request and token usage, to stay under rate limits - Retries failed requests up to {max_attempts} times, to avoid missing data - Logs errors, to diagnose problems with requests  Example command to call script: ``` python examples/api_request_parallel_processor.py \\   --requests_filepath examples/data/example_requests_to_parallel_process.jsonl \\   --save_filepath examples/data/example_requests_to_parallel_process_results.jsonl \\   --request_url https://api.openai.com/v1/embeddings \\   --max_requests_per_minute 1500 \\   --max_tokens_per_minute 6250000 \\   --token_encoding_name cl100k_base \\   --max_attempts 5 \\   --logging_level 20 ```  Inputs: - requests_filepath : str     - path to the file containing the requests to be processed     - file should be a jsonl file, where each line is a json object with API parameters     - e.g., {\"model\": \"text-embedding-ada-002\", \"input\": \"embed me\"}     - as with all jsonl files, take care that newlines in the content are properly escaped (json.dumps does this automatically)     - an example file is provided at examples/data/example_requests_to_parallel_process.jsonl     - the code to generate the example file is appended to the bottom of this script - save_filepath : str, optional     - path to the file where the results will be saved     - file will be a jsonl file, where each line is an array with the original request plus the API response     - e.g., [{\"model\": \"text-embedding-ada-002\", \"input\": \"embed me\"}, {...}]     - if omitted, results will be saved to {requests_filename}_results.jsonl - request_url : str, optional     - URL of the API endpoint to call     - if omitted, will default to \"https://api.openai.com/v1/embeddings\" - api_key : str, optional     - API key to use     - if omitted, the script will attempt to read it from an environment variable {os.getenv(\"OPENAI_API_KEY\")} - max_requests_per_minute : float, optional     - target number of requests to make per minute (will make less if limited by tokens)     - leave headroom by setting this to 50% or 75% of your limit     - if requests are limiting you, try batching multiple embeddings or completions into one request     - if omitted, will default to 1,500 - max_tokens_per_minute : float, optional     - target number of tokens to use per minute (will use less if limited by requests)     - leave headroom by setting this to 50% or 75% of your limit     - if omitted, will default to 125,000 - token_encoding_name : str, optional     - name of the token encoding used, as defined in the `tiktoken` package     - if omitted, will default to \"cl100k_base\" (used by `text-embedding-ada-002`) - max_attempts : int, optional     - number of times to retry a failed request before giving up     - if omitted, will default to 5 - logging_level : int, optional     - level of logging to use; higher numbers will log fewer messages     - 40 = ERROR; will log only when requests fail after all retries     - 30 = WARNING; will log when requests his rate limits or other errors     - 20 = INFO; will log when requests start and the status at finish     - 10 = DEBUG; will log various things as the loop runs to see when they occur     - if omitted, will default to 20 (INFO).  The script is structured as follows:     - Imports     - Define main()         - Initialize things         - In main loop:             - Get next request if one is not already waiting for capacity             - Update available token &amp; request capacity             - If enough capacity available, call API             - The loop pauses if a rate limit error is hit             - The loop breaks when no tasks remain     - Define dataclasses         - StatusTracker (stores script metadata counters; only one instance is created)         - APIRequest (stores API inputs, outputs, metadata; one method to call API)     - Define functions         - api_endpoint_from_url (extracts API endpoint from request URL)         - append_to_jsonl (writes to results file)         - num_tokens_consumed_from_request (bigger function to infer token usage from request)         - task_id_generator_function (yields 1, 2, 3, ...)     - Run main() \"\"\" In\u00a0[\u00a0]: Copied! <pre># imports\nimport aiohttp  # for making API calls concurrently\nimport argparse  # for running script from command line\nimport asyncio  # for running API calls concurrently\nimport json  # for saving results to a jsonl file\nimport logging  # for logging rate limit warnings and other messages\nimport os  # for reading API key\nimport tiktoken  # for counting tokens\nimport time  # for sleeping after rate limit is hit\nfrom dataclasses import dataclass  # for storing API inputs, outputs, and metadata\n</pre> # imports import aiohttp  # for making API calls concurrently import argparse  # for running script from command line import asyncio  # for running API calls concurrently import json  # for saving results to a jsonl file import logging  # for logging rate limit warnings and other messages import os  # for reading API key import tiktoken  # for counting tokens import time  # for sleeping after rate limit is hit from dataclasses import dataclass  # for storing API inputs, outputs, and metadata In\u00a0[\u00a0]: Copied! <pre>async def process_api_requests_from_file(\n    requests_filepath: str,\n    save_filepath: str,\n    request_url: str,\n    api_key: str,\n    max_requests_per_minute: float,\n    max_tokens_per_minute: float,\n    token_encoding_name: str,\n    max_attempts: int,\n    logging_level: int,\n):\n\"\"\"Processes API requests in parallel, throttling to stay under rate limits.\"\"\"\n    # constants\n    seconds_to_pause_after_rate_limit_error = 15\n    seconds_to_sleep_each_loop = 0.001  # 1 ms limits max throughput to 1,000 requests per second\n\n    # initialize logging\n    logging.basicConfig(level=logging_level)\n    logging.debug(f\"Logging initialized at level {logging_level}\")\n\n    # infer API endpoint and construct request header\n    api_endpoint = api_endpoint_from_url(request_url)\n    request_header = {\"Authorization\": f\"Bearer {api_key}\"}\n\n    # initialize trackers\n    queue_of_requests_to_retry = asyncio.Queue()\n    task_id_generator = task_id_generator_function()  # generates integer IDs of 1, 2, 3, ...\n    status_tracker = StatusTracker()  # single instance to track a collection of variables\n    next_request = None  # variable to hold the next request to call\n\n    # initialize available capacity counts\n    available_request_capacity = max_requests_per_minute\n    available_token_capacity = max_tokens_per_minute\n    last_update_time = time.time()\n\n    # intialize flags\n    file_not_finished = True  # after file is empty, we'll skip reading it\n    logging.debug(f\"Initialization complete.\")\n\n    # initialize file reading\n    with open(requests_filepath) as file:\n        # `requests` will provide requests one at a time\n        requests = file.__iter__()\n        logging.debug(f\"File opened. Entering main loop\")\n\n        while True:\n            # get next request (if one is not already waiting for capacity)\n            if next_request is None:\n                if queue_of_requests_to_retry.empty() is False:\n                    next_request = queue_of_requests_to_retry.get_nowait()\n                    logging.debug(f\"Retrying request {next_request.task_id}: {next_request}\")\n                elif file_not_finished:\n                    try:\n                        # get new request\n                        request_json = eval(next(requests))\n                        next_request = APIRequest(\n                            task_id=next(task_id_generator),\n                            request_json=request_json,\n                            token_consumption=num_tokens_consumed_from_request(request_json, api_endpoint, token_encoding_name),\n                            attempts_left=max_attempts,\n                        )\n                        status_tracker.num_tasks_started += 1\n                        status_tracker.num_tasks_in_progress += 1\n                        logging.debug(f\"Reading request {next_request.task_id}: {next_request}\")\n                    except StopIteration:\n                        # if file runs out, set flag to stop reading it\n                        logging.debug(\"Read file exhausted\")\n                        file_not_finished = False\n\n            # update available capacity\n            current_time = time.time()\n            seconds_since_update = current_time - last_update_time\n            available_request_capacity = min(\n                available_request_capacity + max_requests_per_minute * seconds_since_update / 60.0,\n                max_requests_per_minute,\n            )\n            available_token_capacity = min(\n                available_token_capacity + max_tokens_per_minute * seconds_since_update / 60.0,\n                max_tokens_per_minute,\n            )\n            last_update_time = current_time\n\n            # if enough capacity available, call API\n            if next_request:\n                next_request_tokens = next_request.token_consumption\n                if (\n                    available_request_capacity &gt;= 1\n                    and available_token_capacity &gt;= next_request_tokens\n                ):\n                    # update counters\n                    available_request_capacity -= 1\n                    available_token_capacity -= next_request_tokens\n                    next_request.attempts_left -= 1\n\n                    # call API\n                    asyncio.create_task(\n                        next_request.call_API(\n                            request_url=request_url,\n                            request_header=request_header,\n                            retry_queue=queue_of_requests_to_retry,\n                            save_filepath=save_filepath,\n                            status_tracker=status_tracker,\n                        )\n                    )\n                    next_request = None  # reset next_request to empty\n\n            # if all tasks are finished, break\n            if status_tracker.num_tasks_in_progress == 0:\n                break\n\n            # main loop sleeps briefly so concurrent tasks can run\n            await asyncio.sleep(seconds_to_sleep_each_loop)\n\n            # if a rate limit error was hit recently, pause to cool down\n            seconds_since_rate_limit_error = (time.time() - status_tracker.time_of_last_rate_limit_error)\n            if seconds_since_rate_limit_error &lt; seconds_to_pause_after_rate_limit_error:\n                remaining_seconds_to_pause = (seconds_to_pause_after_rate_limit_error - seconds_since_rate_limit_error)\n                await asyncio.sleep(remaining_seconds_to_pause)\n                # ^e.g., if pause is 15 seconds and final limit was hit 5 seconds ago\n                logging.warn(f\"Pausing to cool down until {time.ctime(status_tracker.time_of_last_rate_limit_error + seconds_to_pause_after_rate_limit_error)}\")\n\n        # after finishing, log final status\n        logging.info(f\"\"\"Parallel processing complete. Results saved to {save_filepath}\"\"\")\n        if status_tracker.num_tasks_failed &gt; 0:\n            logging.warning(f\"{status_tracker.num_tasks_failed} / {status_tracker.num_tasks_started} requests failed. Errors logged to {save_filepath}.\")\n        if status_tracker.num_rate_limit_errors &gt; 0:\n            logging.warning(f\"{status_tracker.num_rate_limit_errors} rate limit errors received. Consider running at a lower rate.\")\n</pre> async def process_api_requests_from_file(     requests_filepath: str,     save_filepath: str,     request_url: str,     api_key: str,     max_requests_per_minute: float,     max_tokens_per_minute: float,     token_encoding_name: str,     max_attempts: int,     logging_level: int, ):     \"\"\"Processes API requests in parallel, throttling to stay under rate limits.\"\"\"     # constants     seconds_to_pause_after_rate_limit_error = 15     seconds_to_sleep_each_loop = 0.001  # 1 ms limits max throughput to 1,000 requests per second      # initialize logging     logging.basicConfig(level=logging_level)     logging.debug(f\"Logging initialized at level {logging_level}\")      # infer API endpoint and construct request header     api_endpoint = api_endpoint_from_url(request_url)     request_header = {\"Authorization\": f\"Bearer {api_key}\"}      # initialize trackers     queue_of_requests_to_retry = asyncio.Queue()     task_id_generator = task_id_generator_function()  # generates integer IDs of 1, 2, 3, ...     status_tracker = StatusTracker()  # single instance to track a collection of variables     next_request = None  # variable to hold the next request to call      # initialize available capacity counts     available_request_capacity = max_requests_per_minute     available_token_capacity = max_tokens_per_minute     last_update_time = time.time()      # intialize flags     file_not_finished = True  # after file is empty, we'll skip reading it     logging.debug(f\"Initialization complete.\")      # initialize file reading     with open(requests_filepath) as file:         # `requests` will provide requests one at a time         requests = file.__iter__()         logging.debug(f\"File opened. Entering main loop\")          while True:             # get next request (if one is not already waiting for capacity)             if next_request is None:                 if queue_of_requests_to_retry.empty() is False:                     next_request = queue_of_requests_to_retry.get_nowait()                     logging.debug(f\"Retrying request {next_request.task_id}: {next_request}\")                 elif file_not_finished:                     try:                         # get new request                         request_json = eval(next(requests))                         next_request = APIRequest(                             task_id=next(task_id_generator),                             request_json=request_json,                             token_consumption=num_tokens_consumed_from_request(request_json, api_endpoint, token_encoding_name),                             attempts_left=max_attempts,                         )                         status_tracker.num_tasks_started += 1                         status_tracker.num_tasks_in_progress += 1                         logging.debug(f\"Reading request {next_request.task_id}: {next_request}\")                     except StopIteration:                         # if file runs out, set flag to stop reading it                         logging.debug(\"Read file exhausted\")                         file_not_finished = False              # update available capacity             current_time = time.time()             seconds_since_update = current_time - last_update_time             available_request_capacity = min(                 available_request_capacity + max_requests_per_minute * seconds_since_update / 60.0,                 max_requests_per_minute,             )             available_token_capacity = min(                 available_token_capacity + max_tokens_per_minute * seconds_since_update / 60.0,                 max_tokens_per_minute,             )             last_update_time = current_time              # if enough capacity available, call API             if next_request:                 next_request_tokens = next_request.token_consumption                 if (                     available_request_capacity &gt;= 1                     and available_token_capacity &gt;= next_request_tokens                 ):                     # update counters                     available_request_capacity -= 1                     available_token_capacity -= next_request_tokens                     next_request.attempts_left -= 1                      # call API                     asyncio.create_task(                         next_request.call_API(                             request_url=request_url,                             request_header=request_header,                             retry_queue=queue_of_requests_to_retry,                             save_filepath=save_filepath,                             status_tracker=status_tracker,                         )                     )                     next_request = None  # reset next_request to empty              # if all tasks are finished, break             if status_tracker.num_tasks_in_progress == 0:                 break              # main loop sleeps briefly so concurrent tasks can run             await asyncio.sleep(seconds_to_sleep_each_loop)              # if a rate limit error was hit recently, pause to cool down             seconds_since_rate_limit_error = (time.time() - status_tracker.time_of_last_rate_limit_error)             if seconds_since_rate_limit_error &lt; seconds_to_pause_after_rate_limit_error:                 remaining_seconds_to_pause = (seconds_to_pause_after_rate_limit_error - seconds_since_rate_limit_error)                 await asyncio.sleep(remaining_seconds_to_pause)                 # ^e.g., if pause is 15 seconds and final limit was hit 5 seconds ago                 logging.warn(f\"Pausing to cool down until {time.ctime(status_tracker.time_of_last_rate_limit_error + seconds_to_pause_after_rate_limit_error)}\")          # after finishing, log final status         logging.info(f\"\"\"Parallel processing complete. Results saved to {save_filepath}\"\"\")         if status_tracker.num_tasks_failed &gt; 0:             logging.warning(f\"{status_tracker.num_tasks_failed} / {status_tracker.num_tasks_started} requests failed. Errors logged to {save_filepath}.\")         if status_tracker.num_rate_limit_errors &gt; 0:             logging.warning(f\"{status_tracker.num_rate_limit_errors} rate limit errors received. Consider running at a lower rate.\") <p>dataclasses</p> In\u00a0[\u00a0]: Copied! <pre>@dataclass\nclass StatusTracker:\n\"\"\"Stores metadata about the script's progress. Only one instance is created.\"\"\"\n\n    num_tasks_started: int = 0\n    num_tasks_in_progress: int = 0  # script ends when this reaches 0\n    num_tasks_succeeded: int = 0\n    num_tasks_failed: int = 0\n    num_rate_limit_errors: int = 0\n    num_api_errors: int = 0  # excluding rate limit errors, counted above\n    num_other_errors: int = 0\n    time_of_last_rate_limit_error: int = 0  # used to cool off after hitting rate limits\n</pre> @dataclass class StatusTracker:     \"\"\"Stores metadata about the script's progress. Only one instance is created.\"\"\"      num_tasks_started: int = 0     num_tasks_in_progress: int = 0  # script ends when this reaches 0     num_tasks_succeeded: int = 0     num_tasks_failed: int = 0     num_rate_limit_errors: int = 0     num_api_errors: int = 0  # excluding rate limit errors, counted above     num_other_errors: int = 0     time_of_last_rate_limit_error: int = 0  # used to cool off after hitting rate limits In\u00a0[\u00a0]: Copied! <pre>@dataclass\nclass APIRequest:\n\"\"\"Stores an API request's inputs, outputs, and other metadata. Contains a method to make an API call.\"\"\"\n\n    task_id: int\n    request_json: dict\n    token_consumption: int\n    attempts_left: int\n    result = []\n\n    async def call_API(\n        self,\n        request_url: str,\n        request_header: dict,\n        retry_queue: asyncio.Queue,\n        save_filepath: str,\n        status_tracker: StatusTracker,\n    ):\n\"\"\"Calls the OpenAI API and saves results.\"\"\"\n        logging.info(f\"Starting request #{self.task_id}\")\n        error = None\n        try:\n            async with aiohttp.ClientSession() as session:\n                async with session.post(\n                    url=request_url, headers=request_header, json=self.request_json\n                ) as response:\n                    response = await response.json()\n            if \"error\" in response:\n                logging.warning(\n                    f\"Request {self.task_id} failed with error {response['error']}\"\n                )\n                status_tracker.num_api_errors += 1\n                error = response\n                if \"Rate limit\" in response[\"error\"].get(\"message\", \"\"):\n                    status_tracker.time_of_last_rate_limit_error = time.time()\n                    status_tracker.num_rate_limit_errors += 1\n                    status_tracker.num_api_errors -= 1  # rate limit errors are counted separately\n\n        except Exception as e:  # catching naked exceptions is bad practice, but in this case we'll log &amp; save them\n            logging.warning(f\"Request {self.task_id} failed with Exception {e}\")\n            status_tracker.num_other_errors += 1\n            error = e\n        if error:\n            self.result.append(error)\n            if self.attempts_left:\n                retry_queue.put_nowait(self)\n            else:\n                logging.error(f\"Request {self.request_json} failed after all attempts. Saving errors: {self.result}\")\n                append_to_jsonl([self.request_json, self.result], save_filepath)\n                status_tracker.num_tasks_in_progress -= 1\n                status_tracker.num_tasks_failed += 1\n        else:\n            append_to_jsonl([self.request_json, response], save_filepath)\n            status_tracker.num_tasks_in_progress -= 1\n            status_tracker.num_tasks_succeeded += 1\n            logging.debug(f\"Request {self.task_id} saved to {save_filepath}\")\n</pre> @dataclass class APIRequest:     \"\"\"Stores an API request's inputs, outputs, and other metadata. Contains a method to make an API call.\"\"\"      task_id: int     request_json: dict     token_consumption: int     attempts_left: int     result = []      async def call_API(         self,         request_url: str,         request_header: dict,         retry_queue: asyncio.Queue,         save_filepath: str,         status_tracker: StatusTracker,     ):         \"\"\"Calls the OpenAI API and saves results.\"\"\"         logging.info(f\"Starting request #{self.task_id}\")         error = None         try:             async with aiohttp.ClientSession() as session:                 async with session.post(                     url=request_url, headers=request_header, json=self.request_json                 ) as response:                     response = await response.json()             if \"error\" in response:                 logging.warning(                     f\"Request {self.task_id} failed with error {response['error']}\"                 )                 status_tracker.num_api_errors += 1                 error = response                 if \"Rate limit\" in response[\"error\"].get(\"message\", \"\"):                     status_tracker.time_of_last_rate_limit_error = time.time()                     status_tracker.num_rate_limit_errors += 1                     status_tracker.num_api_errors -= 1  # rate limit errors are counted separately          except Exception as e:  # catching naked exceptions is bad practice, but in this case we'll log &amp; save them             logging.warning(f\"Request {self.task_id} failed with Exception {e}\")             status_tracker.num_other_errors += 1             error = e         if error:             self.result.append(error)             if self.attempts_left:                 retry_queue.put_nowait(self)             else:                 logging.error(f\"Request {self.request_json} failed after all attempts. Saving errors: {self.result}\")                 append_to_jsonl([self.request_json, self.result], save_filepath)                 status_tracker.num_tasks_in_progress -= 1                 status_tracker.num_tasks_failed += 1         else:             append_to_jsonl([self.request_json, response], save_filepath)             status_tracker.num_tasks_in_progress -= 1             status_tracker.num_tasks_succeeded += 1             logging.debug(f\"Request {self.task_id} saved to {save_filepath}\") <p>functions</p> In\u00a0[\u00a0]: Copied! <pre>def api_endpoint_from_url(request_url):\n\"\"\"Extract the API endpoint from the request URL.\"\"\"\n    return request_url.split(\"/\")[-1]\n</pre> def api_endpoint_from_url(request_url):     \"\"\"Extract the API endpoint from the request URL.\"\"\"     return request_url.split(\"/\")[-1] In\u00a0[\u00a0]: Copied! <pre>def append_to_jsonl(data, filename: str) -&gt; None:\n\"\"\"Append a json payload to the end of a jsonl file.\"\"\"\n    json_string = json.dumps(data)\n    with open(filename, \"a\") as f:\n        f.write(json_string + \"\\n\")\n</pre> def append_to_jsonl(data, filename: str) -&gt; None:     \"\"\"Append a json payload to the end of a jsonl file.\"\"\"     json_string = json.dumps(data)     with open(filename, \"a\") as f:         f.write(json_string + \"\\n\") In\u00a0[\u00a0]: Copied! <pre>def num_tokens_consumed_from_request(\n    request_json: dict,\n    api_endpoint: str,\n    token_encoding_name: str,\n):\n\"\"\"Count the number of tokens in the request. Only supports completion and embedding requests.\"\"\"\n    encoding = tiktoken.get_encoding(token_encoding_name)\n    # if completions request, tokens = prompt + n * max_tokens\n    if api_endpoint == \"completions\":\n        prompt = request_json[\"prompt\"]\n        max_tokens = request_json.get(\"max_tokens\", 15)\n        n = request_json.get(\"n\", 1)\n        completion_tokens = n * max_tokens\n        if isinstance(prompt, str):  # single prompt\n            prompt_tokens = len(encoding.encode(prompt))\n            num_tokens = prompt_tokens + completion_tokens\n            return num_tokens\n        elif isinstance(prompt, list):  # multiple prompts\n            prompt_tokens = sum([len(encoding.encode(p)) for p in prompt])\n            num_tokens = prompt_tokens + completion_tokens * len(prompt)\n            return num_tokens\n        else:\n            raise TypeError('Expecting either string or list of strings for \"prompt\" field in completion request')\n    # if embeddings request, tokens = input tokens\n    elif api_endpoint == \"embeddings\":\n        input = request_json[\"input\"]\n        if isinstance(input, str):  # single input\n            num_tokens = len(encoding.encode(input))\n            return num_tokens\n        elif isinstance(input, list):  # multiple inputs\n            num_tokens = sum([len(encoding.encode(i)) for i in input])\n            return num_tokens\n        else:\n            raise TypeError('Expecting either string or list of strings for \"inputs\" field in embedding request')\n    # more logic needed to support other API calls (e.g., edits, inserts, DALL-E)\n    else:\n        raise NotImplementedError(f'API endpoint \"{api_endpoint}\" not implemented in this script')\n</pre> def num_tokens_consumed_from_request(     request_json: dict,     api_endpoint: str,     token_encoding_name: str, ):     \"\"\"Count the number of tokens in the request. Only supports completion and embedding requests.\"\"\"     encoding = tiktoken.get_encoding(token_encoding_name)     # if completions request, tokens = prompt + n * max_tokens     if api_endpoint == \"completions\":         prompt = request_json[\"prompt\"]         max_tokens = request_json.get(\"max_tokens\", 15)         n = request_json.get(\"n\", 1)         completion_tokens = n * max_tokens         if isinstance(prompt, str):  # single prompt             prompt_tokens = len(encoding.encode(prompt))             num_tokens = prompt_tokens + completion_tokens             return num_tokens         elif isinstance(prompt, list):  # multiple prompts             prompt_tokens = sum([len(encoding.encode(p)) for p in prompt])             num_tokens = prompt_tokens + completion_tokens * len(prompt)             return num_tokens         else:             raise TypeError('Expecting either string or list of strings for \"prompt\" field in completion request')     # if embeddings request, tokens = input tokens     elif api_endpoint == \"embeddings\":         input = request_json[\"input\"]         if isinstance(input, str):  # single input             num_tokens = len(encoding.encode(input))             return num_tokens         elif isinstance(input, list):  # multiple inputs             num_tokens = sum([len(encoding.encode(i)) for i in input])             return num_tokens         else:             raise TypeError('Expecting either string or list of strings for \"inputs\" field in embedding request')     # more logic needed to support other API calls (e.g., edits, inserts, DALL-E)     else:         raise NotImplementedError(f'API endpoint \"{api_endpoint}\" not implemented in this script') In\u00a0[\u00a0]: Copied! <pre>def task_id_generator_function():\n\"\"\"Generate integers 0, 1, 2, and so on.\"\"\"\n    task_id = 0\n    while True:\n        yield task_id\n        task_id += 1\n</pre> def task_id_generator_function():     \"\"\"Generate integers 0, 1, 2, and so on.\"\"\"     task_id = 0     while True:         yield task_id         task_id += 1 <p>run script</p> In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    # parse command line arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--requests_filepath\")\n    parser.add_argument(\"--save_filepath\", default=None)\n    parser.add_argument(\"--request_url\", default=\"https://api.openai.com/v1/embeddings\")\n    parser.add_argument(\"--api_key\", default=os.getenv(\"OPENAI_API_KEY\"))\n    parser.add_argument(\"--max_requests_per_minute\", type=int, default=3_000 * 0.5)\n    parser.add_argument(\"--max_tokens_per_minute\", type=int, default=250_000 * 0.5)\n    parser.add_argument(\"--token_encoding_name\", default=\"cl100k_base\")\n    parser.add_argument(\"--max_attempts\", type=int, default=5)\n    parser.add_argument(\"--logging_level\", default=logging.INFO)\n    args = parser.parse_args()\n\n    if args.save_filepath is None:\n        args.save_filepath = args.requests_filepath.replace(\".jsonl\", \"_results.jsonl\")\n\n    # run script\n    asyncio.run(\n        process_api_requests_from_file(\n            requests_filepath=args.requests_filepath,\n            save_filepath=args.save_filepath,\n            request_url=args.request_url,\n            api_key=args.api_key,\n            max_requests_per_minute=float(args.max_requests_per_minute),\n            max_tokens_per_minute=float(args.max_tokens_per_minute),\n            token_encoding_name=args.token_encoding_name,\n            max_attempts=int(args.max_attempts),\n            logging_level=int(args.logging_level),\n        )\n    )\n</pre> if __name__ == \"__main__\":     # parse command line arguments     parser = argparse.ArgumentParser()     parser.add_argument(\"--requests_filepath\")     parser.add_argument(\"--save_filepath\", default=None)     parser.add_argument(\"--request_url\", default=\"https://api.openai.com/v1/embeddings\")     parser.add_argument(\"--api_key\", default=os.getenv(\"OPENAI_API_KEY\"))     parser.add_argument(\"--max_requests_per_minute\", type=int, default=3_000 * 0.5)     parser.add_argument(\"--max_tokens_per_minute\", type=int, default=250_000 * 0.5)     parser.add_argument(\"--token_encoding_name\", default=\"cl100k_base\")     parser.add_argument(\"--max_attempts\", type=int, default=5)     parser.add_argument(\"--logging_level\", default=logging.INFO)     args = parser.parse_args()      if args.save_filepath is None:         args.save_filepath = args.requests_filepath.replace(\".jsonl\", \"_results.jsonl\")      # run script     asyncio.run(         process_api_requests_from_file(             requests_filepath=args.requests_filepath,             save_filepath=args.save_filepath,             request_url=args.request_url,             api_key=args.api_key,             max_requests_per_minute=float(args.max_requests_per_minute),             max_tokens_per_minute=float(args.max_tokens_per_minute),             token_encoding_name=args.token_encoding_name,             max_attempts=int(args.max_attempts),             logging_level=int(args.logging_level),         )     ) In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nAPPENDIX\n\nThe example requests file at openai-cookbook/examples/data/example_requests_to_parallel_process.jsonl contains 10,000 requests to text-embedding-ada-002.\n\nIt was generated with the following code:\n\n```python\nimport json\n\nfilename = \"data/example_requests_to_parallel_process.jsonl\"\nn_requests = 10_000\njobs = [{\"model\": \"text-embedding-ada-002\", \"input\": str(x) + \"\\n\"} for x in range(n_requests)]\nwith open(filename, \"w\") as f:\n    for job in jobs:\n        json_string = json.dumps(job)\n        f.write(json_string + \"\\n\")\n```\n\nAs with all jsonl files, take care that newlines in the content are properly escaped (json.dumps does this automatically).\n\"\"\"\n</pre> \"\"\" APPENDIX  The example requests file at openai-cookbook/examples/data/example_requests_to_parallel_process.jsonl contains 10,000 requests to text-embedding-ada-002.  It was generated with the following code:  ```python import json  filename = \"data/example_requests_to_parallel_process.jsonl\" n_requests = 10_000 jobs = [{\"model\": \"text-embedding-ada-002\", \"input\": str(x) + \"\\n\"} for x in range(n_requests)] with open(filename, \"w\") as f:     for job in jobs:         json_string = json.dumps(job)         f.write(json_string + \"\\n\") ```  As with all jsonl files, take care that newlines in the content are properly escaped (json.dumps does this automatically). \"\"\""},{"location":"examples/azure/completions/","title":"Azure completions example","text":"In\u00a0[\u00a0]: Copied! <pre>import openai\nfrom openai import cli\n</pre> import openai from openai import cli In\u00a0[\u00a0]: Copied! <pre>openai.api_version = '2022-12-01'\nopenai.api_base = '' # Please add your endpoint here\n</pre> openai.api_version = '2022-12-01' openai.api_base = '' # Please add your endpoint here <p>We next have to setup the <code>api_type</code> and <code>api_key</code>. We can either get the key from the portal or we can get it through Microsoft Active Directory Authentication. Depending on this the <code>api_type</code> is either <code>azure</code> or <code>azure_ad</code>.</p> In\u00a0[\u00a0]: Copied! <pre>openai.api_type = 'azure'\nopenai.api_key = ''  # Please add your api key here\n</pre> openai.api_type = 'azure' openai.api_key = ''  # Please add your api key here In\u00a0[\u00a0]: Copied! <pre># from azure.identity import DefaultAzureCredential\n\n# default_credential = DefaultAzureCredential()\n# token = default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n\n# openai.api_type = 'azure_ad'\n# openai.api_key = token.token\n</pre> # from azure.identity import DefaultAzureCredential  # default_credential = DefaultAzureCredential() # token = default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")  # openai.api_type = 'azure_ad' # openai.api_key = token.token In\u00a0[\u00a0]: Copied! <pre>model = \"text-davinci-002\"\n\n# Now let's create the deployment\nprint(f'Creating a new deployment with model: {model}')\nresult = openai.Deployment.create(model=model, scale_settings={\"scale_type\":\"standard\"})\ndeployment_id = result[\"id\"]\nprint(f'Successfully created deployment with id: {deployment_id}')\n</pre> model = \"text-davinci-002\"  # Now let's create the deployment print(f'Creating a new deployment with model: {model}') result = openai.Deployment.create(model=model, scale_settings={\"scale_type\":\"standard\"}) deployment_id = result[\"id\"] print(f'Successfully created deployment with id: {deployment_id}') In\u00a0[\u00a0]: Copied! <pre>print(f'Checking for deployment status.')\nresp = openai.Deployment.retrieve(id=deployment_id)\nstatus = resp[\"status\"]\nprint(f'Deployment {deployment_id} has status: {status}')\nwhile status not in [\"succeeded\", \"failed\"]:\n    resp = openai.Deployment.retrieve(id=deployment_id)\n    status = resp[\"status\"]\n    print(f'Deployment {deployment_id} has status: {status}')\n</pre> print(f'Checking for deployment status.') resp = openai.Deployment.retrieve(id=deployment_id) status = resp[\"status\"] print(f'Deployment {deployment_id} has status: {status}') while status not in [\"succeeded\", \"failed\"]:     resp = openai.Deployment.retrieve(id=deployment_id)     status = resp[\"status\"]     print(f'Deployment {deployment_id} has status: {status}') In\u00a0[\u00a0]: Copied! <pre>prompt = \"The food was delicious and the waiter\"\ncompletion = openai.Completion.create(deployment_id=deployment_id,\n                                     prompt=prompt, stop=\".\", temperature=0)\n                                \nprint(f\"{prompt}{completion['choices'][0]['text']}.\")\n</pre> prompt = \"The food was delicious and the waiter\" completion = openai.Completion.create(deployment_id=deployment_id,                                      prompt=prompt, stop=\".\", temperature=0)                                  print(f\"{prompt}{completion['choices'][0]['text']}.\") In\u00a0[\u00a0]: Copied! <pre>print(f'Deleting deployment: {deployment_id}')\nopenai.Deployment.delete(sid=deployment_id)\n</pre> print(f'Deleting deployment: {deployment_id}') openai.Deployment.delete(sid=deployment_id)"},{"location":"examples/azure/completions/#Azure-completions-example","title":"Azure completions example\u00b6","text":"<p>In this example we'll try to go over all operations needed to get completions working using the Azure endpoints.  This example focuses on completions but also touches on some other operations that are also available using the API. This example is meant to be a quick way of showing simple operations and is not meant as a tutorial.</p>"},{"location":"examples/azure/completions/#Setup","title":"Setup\u00b6","text":"<p>For the following sections to work properly we first have to setup some things. Let's start with the <code>api_base</code> and <code>api_version</code>. To find your <code>api_base</code> go to https://portal.azure.com, find your resource and then under \"Resource Management\" -&gt; \"Keys and Endpoints\" look for the \"Endpoint\" value.</p>"},{"location":"examples/azure/completions/#Setup:-Portal","title":"Setup: Portal\u00b6","text":"<p>Let's first look at getting the key from the portal. Go to https://portal.azure.com, find your resource and then under \"Resource Management\" -&gt; \"Keys and Endpoints\" look for one of the \"Keys\" values.</p>"},{"location":"examples/azure/completions/#(Optional)-Setup:-Microsoft-Active-Directory-Authentication","title":"(Optional) Setup: Microsoft Active Directory Authentication\u00b6","text":"<p>Let's now see how we can get a key via Microsoft Active Directory Authentication. Uncomment the following code if you want to use Active Directory Authentication instead of keys from the portal.</p>"},{"location":"examples/azure/completions/#Deployments","title":"Deployments\u00b6","text":"<p>In this section we are going to create a deployment using the <code>text-davinci-002</code> model that we can then use to create completions.</p>"},{"location":"examples/azure/completions/#Deployments:-Create-manually","title":"Deployments: Create manually\u00b6","text":"<p>Create a new deployment by going to your Resource in your portal under \"Resource Management\" -&gt; \"Model deployments\". Select <code>text-davinci-002</code> as the model.</p>"},{"location":"examples/azure/completions/#(Optional)-Deployments:-Create-programatically","title":"(Optional) Deployments: Create programatically\u00b6","text":"<p>We can also create a deployment using code:</p>"},{"location":"examples/azure/completions/#(Optional)-Deployments:-Wait-for-deployment-to-succeed","title":"(Optional) Deployments: Wait for deployment to succeed\u00b6","text":"<p>Now let's check the status of the newly created deployment and wait till it is succeeded.</p>"},{"location":"examples/azure/completions/#Completions","title":"Completions\u00b6","text":"<p>Now let's send a sample completion to the deployment.</p>"},{"location":"examples/azure/completions/#(Optional)-Deployments:-Delete","title":"(Optional) Deployments: Delete\u00b6","text":"<p>Finally let's delete the deployment</p>"},{"location":"examples/azure/embeddings/","title":"Azure embeddings example","text":"In\u00a0[\u00a0]: Copied! <pre>import openai\nfrom openai import cli\n</pre> import openai from openai import cli In\u00a0[\u00a0]: Copied! <pre>openai.api_version = '2022-12-01'\nopenai.api_base = '' # Please add your endpoint here\n</pre> openai.api_version = '2022-12-01' openai.api_base = '' # Please add your endpoint here <p>We next have to setup the <code>api_type</code> and <code>api_key</code>. We can either get the key from the portal or we can get it through Microsoft Active Directory Authentication. Depending on this the <code>api_type</code> is either <code>azure</code> or <code>azure_ad</code>.</p> In\u00a0[\u00a0]: Copied! <pre>openai.api_type = 'azure'\nopenai.api_key = ''  # Please add your api key here\n</pre> openai.api_type = 'azure' openai.api_key = ''  # Please add your api key here In\u00a0[\u00a0]: Copied! <pre># from azure.identity import DefaultAzureCredential\n\n# default_credential = DefaultAzureCredential()\n# token = default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n\n# openai.api_type = 'azure_ad'\n# openai.api_key = token.token\n</pre> # from azure.identity import DefaultAzureCredential  # default_credential = DefaultAzureCredential() # token = default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")  # openai.api_type = 'azure_ad' # openai.api_key = token.token In\u00a0[\u00a0]: Copied! <pre>model = \"text-similarity-curie-001\"\n\n# Now let's create the deployment\nprint(f'Creating a new deployment with model: {model}')\nresult = openai.Deployment.create(model=model, scale_settings={\"scale_type\":\"standard\"})\ndeployment_id = result[\"id\"]\n</pre> model = \"text-similarity-curie-001\"  # Now let's create the deployment print(f'Creating a new deployment with model: {model}') result = openai.Deployment.create(model=model, scale_settings={\"scale_type\":\"standard\"}) deployment_id = result[\"id\"] In\u00a0[\u00a0]: Copied! <pre>print(f'Checking for deployment status.')\nresp = openai.Deployment.retrieve(id=deployment_id)\nstatus = resp[\"status\"]\nprint(f'Deployment {deployment_id} is with status: {status}')\n</pre> print(f'Checking for deployment status.') resp = openai.Deployment.retrieve(id=deployment_id) status = resp[\"status\"] print(f'Deployment {deployment_id} is with status: {status}') In\u00a0[\u00a0]: Copied! <pre>print('While deployment running, selecting a completed one that supports embeddings.')\ndeployment_id = None\nresult = openai.Deployment.list()\nfor deployment in result.data:\n    if deployment[\"status\"] != \"succeeded\":\n        continue\n    \n    model = openai.Model.retrieve(deployment[\"model\"])\n    if model[\"capabilities\"][\"embeddings\"] != True:\n        continue\n    \n    deployment_id = deployment[\"id\"]\n    break\n\nif not deployment_id:\n    print('No deployment with status: succeeded found.')\nelse:\n    print(f'Found a succeeded deployment that supports embeddings with id: {deployment_id}.')\n</pre> print('While deployment running, selecting a completed one that supports embeddings.') deployment_id = None result = openai.Deployment.list() for deployment in result.data:     if deployment[\"status\"] != \"succeeded\":         continue          model = openai.Model.retrieve(deployment[\"model\"])     if model[\"capabilities\"][\"embeddings\"] != True:         continue          deployment_id = deployment[\"id\"]     break  if not deployment_id:     print('No deployment with status: succeeded found.') else:     print(f'Found a succeeded deployment that supports embeddings with id: {deployment_id}.') In\u00a0[\u00a0]: Copied! <pre>embeddings = openai.Embedding.create(deployment_id=deployment_id,\n                                     input=\"The food was delicious and the waiter...\")\n                                \nprint(embeddings)\n</pre> embeddings = openai.Embedding.create(deployment_id=deployment_id,                                      input=\"The food was delicious and the waiter...\")                                  print(embeddings) In\u00a0[\u00a0]: Copied! <pre>print(f'Deleting deployment: {deployment_id}')\nopenai.Deployment.delete(sid=deployment_id)\n</pre> print(f'Deleting deployment: {deployment_id}') openai.Deployment.delete(sid=deployment_id)"},{"location":"examples/azure/embeddings/#Azure-embeddings-example","title":"Azure embeddings example\u00b6","text":"<p>In this example we'll try to go over all operations for embeddings that can be done using the Azure endpoints.  This example focuses on embeddings but also touches some other operations that are also available using the API. This example is meant to be a quick way of showing simple operations and is not meant as a tutorial.</p>"},{"location":"examples/azure/embeddings/#Setup","title":"Setup\u00b6","text":"<p>For the following sections to work properly we first have to setup some things. Let's start with the <code>api_base</code> and <code>api_version</code>. To find your <code>api_base</code> go to https://portal.azure.com, find your resource and then under \"Resource Management\" -&gt; \"Keys and Endpoints\" look for the \"Endpoint\" value.</p>"},{"location":"examples/azure/embeddings/#Setup:-Portal","title":"Setup: Portal\u00b6","text":"<p>Let's first look at getting the key from the portal. Go to https://portal.azure.com, find your resource and then under \"Resource Management\" -&gt; \"Keys and Endpoints\" look for one of the \"Keys\" values.</p>"},{"location":"examples/azure/embeddings/#(Optional)-Setup:-Microsoft-Active-Directory-Authentication","title":"(Optional) Setup: Microsoft Active Directory Authentication\u00b6","text":"<p>Let's now see how we can get a key via Microsoft Active Directory Authentication. Uncomment the following code if you want to use Active Directory Authentication instead of keys from the portal.</p>"},{"location":"examples/azure/embeddings/#Deployments","title":"Deployments\u00b6","text":"<p>In this section we are going to create a deployment that we can use to create embeddings.</p>"},{"location":"examples/azure/embeddings/#Deployments:-Create-manually","title":"Deployments: Create manually\u00b6","text":"<p>Let's create a deployment using the <code>text-similarity-curie-001</code> model. Create a new deployment by going to your Resource in your portal under \"Resource Management\" -&gt; \"Model deployments\".</p>"},{"location":"examples/azure/embeddings/#(Optional)-Deployments:-Create-programatically","title":"(Optional) Deployments: Create programatically\u00b6","text":"<p>We can also create a deployment using code:</p>"},{"location":"examples/azure/embeddings/#(Optional)-Deployments:-Retrieving","title":"(Optional) Deployments: Retrieving\u00b6","text":"<p>Now let's check the status of the newly created deployment</p>"},{"location":"examples/azure/embeddings/#Deployments:-Listing","title":"Deployments: Listing\u00b6","text":"<p>Now because creating a new deployment takes a long time, let's look in the subscription for an already finished deployment that succeeded.</p>"},{"location":"examples/azure/embeddings/#Embeddings","title":"Embeddings\u00b6","text":"<p>Now let's send a sample embedding to the deployment.</p>"},{"location":"examples/azure/embeddings/#(Optional)-Deployments:-Delete","title":"(Optional) Deployments: Delete\u00b6","text":"<p>Finally let's delete the deployment</p>"},{"location":"examples/azure/finetuning/","title":"Azure Fine tuning example","text":"In\u00a0[\u00a0]: Copied! <pre>import openai\nfrom openai import cli\n</pre> import openai from openai import cli In\u00a0[\u00a0]: Copied! <pre>openai.api_version = '2022-12-01'\nopenai.api_base = '' # Please add your endpoint here\n</pre> openai.api_version = '2022-12-01' openai.api_base = '' # Please add your endpoint here <p>We next have to setup the <code>api_type</code> and <code>api_key</code>. We can either get the key from the portal or we can get it through Microsoft Active Directory Authentication. Depending on this the <code>api_type</code> is either <code>azure</code> or <code>azure_ad</code>.</p> In\u00a0[\u00a0]: Copied! <pre>openai.api_type = 'azure'\nopenai.api_key = ''  # Please add your api key here\n</pre> openai.api_type = 'azure' openai.api_key = ''  # Please add your api key here In\u00a0[\u00a0]: Copied! <pre># from azure.identity import DefaultAzureCredential\n\n# default_credential = DefaultAzureCredential()\n# token = default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n\n# openai.api_type = 'azure_ad'\n# openai.api_key = token.token\n</pre> # from azure.identity import DefaultAzureCredential  # default_credential = DefaultAzureCredential() # token = default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")  # openai.api_type = 'azure_ad' # openai.api_key = token.token In\u00a0[\u00a0]: Copied! <pre>import shutil\nimport json\n\ntraining_file_name = 'training.jsonl'\nvalidation_file_name = 'validation.jsonl'\n\nsample_data = [{\"prompt\": \"When I go to the store, I want an\", \"completion\": \"apple.\"},\n    {\"prompt\": \"When I go to work, I want a\", \"completion\": \"coffee.\"},\n    {\"prompt\": \"When I go home, I want a\", \"completion\": \"soda.\"}]\n\nprint(f'Generating the training file: {training_file_name}')\nwith open(training_file_name, 'w') as training_file:\n    for entry in sample_data:\n        json.dump(entry, training_file)\n        training_file.write('\\n')\n\nprint(f'Copying the training file to the validation file')\nshutil.copy(training_file_name, validation_file_name)\n</pre> import shutil import json  training_file_name = 'training.jsonl' validation_file_name = 'validation.jsonl'  sample_data = [{\"prompt\": \"When I go to the store, I want an\", \"completion\": \"apple.\"},     {\"prompt\": \"When I go to work, I want a\", \"completion\": \"coffee.\"},     {\"prompt\": \"When I go home, I want a\", \"completion\": \"soda.\"}]  print(f'Generating the training file: {training_file_name}') with open(training_file_name, 'w') as training_file:     for entry in sample_data:         json.dump(entry, training_file)         training_file.write('\\n')  print(f'Copying the training file to the validation file') shutil.copy(training_file_name, validation_file_name) In\u00a0[\u00a0]: Copied! <pre>print('Checking for existing uploaded files.')\nresults = []\nfiles = openai.File.list().data\nprint(f'Found {len(files)} total uploaded files in the subscription.')\nfor item in files:\n    if item[\"filename\"] in [training_file_name, validation_file_name]:\n        results.append(item[\"id\"])\nprint(f'Found {len(results)} already uploaded files that match our names.')\n</pre> print('Checking for existing uploaded files.') results = [] files = openai.File.list().data print(f'Found {len(files)} total uploaded files in the subscription.') for item in files:     if item[\"filename\"] in [training_file_name, validation_file_name]:         results.append(item[\"id\"]) print(f'Found {len(results)} already uploaded files that match our names.')  In\u00a0[\u00a0]: Copied! <pre>print(f'Deleting already uploaded files...')\nfor id in results:\n    openai.File.delete(sid = id)\n</pre> print(f'Deleting already uploaded files...') for id in results:     openai.File.delete(sid = id)  In\u00a0[\u00a0]: Copied! <pre>import time\n\ndef check_status(training_id, validation_id):\n    train_status = openai.File.retrieve(training_id)[\"status\"]\n    valid_status = openai.File.retrieve(validation_id)[\"status\"]\n    print(f'Status (training_file | validation_file): {train_status} | {valid_status}')\n    return (train_status, valid_status)\n\n#importing our two files\ntraining_id = cli.FineTune._get_or_upload(training_file_name, True)\nvalidation_id = cli.FineTune._get_or_upload(validation_file_name, True)\n\n#checking the status of the imports\n(train_status, valid_status) = check_status(training_id, validation_id)\n\nwhile train_status not in [\"succeeded\", \"failed\"] or valid_status not in [\"succeeded\", \"failed\"]:\n    time.sleep(1)\n    (train_status, valid_status) = check_status(training_id, validation_id)\n</pre> import time  def check_status(training_id, validation_id):     train_status = openai.File.retrieve(training_id)[\"status\"]     valid_status = openai.File.retrieve(validation_id)[\"status\"]     print(f'Status (training_file | validation_file): {train_status} | {valid_status}')     return (train_status, valid_status)  #importing our two files training_id = cli.FineTune._get_or_upload(training_file_name, True) validation_id = cli.FineTune._get_or_upload(validation_file_name, True)  #checking the status of the imports (train_status, valid_status) = check_status(training_id, validation_id)  while train_status not in [\"succeeded\", \"failed\"] or valid_status not in [\"succeeded\", \"failed\"]:     time.sleep(1)     (train_status, valid_status) = check_status(training_id, validation_id)  In\u00a0[\u00a0]: Copied! <pre>print(f'Downloading training file: {training_id}')\nresult = openai.File.download(training_id)\nprint(result.decode('utf-8'))\n</pre> print(f'Downloading training file: {training_id}') result = openai.File.download(training_id) print(result.decode('utf-8')) In\u00a0[\u00a0]: Copied! <pre>create_args = {\n    \"training_file\": training_id,\n    \"validation_file\": validation_id,\n    \"model\": \"babbage\",\n    \"compute_classification_metrics\": True,\n    \"classification_n_classes\": 3,\n    \"n_epochs\": 20,\n    \"batch_size\": 3,\n    \"learning_rate_multiplier\": 0.3\n}\nresp = openai.FineTune.create(**create_args)\njob_id = resp[\"id\"]\nstatus = resp[\"status\"]\n\nprint(f'Fine-tunning model with jobID: {job_id}.')\n</pre> create_args = {     \"training_file\": training_id,     \"validation_file\": validation_id,     \"model\": \"babbage\",     \"compute_classification_metrics\": True,     \"classification_n_classes\": 3,     \"n_epochs\": 20,     \"batch_size\": 3,     \"learning_rate_multiplier\": 0.3 } resp = openai.FineTune.create(**create_args) job_id = resp[\"id\"] status = resp[\"status\"]  print(f'Fine-tunning model with jobID: {job_id}.') In\u00a0[\u00a0]: Copied! <pre>import signal\nimport datetime\n\ndef signal_handler(sig, frame):\n    status = openai.FineTune.retrieve(job_id).status\n    print(f\"Stream interrupted. Job is still {status}.\")\n    return\n\nprint(f'Streaming events for the fine-tuning job: {job_id}')\nsignal.signal(signal.SIGINT, signal_handler)\n\nevents = openai.FineTune.stream_events(job_id)\ntry:\n    for event in events:\n        print(f'{datetime.datetime.fromtimestamp(event[\"created_at\"])} {event[\"message\"]}')\n\nexcept Exception:\n    print(\"Stream interrupted (client disconnected).\")\n</pre> import signal import datetime  def signal_handler(sig, frame):     status = openai.FineTune.retrieve(job_id).status     print(f\"Stream interrupted. Job is still {status}.\")     return  print(f'Streaming events for the fine-tuning job: {job_id}') signal.signal(signal.SIGINT, signal_handler)  events = openai.FineTune.stream_events(job_id) try:     for event in events:         print(f'{datetime.datetime.fromtimestamp(event[\"created_at\"])} {event[\"message\"]}')  except Exception:     print(\"Stream interrupted (client disconnected).\") In\u00a0[\u00a0]: Copied! <pre>status = openai.FineTune.retrieve(id=job_id)[\"status\"]\nif status not in [\"succeeded\", \"failed\"]:\n    print(f'Job not in terminal status: {status}. Waiting.')\n    while status not in [\"succeeded\", \"failed\"]:\n        time.sleep(2)\n        status = openai.FineTune.retrieve(id=job_id)[\"status\"]\n        print(f'Status: {status}')\nelse:\n    print(f'Finetune job {job_id} finished with status: {status}')\n\nprint('Checking other finetune jobs in the subscription.')\nresult = openai.FineTune.list()\nprint(f'Found {len(result.data)} finetune jobs.')\n</pre> status = openai.FineTune.retrieve(id=job_id)[\"status\"] if status not in [\"succeeded\", \"failed\"]:     print(f'Job not in terminal status: {status}. Waiting.')     while status not in [\"succeeded\", \"failed\"]:         time.sleep(2)         status = openai.FineTune.retrieve(id=job_id)[\"status\"]         print(f'Status: {status}') else:     print(f'Finetune job {job_id} finished with status: {status}')  print('Checking other finetune jobs in the subscription.') result = openai.FineTune.list() print(f'Found {len(result.data)} finetune jobs.') In\u00a0[\u00a0]: Copied! <pre># openai.FineTune.delete(sid=job_id)\n</pre> # openai.FineTune.delete(sid=job_id) In\u00a0[\u00a0]: Copied! <pre>#Fist let's get the model of the previous job:\nresult = openai.FineTune.retrieve(id=job_id)\nif result[\"status\"] == 'succeeded':\n    model = result[\"fine_tuned_model\"]\n\n# Now let's create the deployment\nprint(f'Creating a new deployment with model: {model}')\nresult = openai.Deployment.create(model=model, scale_settings={\"scale_type\":\"standard\"})\ndeployment_id = result[\"id\"]\n</pre> #Fist let's get the model of the previous job: result = openai.FineTune.retrieve(id=job_id) if result[\"status\"] == 'succeeded':     model = result[\"fine_tuned_model\"]  # Now let's create the deployment print(f'Creating a new deployment with model: {model}') result = openai.Deployment.create(model=model, scale_settings={\"scale_type\":\"standard\"}) deployment_id = result[\"id\"]  In\u00a0[\u00a0]: Copied! <pre>print(f'Checking for deployment status.')\nresp = openai.Deployment.retrieve(id=deployment_id)\nstatus = resp[\"status\"]\nprint(f'Deployment {deployment_id} is with status: {status}')\n</pre> print(f'Checking for deployment status.') resp = openai.Deployment.retrieve(id=deployment_id) status = resp[\"status\"] print(f'Deployment {deployment_id} is with status: {status}')  In\u00a0[\u00a0]: Copied! <pre>print('While deployment running, selecting a completed one.')\ndeployment_id = None\nresult = openai.Deployment.list()\nfor deployment in result.data:\n    if deployment[\"status\"] == \"succeeded\":\n        deployment_id = deployment[\"id\"]\n        break\n\nif not deployment_id:\n    print('No deployment with status: succeeded found.')\nelse:\n    print(f'Found a successful deployment with id: {deployment_id}.')\n</pre> print('While deployment running, selecting a completed one.') deployment_id = None result = openai.Deployment.list() for deployment in result.data:     if deployment[\"status\"] == \"succeeded\":         deployment_id = deployment[\"id\"]         break  if not deployment_id:     print('No deployment with status: succeeded found.') else:     print(f'Found a successful deployment with id: {deployment_id}.')  In\u00a0[\u00a0]: Copied! <pre>print('Sending a test completion job')\nstart_phrase = 'When I go home, I want a'\nresponse = openai.Completion.create(deployment_id=deployment_id, prompt=start_phrase, temperature=0, stop=\".\")\ntext = response['choices'][0]['text'].replace('\\n', '').replace(' .', '.').strip()\nprint(f'\"{start_phrase} {text}.\"')\n</pre> print('Sending a test completion job') start_phrase = 'When I go home, I want a' response = openai.Completion.create(deployment_id=deployment_id, prompt=start_phrase, temperature=0, stop=\".\") text = response['choices'][0]['text'].replace('\\n', '').replace(' .', '.').strip() print(f'\"{start_phrase} {text}.\"') In\u00a0[\u00a0]: Copied! <pre>print(f'Deleting deployment: {deployment_id}')\nopenai.Deployment.delete(sid=deployment_id)\n</pre> print(f'Deleting deployment: {deployment_id}') openai.Deployment.delete(sid=deployment_id) <p>Thank you</p>"},{"location":"examples/azure/finetuning/#Azure-Fine-tuning-example","title":"Azure Fine tuning example\u00b6","text":"<p>In this example we'll try to go over all operations that can be done using the Azure endpoints and their differences with the openAI endpoints (if any). This example focuses on finetuning but also touches on the majority of operations that are available using the API. This example is meant to be a quick way of showing simple operations and is not meant as a finetune model adaptation tutorial.</p>"},{"location":"examples/azure/finetuning/#Setup","title":"Setup\u00b6","text":"<p>For the following sections to work properly we first have to setup some things. Let's start with the <code>api_base</code> and <code>api_version</code>. To find your <code>api_base</code> go to https://portal.azure.com, find your resource and then under \"Resource Management\" -&gt; \"Keys and Endpoints\" look for the \"Endpoint\" value.</p>"},{"location":"examples/azure/finetuning/#Setup:-Portal","title":"Setup: Portal\u00b6","text":"<p>Let's first look at getting the key from the portal. Go to https://portal.azure.com, find your resource and then under \"Resource Management\" -&gt; \"Keys and Endpoints\" look for one of the \"Keys\" values.</p>"},{"location":"examples/azure/finetuning/#(Optional)-Setup:-Microsoft-Active-Directory-Authentication","title":"(Optional) Setup: Microsoft Active Directory Authentication\u00b6","text":"<p>Let's now see how we can get a key via Microsoft Active Directory Authentication. Uncomment the following code if you want to use Active Directory Authentication instead of keys from the portal.</p>"},{"location":"examples/azure/finetuning/#Files","title":"Files\u00b6","text":"<p>In the next section we will focus on the files operations: importing, listing, retrieving, deleting. For this we need to create 2 temporary files with some sample data. For the sake of simplicity, we will use the same data for training and validation.</p>"},{"location":"examples/azure/finetuning/#Files:-Listing","title":"Files: Listing\u00b6","text":"<p>List all of the uploaded files and check for the ones that are named \"training.jsonl\" or \"validation.jsonl\"</p>"},{"location":"examples/azure/finetuning/#Files:-Deleting","title":"Files: Deleting\u00b6","text":"<p>Let's now delete those found files (if any) since we're going to be re-uploading them next.</p>"},{"location":"examples/azure/finetuning/#Files:-Importing-&-Retrieving","title":"Files: Importing &amp; Retrieving\u00b6","text":"<p>Now, let's import our two files ('training.jsonl' and 'validation.jsonl') and keep those IDs since we're going to use them later for finetuning. For this operation we are going to use the cli wrapper which does a bit more checks before uploading and also gives us progress. In addition, after uploading we're going to check the status our import until it has succeeded (or failed if something goes wrong)</p>"},{"location":"examples/azure/finetuning/#Files:-Downloading","title":"Files: Downloading\u00b6","text":"<p>Now let's download one of the files, the training file for example, to check that everything was in order during importing and all bits are there.</p>"},{"location":"examples/azure/finetuning/#Finetune","title":"Finetune\u00b6","text":"<p>In this section we are going to use the two training and validation files that we imported in the previous section, to train a finetune model.</p>"},{"location":"examples/azure/finetuning/#Finetune:-Adapt","title":"Finetune: Adapt\u00b6","text":"<p>First let's create the finetune adaptation job.</p>"},{"location":"examples/azure/finetuning/#Finetune:-Streaming","title":"Finetune: Streaming\u00b6","text":"<p>While the job runs, we can subscribe to the streaming events to check the progress of the operation.</p>"},{"location":"examples/azure/finetuning/#Finetune:-Listing-and-Retrieving","title":"Finetune: Listing and Retrieving\u00b6","text":"<p>Now let's check that our operation was successful and in addition we can look at all of the finetuning operations using a list operation.</p>"},{"location":"examples/azure/finetuning/#Finetune:-Deleting","title":"Finetune: Deleting\u00b6","text":"<p>Finally we can delete our finetune job. WARNING: Please skip this step if you want to continue with the next section as the finetune model is needed. (The delete code is commented out by default)</p>"},{"location":"examples/azure/finetuning/#Deployments","title":"Deployments\u00b6","text":"<p>In this section we are going to create a deployment using the finetune model that we just adapted and then used the deployment to create a simple completion operation.</p>"},{"location":"examples/azure/finetuning/#Deployments:-Create","title":"Deployments: Create\u00b6","text":"<p>Let's create a deployment using the fine-tune model.</p>"},{"location":"examples/azure/finetuning/#Deployments:-Retrieving","title":"Deployments: Retrieving\u00b6","text":"<p>Now let's check the status of the newly created deployment</p>"},{"location":"examples/azure/finetuning/#Deployments:-Listing","title":"Deployments: Listing\u00b6","text":"<p>Now because creating a new deployment takes a long time, let's look in the subscription for an already finished deployment that succeeded.</p>"},{"location":"examples/azure/finetuning/#Completions","title":"Completions\u00b6","text":"<p>Now let's send a sample completion to the deployment.</p>"},{"location":"examples/azure/finetuning/#Deployments:-Delete","title":"Deployments: Delete\u00b6","text":"<p>Finally let's delete the deployment</p>"},{"location":"examples/book_translation/translate_latex_book/","title":"Translate a book writen in LaTeX from Slovenian into English","text":"In\u00a0[19]: Copied! <pre>import openai\nfrom transformers import GPT2Tokenizer\n\n# OpenAI GPT-2 tokenizer is the same as GPT-3 tokenizer\n# we use it to count the number of tokens in the text\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n\nwith open(\"data/geometry_slovenian.tex\", \"r\") as f:\n    text = f.read()\n</pre> import openai from transformers import GPT2Tokenizer  # OpenAI GPT-2 tokenizer is the same as GPT-3 tokenizer # we use it to count the number of tokens in the text tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")  with open(\"data/geometry_slovenian.tex\", \"r\") as f:     text = f.read() Out[19]: <pre>1485565</pre> In\u00a0[20]: Copied! <pre>chunks = text.split('\\n\\n')\nntokens = []\nfor chunk in chunks:\n    ntokens.append(len(tokenizer.encode(chunk)))\nmax(ntokens)\n</pre> chunks = text.split('\\n\\n') ntokens = [] for chunk in chunks:     ntokens.append(len(tokenizer.encode(chunk))) max(ntokens) <pre>Token indices sequence length is longer than the specified maximum sequence length for this model (1327 &gt; 1024). Running this sequence through the model will result in indexing errors\n</pre> Out[20]: <pre>1473</pre> <p>It turns out that a double newline is a good separator in this case, in order not to break the flow of the text. Also no individual chunk is larger than 1500 tokens. The model we will use is text-davinci-002, which has a limit of 4096 tokens, so we don't need to worry about breaking the chunks down further.</p> <p>We will group the shorter chunks into chunks of around 1000 tokens, to increase the coherence of the text, and decrease the frequency of breaks within the text.</p> In\u00a0[21]: Copied! <pre>def group_chunks(chunks, ntokens, max_len=1000):\n\"\"\"\n    Group very short chunks, to form approximately a page long chunks.\n    \"\"\"\n    batches = []\n    cur_batch = \"\"\n    cur_tokens = 0\n\n    # iterate over chunks, and group the short ones together\n    for chunk, ntoken in zip(chunks, ntokens):\n        cur_tokens += ntoken + 2  # +2 for the newlines between chunks\n\n        # if adding this chunk would exceed the max length, finalize the current batch and start a new one\n        if ntoken + cur_tokens &gt; max_len:\n            batches.append(cur_batch)\n            cur_batch = chunk\n        else:\n            cur_batch += \"\\n\\n\" + chunk\n    batches.append(cur_batch)\n    return batches\n\nchunks = group_chunks(chunks, ntokens)\nlen(chunks)\n</pre> def group_chunks(chunks, ntokens, max_len=1000):     \"\"\"     Group very short chunks, to form approximately a page long chunks.     \"\"\"     batches = []     cur_batch = \"\"     cur_tokens = 0      # iterate over chunks, and group the short ones together     for chunk, ntoken in zip(chunks, ntokens):         cur_tokens += ntoken + 2  # +2 for the newlines between chunks          # if adding this chunk would exceed the max length, finalize the current batch and start a new one         if ntoken + cur_tokens &gt; max_len:             batches.append(cur_batch)             cur_batch = chunk         else:             cur_batch += \"\\n\\n\" + chunk     batches.append(cur_batch)     return batches  chunks = group_chunks(chunks, ntokens) len(chunks) Out[21]: <pre>869</pre> <p>Notice that adding a sample untranslated and translated first command, where only the content of the chapter name needs to be translated, helps to get more consistent results.</p> <p>The format of the prompt sent to the model consists of:</p> <ol> <li>A high level instruction to translate only the text, but not commands into the desired language</li> <li>A sample untranslated command, where only the content of the chapter name needs to be translated</li> <li>The chunk of text to be translated</li> <li>The translated sample command from 2, which shows the model the beginning of the translation process</li> </ol> <p>The expected output is the translated chunk of text.</p> In\u00a0[40]: Copied! <pre>def translate_chunk(chunk, engine='text-davinci-002',\n                    dest_language='English',\n                    sample_translation=(\"\\poglavje{Osnove Geometrije} \\label{osn9Geom}\", \"\\poglavje{The basics of Geometry} \\label{osn9Geom}\")\n                    ):\n    prompt = f'''Translate only the text from the following LaTeX document into {dest_language}. Leave all LaTeX commands unchanged\n\"\"\"\n{sample_translation[0]}\n{chunk}\"\"\"\n\n{sample_translation[1]}\n'''\n    response = openai.Completion.create(\n        prompt=prompt,\n        engine=engine,\n        temperature=0,\n        top_p=1,\n        max_tokens=1500,\n    )\n    result = response['choices'][0]['text'].strip()\n    result = result.replace('\"\"\"', '') # remove the double quotes, as we used them to surround the text\n    return result\nprint(translate_chunk(chunks[800], engine='text-davinci-002', dest_language='English'))\n</pre> def translate_chunk(chunk, engine='text-davinci-002',                     dest_language='English',                     sample_translation=(\"\\poglavje{Osnove Geometrije} \\label{osn9Geom}\", \"\\poglavje{The basics of Geometry} \\label{osn9Geom}\")                     ):     prompt = f'''Translate only the text from the following LaTeX document into {dest_language}. Leave all LaTeX commands unchanged      \"\"\" {sample_translation[0]} {chunk}\"\"\"  {sample_translation[1]} '''     response = openai.Completion.create(         prompt=prompt,         engine=engine,         temperature=0,         top_p=1,         max_tokens=1500,     )     result = response['choices'][0]['text'].strip()     result = result.replace('\"\"\"', '') # remove the double quotes, as we used them to surround the text     return result print(translate_chunk(chunks[800], engine='text-davinci-002', dest_language='English')) <pre>Let $\\mathcal{I}=\\mathcal{S}_{AB} \\circ\\mathcal{S}_{CA}\n    \\circ\\mathcal{S}_{BC}$. By  \\ref{izoZrcdrsprq} is\n    $\\mathcal{I}$ a mirror reflection. Let $A_1$, $B_1$ and $C_1$ be in order the center points of the lines $BC$, $AC$ and $AB$ of the triangle $ABC$.\n    Because it is a right triangle is $\\mathcal{I}(A_1C_1)=A_1C_1$, which\n    means that the line $A_1C_1$ is of this mirror reflection. It is not\n    difficult to prove that for the point $A'_1=\\mathcal{I}(A_1)$ (both\n    lie on the axis $A_1C_1$) is\n    $\\overrightarrow{A_1A'_1}=3\\overrightarrow{A_1C_1}$, so\n    $\\mathcal{I}=\\mathcal{G}_{3\\overrightarrow{A_1C_1}}$.\n\n\\item  \\res{Given are the points $A$ and $B$ on the same side of the line\n$p$.\nDraw the line  $XY$, which lies on the line $p$ and is consistent\nwith the given line $l$, so that the sum\n$|AX|+|XY|+|YB|$ is minimal.}\n\nLet $A'=\\mathcal{G}_{\\overrightarrow{MN}}(A)$ (where $M,N\\in\np$ and $MN\\cong l$). The point $Y$ is obtained as the intersection of the lines $p$\nand $X'Y$ (see also example \\ref{HeronProbl}).\n\n\\item  \\res{Let $ABC$ be an isosceles right triangle with a right angle at the vertex $A$. What does the composite\n$\\mathcal{G}_{\\overrightarrow{AB}}\\circ \\mathcal{G}_{\\overrightarrow{CA}}$ represent?}\n\nLet $p$ and $q$ be the simetrali of the sides $CA$ and $AB$ of the triangle\n$ABC$. By  \\ref{izoZrcDrsKompSrOsn} is:\n $$\\mathcal{G}_{\\overrightarrow{AB}}\\circ\n \\mathcal{G}_{\\overrightarrow{CA}}=\n \\mathcal{S}_q\\circ\\mathcal{S}_A\\circ\\mathcal{S}_A\\circ\\mathcal{S}_p=\n \\mathcal{S}_q\\circ\\mathcal{S}_p.$$ Because $ABC$ is an isosceles\n right triangle with a right angle at the vertex $A$, the lines $p$ and $q$ are perpendicular and intersect at the center $S$\n of the hypotenuse $BC$. Therefore\n $\\mathcal{G}_{\\overrightarrow{AB}}\\circ\n \\mathcal{G}_{\\overrightarrow{CA}}=\\mathcal{S}_q\n \\circ\\mathcal{S}_p=\\mathcal{S}_S$.\n\n\\item \\res{In the same plane are given the lines\n$a$, $b$ and $c$.\nDraw the points $A\\in a$ and $B\\in b$\nso that $\\mathcal{S}_c(A)=B$.}\n</pre> <p>We can see here that this one chunk in particular translates only the text, but leaves LaTeX commands intact.</p> <p>Let's now translate all the chunks in the book - this will take 2-3 hours, as we're processing requests sequentially.</p> In\u00a0[39]: Copied! <pre>dest_language = \"English\"\n\ntranslated_chunks = []\nfor i, chunk in enumerate(chunks):\n    print(str(i+1) + \" / \" + str(len(chunks)))\n    # translate each chunk\n    translated_chunks.append(translate_chunk(chunk, engine='text-davinci-002', dest_language=dest_language))\n\n# join the chunks together\nresult = '\\n\\n'.join(translated_chunks)\n\n# save the final result\nwith open(f\"data/geometry_{dest_language}.tex\", \"w\") as f:\n    f.write(result)\n</pre> dest_language = \"English\"  translated_chunks = [] for i, chunk in enumerate(chunks):     print(str(i+1) + \" / \" + str(len(chunks)))     # translate each chunk     translated_chunks.append(translate_chunk(chunk, engine='text-davinci-002', dest_language=dest_language))  # join the chunks together result = '\\n\\n'.join(translated_chunks)  # save the final result with open(f\"data/geometry_{dest_language}.tex\", \"w\") as f:     f.write(result) <pre>0 / 869\n1 / 869\n2 / 869\n3 / 869\n4 / 869\n5 / 869\n6 / 869\n7 / 869\n8 / 869\n9 / 869\n10 / 869\n11 / 869\n12 / 869\n13 / 869\n14 / 869\n15 / 869\n16 / 869\n17 / 869\n18 / 869\n19 / 869\n20 / 869\n21 / 869\n22 / 869\n23 / 869\n24 / 869\n25 / 869\n26 / 869\n27 / 869\n28 / 869\n29 / 869\n30 / 869\n31 / 869\n32 / 869\n33 / 869\n34 / 869\n35 / 869\n36 / 869\n37 / 869\n38 / 869\n39 / 869\n40 / 869\n41 / 869\n42 / 869\n43 / 869\n44 / 869\n45 / 869\n46 / 869\n47 / 869\n48 / 869\n49 / 869\n50 / 869\n51 / 869\n52 / 869\n53 / 869\n54 / 869\n55 / 869\n56 / 869\n57 / 869\n58 / 869\n59 / 869\n60 / 869\n61 / 869\n62 / 869\n63 / 869\n64 / 869\n65 / 869\n66 / 869\n67 / 869\n68 / 869\n69 / 869\n70 / 869\n71 / 869\n72 / 869\n73 / 869\n74 / 869\n75 / 869\n76 / 869\n77 / 869\n78 / 869\n79 / 869\n80 / 869\n81 / 869\n82 / 869\n83 / 869\n84 / 869\n85 / 869\n86 / 869\n87 / 869\n88 / 869\n89 / 869\n90 / 869\n91 / 869\n92 / 869\n93 / 869\n94 / 869\n95 / 869\n96 / 869\n97 / 869\n98 / 869\n99 / 869\n100 / 869\n101 / 869\n102 / 869\n103 / 869\n104 / 869\n105 / 869\n106 / 869\n107 / 869\n108 / 869\n109 / 869\n110 / 869\n111 / 869\n112 / 869\n113 / 869\n114 / 869\n115 / 869\n116 / 869\n117 / 869\n118 / 869\n119 / 869\n120 / 869\n121 / 869\n122 / 869\n123 / 869\n124 / 869\n125 / 869\n126 / 869\n127 / 869\n128 / 869\n129 / 869\n130 / 869\n131 / 869\n132 / 869\n133 / 869\n134 / 869\n135 / 869\n136 / 869\n137 / 869\n138 / 869\n139 / 869\n140 / 869\n141 / 869\n142 / 869\n143 / 869\n144 / 869\n145 / 869\n146 / 869\n147 / 869\n148 / 869\n149 / 869\n150 / 869\n151 / 869\n152 / 869\n153 / 869\n154 / 869\n155 / 869\n156 / 869\n157 / 869\n158 / 869\n159 / 869\n160 / 869\n161 / 869\n162 / 869\n163 / 869\n164 / 869\n165 / 869\n166 / 869\n167 / 869\n168 / 869\n169 / 869\n170 / 869\n171 / 869\n172 / 869\n173 / 869\n174 / 869\n175 / 869\n176 / 869\n177 / 869\n178 / 869\n179 / 869\n180 / 869\n181 / 869\n182 / 869\n183 / 869\n184 / 869\n185 / 869\n186 / 869\n187 / 869\n188 / 869\n189 / 869\n190 / 869\n191 / 869\n192 / 869\n193 / 869\n194 / 869\n195 / 869\n196 / 869\n197 / 869\n198 / 869\n199 / 869\n200 / 869\n201 / 869\n202 / 869\n203 / 869\n204 / 869\n205 / 869\n206 / 869\n207 / 869\n208 / 869\n209 / 869\n210 / 869\n211 / 869\n212 / 869\n213 / 869\n214 / 869\n215 / 869\n216 / 869\n217 / 869\n218 / 869\n219 / 869\n220 / 869\n221 / 869\n222 / 869\n223 / 869\n224 / 869\n225 / 869\n226 / 869\n227 / 869\n228 / 869\n229 / 869\n230 / 869\n231 / 869\n232 / 869\n233 / 869\n234 / 869\n235 / 869\n236 / 869\n237 / 869\n238 / 869\n239 / 869\n240 / 869\n241 / 869\n242 / 869\n243 / 869\n244 / 869\n245 / 869\n246 / 869\n247 / 869\n248 / 869\n249 / 869\n250 / 869\n251 / 869\n252 / 869\n253 / 869\n254 / 869\n255 / 869\n256 / 869\n257 / 869\n258 / 869\n259 / 869\n260 / 869\n261 / 869\n262 / 869\n263 / 869\n264 / 869\n265 / 869\n266 / 869\n267 / 869\n268 / 869\n269 / 869\n270 / 869\n271 / 869\n272 / 869\n273 / 869\n274 / 869\n275 / 869\n276 / 869\n277 / 869\n278 / 869\n279 / 869\n280 / 869\n281 / 869\n282 / 869\n283 / 869\n284 / 869\n285 / 869\n286 / 869\n287 / 869\n288 / 869\n289 / 869\n290 / 869\n291 / 869\n292 / 869\n293 / 869\n294 / 869\n295 / 869\n296 / 869\n297 / 869\n298 / 869\n299 / 869\n300 / 869\n301 / 869\n302 / 869\n303 / 869\n304 / 869\n305 / 869\n306 / 869\n307 / 869\n308 / 869\n309 / 869\n310 / 869\n311 / 869\n312 / 869\n313 / 869\n314 / 869\n315 / 869\n316 / 869\n317 / 869\n318 / 869\n319 / 869\n320 / 869\n321 / 869\n322 / 869\n323 / 869\n324 / 869\n325 / 869\n326 / 869\n327 / 869\n328 / 869\n329 / 869\n330 / 869\n331 / 869\n332 / 869\n333 / 869\n334 / 869\n335 / 869\n336 / 869\n337 / 869\n338 / 869\n339 / 869\n340 / 869\n341 / 869\n342 / 869\n343 / 869\n344 / 869\n345 / 869\n346 / 869\n347 / 869\n348 / 869\n349 / 869\n350 / 869\n351 / 869\n352 / 869\n353 / 869\n354 / 869\n355 / 869\n356 / 869\n357 / 869\n358 / 869\n359 / 869\n360 / 869\n361 / 869\n362 / 869\n363 / 869\n364 / 869\n365 / 869\n366 / 869\n367 / 869\n368 / 869\n369 / 869\n370 / 869\n371 / 869\n372 / 869\n373 / 869\n374 / 869\n375 / 869\n376 / 869\n377 / 869\n378 / 869\n379 / 869\n380 / 869\n381 / 869\n382 / 869\n383 / 869\n384 / 869\n385 / 869\n386 / 869\n387 / 869\n388 / 869\n389 / 869\n390 / 869\n391 / 869\n392 / 869\n393 / 869\n394 / 869\n395 / 869\n396 / 869\n397 / 869\n398 / 869\n399 / 869\n400 / 869\n401 / 869\n402 / 869\n403 / 869\n404 / 869\n405 / 869\n406 / 869\n407 / 869\n408 / 869\n409 / 869\n410 / 869\n411 / 869\n412 / 869\n413 / 869\n414 / 869\n415 / 869\n416 / 869\n417 / 869\n418 / 869\n419 / 869\n420 / 869\n421 / 869\n422 / 869\n423 / 869\n424 / 869\n425 / 869\n426 / 869\n427 / 869\n428 / 869\n429 / 869\n430 / 869\n431 / 869\n432 / 869\n433 / 869\n434 / 869\n435 / 869\n436 / 869\n437 / 869\n438 / 869\n439 / 869\n440 / 869\n441 / 869\n442 / 869\n443 / 869\n444 / 869\n445 / 869\n446 / 869\n447 / 869\n448 / 869\n449 / 869\n450 / 869\n451 / 869\n452 / 869\n453 / 869\n454 / 869\n455 / 869\n456 / 869\n457 / 869\n458 / 869\n459 / 869\n460 / 869\n461 / 869\n462 / 869\n463 / 869\n464 / 869\n465 / 869\n466 / 869\n467 / 869\n468 / 869\n469 / 869\n470 / 869\n471 / 869\n472 / 869\n473 / 869\n474 / 869\n475 / 869\n476 / 869\n477 / 869\n478 / 869\n479 / 869\n480 / 869\n481 / 869\n482 / 869\n483 / 869\n484 / 869\n485 / 869\n486 / 869\n487 / 869\n488 / 869\n489 / 869\n490 / 869\n491 / 869\n492 / 869\n493 / 869\n494 / 869\n495 / 869\n496 / 869\n497 / 869\n498 / 869\n499 / 869\n500 / 869\n501 / 869\n502 / 869\n503 / 869\n504 / 869\n505 / 869\n506 / 869\n507 / 869\n508 / 869\n509 / 869\n510 / 869\n511 / 869\n512 / 869\n513 / 869\n514 / 869\n515 / 869\n516 / 869\n517 / 869\n518 / 869\n519 / 869\n520 / 869\n521 / 869\n522 / 869\n523 / 869\n524 / 869\n525 / 869\n526 / 869\n527 / 869\n528 / 869\n529 / 869\n530 / 869\n531 / 869\n532 / 869\n533 / 869\n534 / 869\n535 / 869\n536 / 869\n537 / 869\n538 / 869\n539 / 869\n540 / 869\n541 / 869\n542 / 869\n543 / 869\n544 / 869\n545 / 869\n546 / 869\n547 / 869\n548 / 869\n549 / 869\n550 / 869\n551 / 869\n552 / 869\n553 / 869\n554 / 869\n555 / 869\n556 / 869\n557 / 869\n558 / 869\n559 / 869\n560 / 869\n561 / 869\n562 / 869\n563 / 869\n564 / 869\n565 / 869\n566 / 869\n567 / 869\n568 / 869\n569 / 869\n570 / 869\n571 / 869\n572 / 869\n573 / 869\n574 / 869\n575 / 869\n576 / 869\n577 / 869\n578 / 869\n579 / 869\n580 / 869\n581 / 869\n582 / 869\n583 / 869\n584 / 869\n585 / 869\n586 / 869\n587 / 869\n588 / 869\n589 / 869\n590 / 869\n591 / 869\n592 / 869\n593 / 869\n594 / 869\n595 / 869\n596 / 869\n597 / 869\n598 / 869\n599 / 869\n600 / 869\n601 / 869\n602 / 869\n603 / 869\n604 / 869\n605 / 869\n606 / 869\n607 / 869\n608 / 869\n609 / 869\n610 / 869\n611 / 869\n612 / 869\n613 / 869\n614 / 869\n615 / 869\n616 / 869\n617 / 869\n618 / 869\n619 / 869\n620 / 869\n621 / 869\n622 / 869\n623 / 869\n624 / 869\n625 / 869\n626 / 869\n627 / 869\n628 / 869\n629 / 869\n630 / 869\n631 / 869\n632 / 869\n633 / 869\n634 / 869\n635 / 869\n636 / 869\n637 / 869\n638 / 869\n639 / 869\n640 / 869\n641 / 869\n642 / 869\n643 / 869\n644 / 869\n645 / 869\n646 / 869\n647 / 869\n648 / 869\n649 / 869\n650 / 869\n651 / 869\n652 / 869\n653 / 869\n654 / 869\n655 / 869\n656 / 869\n657 / 869\n658 / 869\n659 / 869\n660 / 869\n661 / 869\n662 / 869\n663 / 869\n664 / 869\n665 / 869\n666 / 869\n667 / 869\n668 / 869\n669 / 869\n670 / 869\n671 / 869\n672 / 869\n673 / 869\n674 / 869\n675 / 869\n676 / 869\n677 / 869\n678 / 869\n679 / 869\n680 / 869\n681 / 869\n682 / 869\n683 / 869\n684 / 869\n685 / 869\n686 / 869\n687 / 869\n688 / 869\n689 / 869\n690 / 869\n691 / 869\n692 / 869\n693 / 869\n694 / 869\n695 / 869\n696 / 869\n697 / 869\n698 / 869\n699 / 869\n700 / 869\n701 / 869\n702 / 869\n703 / 869\n704 / 869\n705 / 869\n706 / 869\n707 / 869\n708 / 869\n709 / 869\n710 / 869\n711 / 869\n712 / 869\n713 / 869\n714 / 869\n715 / 869\n716 / 869\n717 / 869\n718 / 869\n719 / 869\n720 / 869\n721 / 869\n722 / 869\n723 / 869\n724 / 869\n725 / 869\n726 / 869\n727 / 869\n728 / 869\n729 / 869\n730 / 869\n731 / 869\n732 / 869\n733 / 869\n734 / 869\n735 / 869\n736 / 869\n737 / 869\n738 / 869\n739 / 869\n740 / 869\n741 / 869\n742 / 869\n743 / 869\n744 / 869\n745 / 869\n746 / 869\n747 / 869\n748 / 869\n749 / 869\n750 / 869\n751 / 869\n752 / 869\n753 / 869\n754 / 869\n755 / 869\n756 / 869\n757 / 869\n758 / 869\n759 / 869\n760 / 869\n761 / 869\n762 / 869\n763 / 869\n764 / 869\n765 / 869\n766 / 869\n767 / 869\n768 / 869\n769 / 869\n770 / 869\n771 / 869\n772 / 869\n773 / 869\n774 / 869\n775 / 869\n776 / 869\n777 / 869\n778 / 869\n779 / 869\n780 / 869\n781 / 869\n782 / 869\n783 / 869\n784 / 869\n785 / 869\n786 / 869\n787 / 869\n788 / 869\n789 / 869\n790 / 869\n791 / 869\n792 / 869\n793 / 869\n794 / 869\n795 / 869\n796 / 869\n797 / 869\n798 / 869\n799 / 869\n800 / 869\n801 / 869\n802 / 869\n803 / 869\n804 / 869\n805 / 869\n806 / 869\n807 / 869\n808 / 869\n809 / 869\n810 / 869\n811 / 869\n812 / 869\n813 / 869\n814 / 869\n815 / 869\n816 / 869\n817 / 869\n818 / 869\n819 / 869\n820 / 869\n821 / 869\n822 / 869\n823 / 869\n824 / 869\n825 / 869\n826 / 869\n827 / 869\n828 / 869\n829 / 869\n830 / 869\n831 / 869\n832 / 869\n833 / 869\n834 / 869\n835 / 869\n836 / 869\n837 / 869\n838 / 869\n839 / 869\n840 / 869\n841 / 869\n842 / 869\n843 / 869\n844 / 869\n845 / 869\n846 / 869\n847 / 869\n848 / 869\n849 / 869\n850 / 869\n851 / 869\n852 / 869\n853 / 869\n854 / 869\n855 / 869\n856 / 869\n857 / 869\n858 / 869\n859 / 869\n860 / 869\n861 / 869\n862 / 869\n863 / 869\n864 / 869\n865 / 869\n866 / 869\n867 / 869\n868 / 869\n</pre>"},{"location":"examples/book_translation/translate_latex_book/#Translate-a-book-writen-in-LaTeX-from-Slovenian-into-English","title":"Translate a book writen in LaTeX from Slovenian into English\u00b6","text":"<p>With permission of the author, we will demonstrate how to translate the book Euclidean Plane Geometry, written by Milan Mitrovi\u0107 from Slovenian into English, without modifying any of the LaTeX commands.</p> <p>To achieve this, we will first split the book into chunks, each roughly a page long, then translate each chunk into English, and finally stitch them back together.</p>"},{"location":"examples/book_translation/translate_latex_book/#1.-Read-in-the-data","title":"1. Read in the data\u00b6","text":""},{"location":"examples/book_translation/translate_latex_book/#1.1-Count-the-tokens-in-each-chunk","title":"1.1 Count the tokens in each chunk\u00b6","text":""},{"location":"examples/dalle/Image_generations_edits_and_variations_with_DALL-E/","title":"DALL-E","text":"In\u00a0[1]: Copied! <pre># imports\nimport openai  # OpenAI Python library to make API calls\nimport requests  # used to download images\nimport os  # used to access filepaths\nfrom PIL import Image  # used to print and edit images\n\n# set API key\nopenai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n</pre> # imports import openai  # OpenAI Python library to make API calls import requests  # used to download images import os  # used to access filepaths from PIL import Image  # used to print and edit images  # set API key openai.api_key = os.environ.get(\"OPENAI_API_KEY\")  In\u00a0[2]: Copied! <pre># set a directory to save DALL-E images to\nimage_dir_name = \"images\"\nimage_dir = os.path.join(os.curdir, image_dir_name)\n\n# create the directory if it doesn't yet exist\nif not os.path.isdir(image_dir):\n    os.mkdir(image_dir)\n\n# print the directory to save to\nprint(f\"{image_dir=}\")\n</pre> # set a directory to save DALL-E images to image_dir_name = \"images\" image_dir = os.path.join(os.curdir, image_dir_name)  # create the directory if it doesn't yet exist if not os.path.isdir(image_dir):     os.mkdir(image_dir)  # print the directory to save to print(f\"{image_dir=}\")  <pre>image_dir='./images'\n</pre> In\u00a0[3]: Copied! <pre># create an image\n\n# set the prompt\nprompt = \"A cyberpunk monkey hacker dreaming of a beautiful bunch of bananas, digital art\"\n\n# call the OpenAI API\ngeneration_response = openai.Image.create(\n    prompt=prompt,\n    n=1,\n    size=\"1024x1024\",\n    response_format=\"url\",\n)\n\n# print response\nprint(generation_response)\n</pre> # create an image  # set the prompt prompt = \"A cyberpunk monkey hacker dreaming of a beautiful bunch of bananas, digital art\"  # call the OpenAI API generation_response = openai.Image.create(     prompt=prompt,     n=1,     size=\"1024x1024\",     response_format=\"url\", )  # print response print(generation_response)  <pre>{\n  \"created\": 1667611641,\n  \"data\": [\n    {\n      \"url\": \"https://oaidalleapiprodscus.blob.core.windows.net/private/org-l89177bnhkme4a44292n5r3j/user-dS3DiwfhpogyYlat6i42W0QF/img-SFJhix3AV4bmPFvqYRJDkssp.png?st=2022-11-05T00%3A27%3A21Z&amp;se=2022-11-05T02%3A27%3A21Z&amp;sp=r&amp;sv=2021-08-06&amp;sr=b&amp;rscd=inline&amp;rsct=image/png&amp;skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&amp;sktid=a48cca56-e6da-484e-a814-9c849652bcb3&amp;skt=2022-11-05T01%3A27%3A21Z&amp;ske=2022-11-06T01%3A27%3A21Z&amp;sks=b&amp;skv=2021-08-06&amp;sig=0ZHl38v5UTFjA7V5Oshu8M58uHI5itEfvo2PX0aO6kA%3D\"\n    }\n  ]\n}\n</pre> <p>Note: If you get this error - <code>AttributeError: module 'openai' has no attribute 'Image'</code> - you'll need to upgrade your OpenAI package to the latest version. You can do this by running <code>pip install openai --upgrade</code> in your terminal.</p> In\u00a0[4]: Copied! <pre># save the image\ngenerated_image_name = \"generated_image.png\"  # any name you like; the filetype should be .png\ngenerated_image_filepath = os.path.join(image_dir, generated_image_name)\ngenerated_image_url = generation_response[\"data\"][0][\"url\"]  # extract image URL from response\ngenerated_image = requests.get(generated_image_url).content  # download the image\n\nwith open(generated_image_filepath, \"wb\") as image_file:\n    image_file.write(generated_image)  # write the image to the file\n</pre> # save the image generated_image_name = \"generated_image.png\"  # any name you like; the filetype should be .png generated_image_filepath = os.path.join(image_dir, generated_image_name) generated_image_url = generation_response[\"data\"][0][\"url\"]  # extract image URL from response generated_image = requests.get(generated_image_url).content  # download the image  with open(generated_image_filepath, \"wb\") as image_file:     image_file.write(generated_image)  # write the image to the file  In\u00a0[5]: Copied! <pre># print the image\nprint(generated_image_filepath)\ndisplay(Image.open(generated_image_filepath))\n</pre> # print the image print(generated_image_filepath) display(Image.open(generated_image_filepath))  <pre>./images/generated_image.png\n</pre> In\u00a0[6]: Copied! <pre># create variations\n\n# call the OpenAI API, using `create_variation` rather than `create`\nvariation_response = openai.Image.create_variation(\n    image=generated_image,  # generated_image is the image generated above\n    n=2,\n    size=\"1024x1024\",\n    response_format=\"url\",\n)\n\n# print response\nprint(variation_response)\n</pre> # create variations  # call the OpenAI API, using `create_variation` rather than `create` variation_response = openai.Image.create_variation(     image=generated_image,  # generated_image is the image generated above     n=2,     size=\"1024x1024\",     response_format=\"url\", )  # print response print(variation_response)  <pre>{\n  \"created\": 1667611666,\n  \"data\": [\n    {\n      \"url\": \"https://oaidalleapiprodscus.blob.core.windows.net/private/org-l89177bnhkme4a44292n5r3j/user-dS3DiwfhpogyYlat6i42W0QF/img-7HTTBl2k9l4Ir4BTHXnJvFz9.png?st=2022-11-05T00%3A27%3A46Z&amp;se=2022-11-05T02%3A27%3A46Z&amp;sp=r&amp;sv=2021-08-06&amp;sr=b&amp;rscd=inline&amp;rsct=image/png&amp;skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&amp;sktid=a48cca56-e6da-484e-a814-9c849652bcb3&amp;skt=2022-11-04T01%3A50%3A22Z&amp;ske=2022-11-05T01%3A50%3A22Z&amp;sks=b&amp;skv=2021-08-06&amp;sig=QlcKhn427bOAQobM8CmpEf3K90OiumP5jOQwkJpcH6Y%3D\"\n    },\n    {\n      \"url\": \"https://oaidalleapiprodscus.blob.core.windows.net/private/org-l89177bnhkme4a44292n5r3j/user-dS3DiwfhpogyYlat6i42W0QF/img-KGKrKGzlsXN0INxaeII2t8XG.png?st=2022-11-05T00%3A27%3A46Z&amp;se=2022-11-05T02%3A27%3A46Z&amp;sp=r&amp;sv=2021-08-06&amp;sr=b&amp;rscd=inline&amp;rsct=image/png&amp;skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&amp;sktid=a48cca56-e6da-484e-a814-9c849652bcb3&amp;skt=2022-11-04T01%3A50%3A22Z&amp;ske=2022-11-05T01%3A50%3A22Z&amp;sks=b&amp;skv=2021-08-06&amp;sig=RbPoAwXMVfdPxKF40ZjVjlclrnzaQZS%2BxzhgkEcYhOk%3D\"\n    }\n  ]\n}\n</pre> In\u00a0[7]: Copied! <pre># save the images\nvariation_urls = [datum[\"url\"] for datum in variation_response[\"data\"]]  # extract URLs\nvariation_images = [requests.get(url).content for url in variation_urls]  # download images\nvariation_image_names = [f\"variation_image_{i}.png\" for i in range(len(variation_images))]  # create names\nvariation_image_filepaths = [os.path.join(image_dir, name) for name in variation_image_names]  # create filepaths\nfor image, filepath in zip(variation_images, variation_image_filepaths):  # loop through the variations\n    with open(filepath, \"wb\") as image_file:  # open the file\n        image_file.write(image)  # write the image to the file\n</pre> # save the images variation_urls = [datum[\"url\"] for datum in variation_response[\"data\"]]  # extract URLs variation_images = [requests.get(url).content for url in variation_urls]  # download images variation_image_names = [f\"variation_image_{i}.png\" for i in range(len(variation_images))]  # create names variation_image_filepaths = [os.path.join(image_dir, name) for name in variation_image_names]  # create filepaths for image, filepath in zip(variation_images, variation_image_filepaths):  # loop through the variations     with open(filepath, \"wb\") as image_file:  # open the file         image_file.write(image)  # write the image to the file  In\u00a0[8]: Copied! <pre># print the original image\nprint(generated_image_filepath)\ndisplay(Image.open(generated_image_filepath))\n\n# print the new variations\nfor variation_image_filepaths in variation_image_filepaths:\n    print(variation_image_filepaths)\n    display(Image.open(variation_image_filepaths))\n</pre> # print the original image print(generated_image_filepath) display(Image.open(generated_image_filepath))  # print the new variations for variation_image_filepaths in variation_image_filepaths:     print(variation_image_filepaths)     display(Image.open(variation_image_filepaths))  <pre>./images/generated_image.png\n</pre> <pre>./images/variation_image_0.png\n</pre> <pre>./images/variation_image_1.png\n</pre> In\u00a0[9]: Copied! <pre># create a mask\nwidth = 1024\nheight = 1024\nmask = Image.new(\"RGBA\", (width, height), (0, 0, 0, 1))  # create an opaque image mask\n\n# set the bottom half to be transparent\nfor x in range(width):\n    for y in range(height // 2, height):  # only loop over the bottom half of the mask\n        # set alpha (A) to zero to turn pixel transparent\n        alpha = 0\n        mask.putpixel((x, y), (0, 0, 0, alpha))\n\n# save the mask\nmask_name = \"bottom_half_mask.png\"\nmask_filepath = os.path.join(image_dir, mask_name)\nmask.save(mask_filepath)\n</pre> # create a mask width = 1024 height = 1024 mask = Image.new(\"RGBA\", (width, height), (0, 0, 0, 1))  # create an opaque image mask  # set the bottom half to be transparent for x in range(width):     for y in range(height // 2, height):  # only loop over the bottom half of the mask         # set alpha (A) to zero to turn pixel transparent         alpha = 0         mask.putpixel((x, y), (0, 0, 0, alpha))  # save the mask mask_name = \"bottom_half_mask.png\" mask_filepath = os.path.join(image_dir, mask_name) mask.save(mask_filepath)  In\u00a0[10]: Copied! <pre># edit an image\n\n# call the OpenAI API\nedit_response = openai.Image.create_edit(\n    image=open(generated_image_filepath, \"rb\"),  # from the generation section\n    mask=open(mask_filepath, \"rb\"),  # from right above\n    prompt=prompt,  # from the generation section\n    n=1,\n    size=\"1024x1024\",\n    response_format=\"url\",\n)\n\n# print response\nprint(edit_response)\n</pre> # edit an image  # call the OpenAI API edit_response = openai.Image.create_edit(     image=open(generated_image_filepath, \"rb\"),  # from the generation section     mask=open(mask_filepath, \"rb\"),  # from right above     prompt=prompt,  # from the generation section     n=1,     size=\"1024x1024\",     response_format=\"url\", )  # print response print(edit_response)  <pre>{\n  \"created\": 1667611683,\n  \"data\": [\n    {\n      \"url\": \"https://oaidalleapiprodscus.blob.core.windows.net/private/org-l89177bnhkme4a44292n5r3j/user-dS3DiwfhpogyYlat6i42W0QF/img-F5XQFFBLrN7LdXuG5CkQJpxr.png?st=2022-11-05T00%3A28%3A03Z&amp;se=2022-11-05T02%3A28%3A03Z&amp;sp=r&amp;sv=2021-08-06&amp;sr=b&amp;rscd=inline&amp;rsct=image/png&amp;skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&amp;sktid=a48cca56-e6da-484e-a814-9c849652bcb3&amp;skt=2022-11-04T02%3A06%3A29Z&amp;ske=2022-11-05T02%3A06%3A29Z&amp;sks=b&amp;skv=2021-08-06&amp;sig=2UhH%2BkKdvDVoRcgWJhmNFVzpvLzBAZpnA/tU80Zc8M0%3D\"\n    }\n  ]\n}\n</pre> In\u00a0[11]: Copied! <pre># save the image\nedited_image_name = \"edited_image.png\"  # any name you like; the filetype should be .png\nedited_image_filepath = os.path.join(image_dir, edited_image_name)\nedited_image_url = edit_response[\"data\"][0][\"url\"]  # extract image URL from response\nedited_image = requests.get(edited_image_url).content  # download the image\n\nwith open(edited_image_filepath, \"wb\") as image_file:\n    image_file.write(edited_image)  # write the image to the file\n</pre> # save the image edited_image_name = \"edited_image.png\"  # any name you like; the filetype should be .png edited_image_filepath = os.path.join(image_dir, edited_image_name) edited_image_url = edit_response[\"data\"][0][\"url\"]  # extract image URL from response edited_image = requests.get(edited_image_url).content  # download the image  with open(edited_image_filepath, \"wb\") as image_file:     image_file.write(edited_image)  # write the image to the file  In\u00a0[12]: Copied! <pre># print the original image\nprint(generated_image_filepath)\ndisplay(Image.open(generated_image_filepath))\n\n# print edited image\nprint(edited_image_filepath)\ndisplay(Image.open(edited_image_filepath))\n</pre> # print the original image print(generated_image_filepath) display(Image.open(generated_image_filepath))  # print edited image print(edited_image_filepath) display(Image.open(edited_image_filepath))  <pre>./images/generated_image.png\n</pre> <pre>./images/edited_image.png\n</pre>"},{"location":"examples/dalle/Image_generations_edits_and_variations_with_DALL-E/#DALL-E","title":"DALL-E\u00b6","text":"<p>This notebook shows how to use OpenAI's DALL-E image API endpoints.</p> <p>There are three API endpoints:</p> <ul> <li>Generations: generates an image or images based on an input caption</li> <li>Edits: edits or extends an existing image</li> <li>Variations: generates variations of an input image</li> </ul>"},{"location":"examples/dalle/Image_generations_edits_and_variations_with_DALL-E/#Setup","title":"Setup\u00b6","text":"<ul> <li>Import the packages you'll need</li> <li>Import your OpenAI API key: You can do this by running ``export OPENAI_API_KEY=\"your API key\"<code>\\</code> in your terminal.</li> <li>Set a directory to save images to</li> </ul>"},{"location":"examples/dalle/Image_generations_edits_and_variations_with_DALL-E/#Generations","title":"Generations\u00b6","text":"<p>The generation API endpoint creates an image based on a text prompt.</p> <p>Required inputs:</p> <ul> <li>prompt (str): A text description of the desired image(s). The maximum length is 1000 characters.</li> </ul> <p>Optional inputs:</p> <ul> <li>n (int): The number of images to generate. Must be between 1 and 10. Defaults to 1.</li> <li>size (str): The size of the generated images. Must be one of \"256x256\", \"512x512\", or \"1024x1024\". Smaller images are faster. Defaults to \"1024x1024\".</li> <li>response_format (str): The format in which the generated images are returned. Must be one of \"url\" or \"b64_json\". Defaults to \"url\".</li> <li>user (str): A unique identifier representing your end-user, which will help OpenAI to monitor and detect abuse. Learn more.</li> </ul>"},{"location":"examples/dalle/Image_generations_edits_and_variations_with_DALL-E/#Variations","title":"Variations\u00b6","text":"<p>The variations endpoint generates new images (variations) similar to an input image.</p> <p>Here we'll generate variations of the image generated above.</p> <p>Required inputs:</p> <ul> <li>image (str): The image to use as the basis for the variation(s). Must be a valid PNG file, less than 4MB, and square.</li> </ul> <p>Optional inputs:</p> <ul> <li>n (int): The number of images to generate. Must be between 1 and 10. Defaults to 1.</li> <li>size (str): The size of the generated images. Must be one of \"256x256\", \"512x512\", or \"1024x1024\". Smaller images are faster. Defaults to \"1024x1024\".</li> <li>response_format (str): The format in which the generated images are returned. Must be one of \"url\" or \"b64_json\". Defaults to \"url\".</li> <li>user (str): A unique identifier representing your end-user, which will help OpenAI to monitor and detect abuse. Learn more.</li> </ul>"},{"location":"examples/dalle/Image_generations_edits_and_variations_with_DALL-E/#Edits","title":"Edits\u00b6","text":"<p>The edit endpoint uses DALL-E to generate a specified portion of an existing image. Three inputs are needed: the image to edit, a mask specifying the portion to be regenerated, and a prompt describing the desired image.</p> <p>Required inputs:</p> <ul> <li>image (str): The image to edit. Must be a valid PNG file, less than 4MB, and square.</li> <li>mask (str): An additional image whose fully transparent areas (e.g. where alpha is zero) indicate where <code>image</code> should be edited. Must be a valid PNG file, less than 4MB, and have the same dimensions as <code>image</code>.</li> <li>prompt (str): A text description of the desired image(s). The maximum length is 1000 characters.</li> </ul> <p>Optional inputs:</p> <ul> <li>n (int): The number of images to generate. Must be between 1 and 10. Defaults to 1.</li> <li>size (str): The size of the generated images. Must be one of \"256x256\", \"512x512\", or \"1024x1024\". Smaller images are faster. Defaults to \"1024x1024\".</li> <li>response_format (str): The format in which the generated images are returned. Must be one of \"url\" or \"b64_json\". Defaults to \"url\".</li> <li>user (str): A unique identifier representing your end-user, which will help OpenAI to monitor and detect abuse. Learn more.</li> </ul>"},{"location":"examples/dalle/Image_generations_edits_and_variations_with_DALL-E/#Set-Edit-Area","title":"Set Edit Area\u00b6","text":"<p>An edit requires a \"mask\" to specify which portion of the image to regenerate. Any pixel with an alpha of 0 (transparent) will be regenerated. The code below creates a 1024x1024 mask where the bottom half is transparent.</p>"},{"location":"examples/dalle/Image_generations_edits_and_variations_with_DALL-E/#Perform-Edit","title":"Perform Edit\u00b6","text":"<p>Now we supply our image, caption and mask to the API to get 5 examples of edits to our image</p>"},{"location":"examples/fine-tuned_qa/answers_with_ft/","title":"Answers with ft","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nNote: To answer questions based on text documents, we recommend the procedure in \n[Question Answering using Embeddings](https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb).\nSome of the code below may rely on [deprecated API endpoints](https://github.com/openai/openai-cookbook/tree/main/transition_guides_for_deprecated_API_endpoints).\n\"\"\"\n</pre> \"\"\" Note: To answer questions based on text documents, we recommend the procedure in  [Question Answering using Embeddings](https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb). Some of the code below may rely on [deprecated API endpoints](https://github.com/openai/openai-cookbook/tree/main/transition_guides_for_deprecated_API_endpoints). \"\"\" In\u00a0[\u00a0]: Copied! <pre>import argparse\n</pre> import argparse In\u00a0[\u00a0]: Copied! <pre>import openai\n</pre> import openai In\u00a0[\u00a0]: Copied! <pre>def create_context(\n    question, search_file_id, max_len=1800, search_model=\"ada\", max_rerank=10\n):\n\"\"\"\n    Create a context for a question by finding the most similar context from the search file.\n    :param question: The question\n    :param search_file_id: The file id of the search file\n    :param max_len: The maximum length of the returned context (in tokens)\n    :param search_model: The search model to use\n    :param max_rerank: The maximum number of reranking\n    :return: The context\n    \"\"\"\n    results = openai.Engine(search_model).search(\n        search_model=search_model,\n        query=question,\n        max_rerank=max_rerank,\n        file=search_file_id,\n        return_metadata=True,\n    )\n    returns = []\n    cur_len = 0\n    for result in results[\"data\"]:\n        cur_len += int(result[\"metadata\"]) + 4\n        if cur_len &gt; max_len:\n            break\n        returns.append(result[\"text\"])\n    return \"\\n\\n###\\n\\n\".join(returns)\n</pre> def create_context(     question, search_file_id, max_len=1800, search_model=\"ada\", max_rerank=10 ):     \"\"\"     Create a context for a question by finding the most similar context from the search file.     :param question: The question     :param search_file_id: The file id of the search file     :param max_len: The maximum length of the returned context (in tokens)     :param search_model: The search model to use     :param max_rerank: The maximum number of reranking     :return: The context     \"\"\"     results = openai.Engine(search_model).search(         search_model=search_model,         query=question,         max_rerank=max_rerank,         file=search_file_id,         return_metadata=True,     )     returns = []     cur_len = 0     for result in results[\"data\"]:         cur_len += int(result[\"metadata\"]) + 4         if cur_len &gt; max_len:             break         returns.append(result[\"text\"])     return \"\\n\\n###\\n\\n\".join(returns) In\u00a0[\u00a0]: Copied! <pre>def answer_question(\n    search_file_id=\"&lt;SEARCH_FILE_ID&gt;\",\n    fine_tuned_qa_model=\"&lt;FT_QA_MODEL_ID&gt;\",\n    question=\"Which country won the European Football championship in 2021?\",\n    max_len=1800,\n    search_model=\"ada\",\n    max_rerank=10,\n    debug=False,\n    stop_sequence=[\"\\n\", \".\"],\n    max_tokens=100,\n):\n\"\"\"\n    Answer a question based on the most similar context from the search file, using your fine-tuned model.\n    :param question: The question\n    :param fine_tuned_qa_model: The fine tuned QA model\n    :param search_file_id: The file id of the search file\n    :param max_len: The maximum length of the returned context (in tokens)\n    :param search_model: The search model to use\n    :param max_rerank: The maximum number of reranking\n    :param debug: Whether to output debug information\n    :param stop_sequence: The stop sequence for Q&amp;A model\n    :param max_tokens: The maximum number of tokens to return\n    :return: The answer\n    \"\"\"\n    context = create_context(\n        question,\n        search_file_id,\n        max_len=max_len,\n        search_model=search_model,\n        max_rerank=max_rerank,\n    )\n    if debug:\n        print(\"Context:\\n\" + context)\n        print(\"\\n\\n\")\n    try:\n        # fine-tuned models requires model parameter, whereas other models require engine parameter\n        model_param = (\n            {\"model\": fine_tuned_qa_model}\n            if \":\" in fine_tuned_qa_model\n            and fine_tuned_qa_model.split(\":\")[1].startswith(\"ft\")\n            else {\"engine\": fine_tuned_qa_model}\n        )\n        response = openai.Completion.create(\n            prompt=f\"Answer the question based on the context below\\n\\nText: {context}\\n\\n---\\n\\nQuestion: {question}\\nAnswer:\",\n            temperature=0,\n            max_tokens=max_tokens,\n            top_p=1,\n            frequency_penalty=0,\n            presence_penalty=0,\n            stop=stop_sequence,\n            **model_param,\n        )\n        return response[\"choices\"][0][\"text\"]\n    except Exception as e:\n        print(e)\n        return \"\"\n</pre> def answer_question(     search_file_id=\"\",     fine_tuned_qa_model=\"\",     question=\"Which country won the European Football championship in 2021?\",     max_len=1800,     search_model=\"ada\",     max_rerank=10,     debug=False,     stop_sequence=[\"\\n\", \".\"],     max_tokens=100, ):     \"\"\"     Answer a question based on the most similar context from the search file, using your fine-tuned model.     :param question: The question     :param fine_tuned_qa_model: The fine tuned QA model     :param search_file_id: The file id of the search file     :param max_len: The maximum length of the returned context (in tokens)     :param search_model: The search model to use     :param max_rerank: The maximum number of reranking     :param debug: Whether to output debug information     :param stop_sequence: The stop sequence for Q&amp;A model     :param max_tokens: The maximum number of tokens to return     :return: The answer     \"\"\"     context = create_context(         question,         search_file_id,         max_len=max_len,         search_model=search_model,         max_rerank=max_rerank,     )     if debug:         print(\"Context:\\n\" + context)         print(\"\\n\\n\")     try:         # fine-tuned models requires model parameter, whereas other models require engine parameter         model_param = (             {\"model\": fine_tuned_qa_model}             if \":\" in fine_tuned_qa_model             and fine_tuned_qa_model.split(\":\")[1].startswith(\"ft\")             else {\"engine\": fine_tuned_qa_model}         )         response = openai.Completion.create(             prompt=f\"Answer the question based on the context below\\n\\nText: {context}\\n\\n---\\n\\nQuestion: {question}\\nAnswer:\",             temperature=0,             max_tokens=max_tokens,             top_p=1,             frequency_penalty=0,             presence_penalty=0,             stop=stop_sequence,             **model_param,         )         return response[\"choices\"][0][\"text\"]     except Exception as e:         print(e)         return \"\" In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"Rudimentary functionality of the answers endpoint with a fine-tuned Q&amp;A model.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(\n        \"--search_file_id\", help=\"Search file id\", required=True, type=str\n    )\n    parser.add_argument(\n        \"--fine_tuned_qa_model\", help=\"Fine-tuned QA model id\", required=True, type=str\n    )\n    parser.add_argument(\n        \"--question\", help=\"Question to answer\", required=True, type=str\n    )\n    parser.add_argument(\n        \"--max_len\",\n        help=\"Maximum length of the returned context (in tokens)\",\n        default=1800,\n        type=int,\n    )\n    parser.add_argument(\n        \"--search_model\", help=\"Search model to use\", default=\"ada\", type=str\n    )\n    parser.add_argument(\n        \"--max_rerank\",\n        help=\"Maximum number of reranking for the search\",\n        default=10,\n        type=int,\n    )\n    parser.add_argument(\n        \"--debug\", help=\"Print debug information (context used)\", action=\"store_true\"\n    )\n    parser.add_argument(\n        \"--stop_sequence\",\n        help=\"Stop sequences for the Q&amp;A model\",\n        default=[\"\\n\", \".\"],\n        nargs=\"+\",\n        type=str,\n    )\n    parser.add_argument(\n        \"--max_tokens\",\n        help=\"Maximum number of tokens to return\",\n        default=100,\n        type=int,\n    )\n    args = parser.parse_args()\n    response = answer_question(\n        search_file_id=args.search_file_id,\n        fine_tuned_qa_model=args.fine_tuned_qa_model,\n        question=args.question,\n        max_len=args.max_len,\n        search_model=args.search_model,\n        max_rerank=args.max_rerank,\n        debug=args.debug,\n        stop_sequence=args.stop_sequence,\n        max_tokens=args.max_tokens,\n    )\n    print(f\"Answer:{response}\")\n</pre> if __name__ == \"__main__\":     parser = argparse.ArgumentParser(         description=\"Rudimentary functionality of the answers endpoint with a fine-tuned Q&amp;A model.\",         formatter_class=argparse.ArgumentDefaultsHelpFormatter,     )     parser.add_argument(         \"--search_file_id\", help=\"Search file id\", required=True, type=str     )     parser.add_argument(         \"--fine_tuned_qa_model\", help=\"Fine-tuned QA model id\", required=True, type=str     )     parser.add_argument(         \"--question\", help=\"Question to answer\", required=True, type=str     )     parser.add_argument(         \"--max_len\",         help=\"Maximum length of the returned context (in tokens)\",         default=1800,         type=int,     )     parser.add_argument(         \"--search_model\", help=\"Search model to use\", default=\"ada\", type=str     )     parser.add_argument(         \"--max_rerank\",         help=\"Maximum number of reranking for the search\",         default=10,         type=int,     )     parser.add_argument(         \"--debug\", help=\"Print debug information (context used)\", action=\"store_true\"     )     parser.add_argument(         \"--stop_sequence\",         help=\"Stop sequences for the Q&amp;A model\",         default=[\"\\n\", \".\"],         nargs=\"+\",         type=str,     )     parser.add_argument(         \"--max_tokens\",         help=\"Maximum number of tokens to return\",         default=100,         type=int,     )     args = parser.parse_args()     response = answer_question(         search_file_id=args.search_file_id,         fine_tuned_qa_model=args.fine_tuned_qa_model,         question=args.question,         max_len=args.max_len,         search_model=args.search_model,         max_rerank=args.max_rerank,         debug=args.debug,         stop_sequence=args.stop_sequence,         max_tokens=args.max_tokens,     )     print(f\"Answer:{response}\")"},{"location":"examples/fine-tuned_qa/olympics-1-collect-data/","title":"1. Collect Wikipedia data about Olympic Games 2020","text":"<p>Note: To answer questions based on text documents, we recommend the procedure in Question Answering using Embeddings. Some of the code below may rely on deprecated API endpoints.</p> In\u00a0[1]: Copied! <pre>import pandas as pd\nimport wikipedia\n\n\ndef filter_olympic_2020_titles(titles):\n\"\"\"\n    Get the titles which are related to Olympic games hosted in 2020, given a list of titles\n    \"\"\"\n    titles = [title for title in titles if '2020' in title and 'olympi' in title.lower()]\n    \n    return titles\n\ndef get_wiki_page(title):\n\"\"\"\n    Get the wikipedia page given a title\n    \"\"\"\n    try:\n        return wikipedia.page(title)\n    except wikipedia.exceptions.DisambiguationError as e:\n        return wikipedia.page(e.options[0])\n    except wikipedia.exceptions.PageError as e:\n        return None\n\ndef recursively_find_all_pages(titles, titles_so_far=set()):\n\"\"\"\n    Recursively find all the pages that are linked to the Wikipedia titles in the list\n    \"\"\"\n    all_pages = []\n    \n    titles = list(set(titles) - titles_so_far)\n    titles = filter_olympic_2020_titles(titles)\n    titles_so_far.update(titles)\n    for title in titles:\n        page = get_wiki_page(title)\n        if page is None:\n            continue\n        all_pages.append(page)\n\n        new_pages = recursively_find_all_pages(page.links, titles_so_far)\n        for pg in new_pages:\n            if pg.title not in [p.title for p in all_pages]:\n                all_pages.append(pg)\n        titles_so_far.update(page.links)\n    return all_pages\n\n\npages = recursively_find_all_pages([\"2020 Summer Olympics\"])\nlen(pages)\n</pre> import pandas as pd import wikipedia   def filter_olympic_2020_titles(titles):     \"\"\"     Get the titles which are related to Olympic games hosted in 2020, given a list of titles     \"\"\"     titles = [title for title in titles if '2020' in title and 'olympi' in title.lower()]          return titles  def get_wiki_page(title):     \"\"\"     Get the wikipedia page given a title     \"\"\"     try:         return wikipedia.page(title)     except wikipedia.exceptions.DisambiguationError as e:         return wikipedia.page(e.options[0])     except wikipedia.exceptions.PageError as e:         return None  def recursively_find_all_pages(titles, titles_so_far=set()):     \"\"\"     Recursively find all the pages that are linked to the Wikipedia titles in the list     \"\"\"     all_pages = []          titles = list(set(titles) - titles_so_far)     titles = filter_olympic_2020_titles(titles)     titles_so_far.update(titles)     for title in titles:         page = get_wiki_page(title)         if page is None:             continue         all_pages.append(page)          new_pages = recursively_find_all_pages(page.links, titles_so_far)         for pg in new_pages:             if pg.title not in [p.title for p in all_pages]:                 all_pages.append(pg)         titles_so_far.update(page.links)     return all_pages   pages = recursively_find_all_pages([\"2020 Summer Olympics\"]) len(pages) Out[1]: <pre>909</pre> In\u00a0[2]: Copied! <pre>import re\nfrom typing import Set\nfrom transformers import GPT2TokenizerFast\n\nimport numpy as np\nfrom nltk.tokenize import sent_tokenize\n\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n\ndef count_tokens(text: str) -&gt; int:\n\"\"\"count the number of tokens in a string\"\"\"\n    return len(tokenizer.encode(text))\n\ndef reduce_long(\n    long_text: str, long_text_tokens: bool = False, max_len: int = 590\n) -&gt; str:\n\"\"\"\n    Reduce a long text to a maximum of `max_len` tokens by potentially cutting at a sentence end\n    \"\"\"\n    if not long_text_tokens:\n        long_text_tokens = count_tokens(long_text)\n    if long_text_tokens &gt; max_len:\n        sentences = sent_tokenize(long_text.replace(\"\\n\", \" \"))\n        ntokens = 0\n        for i, sentence in enumerate(sentences):\n            ntokens += 1 + count_tokens(sentence)\n            if ntokens &gt; max_len:\n                return \". \".join(sentences[:i][:-1]) + \".\"\n\n    return long_text\n\ndiscard_categories = ['See also', 'References', 'External links', 'Further reading', \"Footnotes\",\n    \"Bibliography\", \"Sources\", \"Citations\", \"Literature\", \"Footnotes\", \"Notes and references\",\n    \"Photo gallery\", \"Works cited\", \"Photos\", \"Gallery\", \"Notes\", \"References and sources\",\n    \"References and notes\",]\n\n\ndef extract_sections(\n    wiki_text: str,\n    title: str,\n    max_len: int = 1500,\n    discard_categories: Set[str] = discard_categories,\n) -&gt; str:\n\"\"\"\n    Extract the sections of a Wikipedia page, discarding the references and other low information sections\n    \"\"\"\n    if len(wiki_text) == 0:\n        return []\n\n    # find all headings and the coresponding contents\n    headings = re.findall(\"==+ .* ==+\", wiki_text)\n    for heading in headings:\n        wiki_text = wiki_text.replace(heading, \"==+ !! ==+\")\n    contents = wiki_text.split(\"==+ !! ==+\")\n    contents = [c.strip() for c in contents]\n    assert len(headings) == len(contents) - 1\n\n    cont = contents.pop(0).strip()\n    outputs = [(title, \"Summary\", cont, count_tokens(cont)+4)]\n    \n    # discard the discard categories, accounting for a tree structure\n    max_level = 100\n    keep_group_level = max_level\n    remove_group_level = max_level\n    nheadings, ncontents = [], []\n    for heading, content in zip(headings, contents):\n        plain_heading = \" \".join(heading.split(\" \")[1:-1])\n        num_equals = len(heading.split(\" \")[0])\n        if num_equals &lt;= keep_group_level:\n            keep_group_level = max_level\n\n        if num_equals &gt; remove_group_level:\n            if (\n                num_equals &lt;= keep_group_level\n            ):\n                continue\n        keep_group_level = max_level\n        if plain_heading in discard_categories:\n            remove_group_level = num_equals\n            keep_group_level = max_level\n            continue\n        nheadings.append(heading.replace(\"=\", \"\").strip())\n        ncontents.append(content)\n        remove_group_level = max_level\n\n    # count the tokens of each section\n    ncontent_ntokens = [\n        count_tokens(c)\n        + 3\n        + count_tokens(\" \".join(h.split(\" \")[1:-1]))\n        - (1 if len(c) == 0 else 0)\n        for h, c in zip(nheadings, ncontents)\n    ]\n\n    # Create a tuple of (title, section_name, content, number of tokens)\n    outputs += [(title, h, c, t) if t&lt;max_len \n                else (title, h, reduce_long(c, max_len), count_tokens(reduce_long(c,max_len))) \n                    for h, c, t in zip(nheadings, ncontents, ncontent_ntokens)]\n    \n    return outputs\n\n# Example page being processed into sections\nbermuda_page = get_wiki_page('Bermuda at the 2020 Summer Olympics')\nber = extract_sections(bermuda_page.content, bermuda_page.title)\n\n# Example section\nber[-1]\n</pre>  import re from typing import Set from transformers import GPT2TokenizerFast  import numpy as np from nltk.tokenize import sent_tokenize  tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")  def count_tokens(text: str) -&gt; int:     \"\"\"count the number of tokens in a string\"\"\"     return len(tokenizer.encode(text))  def reduce_long(     long_text: str, long_text_tokens: bool = False, max_len: int = 590 ) -&gt; str:     \"\"\"     Reduce a long text to a maximum of `max_len` tokens by potentially cutting at a sentence end     \"\"\"     if not long_text_tokens:         long_text_tokens = count_tokens(long_text)     if long_text_tokens &gt; max_len:         sentences = sent_tokenize(long_text.replace(\"\\n\", \" \"))         ntokens = 0         for i, sentence in enumerate(sentences):             ntokens += 1 + count_tokens(sentence)             if ntokens &gt; max_len:                 return \". \".join(sentences[:i][:-1]) + \".\"      return long_text  discard_categories = ['See also', 'References', 'External links', 'Further reading', \"Footnotes\",     \"Bibliography\", \"Sources\", \"Citations\", \"Literature\", \"Footnotes\", \"Notes and references\",     \"Photo gallery\", \"Works cited\", \"Photos\", \"Gallery\", \"Notes\", \"References and sources\",     \"References and notes\",]   def extract_sections(     wiki_text: str,     title: str,     max_len: int = 1500,     discard_categories: Set[str] = discard_categories, ) -&gt; str:     \"\"\"     Extract the sections of a Wikipedia page, discarding the references and other low information sections     \"\"\"     if len(wiki_text) == 0:         return []      # find all headings and the coresponding contents     headings = re.findall(\"==+ .* ==+\", wiki_text)     for heading in headings:         wiki_text = wiki_text.replace(heading, \"==+ !! ==+\")     contents = wiki_text.split(\"==+ !! ==+\")     contents = [c.strip() for c in contents]     assert len(headings) == len(contents) - 1      cont = contents.pop(0).strip()     outputs = [(title, \"Summary\", cont, count_tokens(cont)+4)]          # discard the discard categories, accounting for a tree structure     max_level = 100     keep_group_level = max_level     remove_group_level = max_level     nheadings, ncontents = [], []     for heading, content in zip(headings, contents):         plain_heading = \" \".join(heading.split(\" \")[1:-1])         num_equals = len(heading.split(\" \")[0])         if num_equals &lt;= keep_group_level:             keep_group_level = max_level          if num_equals &gt; remove_group_level:             if (                 num_equals &lt;= keep_group_level             ):                 continue         keep_group_level = max_level         if plain_heading in discard_categories:             remove_group_level = num_equals             keep_group_level = max_level             continue         nheadings.append(heading.replace(\"=\", \"\").strip())         ncontents.append(content)         remove_group_level = max_level      # count the tokens of each section     ncontent_ntokens = [         count_tokens(c)         + 3         + count_tokens(\" \".join(h.split(\" \")[1:-1]))         - (1 if len(c) == 0 else 0)         for h, c in zip(nheadings, ncontents)     ]      # Create a tuple of (title, section_name, content, number of tokens)     outputs += [(title, h, c, t) if t Out[2]: <pre>('Bermuda at the 2020 Summer Olympics',\n 'Equestrian',\n \"Bermuda entered one dressage rider into the Olympic competition by finishing in the top four, outside the group selection, of the individual FEI Olympic Rankings for Groups D and E (North, Central, and South America), marking the country's recurrence to the sport after an eight-year absence. The quota was later withdrawn, following an injury of Annabelle Collins' main horse Joyero and a failure to obtain minimum eligibility requirements (MER) aboard a new horse Chuppy Checker.\",\n 104)</pre> In\u00a0[3]: Copied! <pre>res = []\nfor page in pages:\n    res += extract_sections(page.content, page.title)\ndf = pd.DataFrame(res, columns=[\"title\", \"heading\", \"content\", \"tokens\"])\ndf = df[df.tokens&gt;40]\ndf = df.drop_duplicates(['title','heading'])\ndf = df.reset_index().drop('index',axis=1) # reset index\ndf.head()\n</pre> res = [] for page in pages:     res += extract_sections(page.content, page.title) df = pd.DataFrame(res, columns=[\"title\", \"heading\", \"content\", \"tokens\"]) df = df[df.tokens&gt;40] df = df.drop_duplicates(['title','heading']) df = df.reset_index().drop('index',axis=1) # reset index df.head() <pre>Token indices sequence length is longer than the specified maximum sequence length for this model (1060 &gt; 1024). Running this sequence through the model will result in indexing errors\n</pre> Out[3]: title heading content tokens 0 2020 Summer Olympics Summary The 2020 Summer Olympics (Japanese: 2020\u5e74\u590f\u5b63\u30aa\u30ea\u30f3... 713 1 2020 Summer Olympics Host city selection The International Olympic Committee (IOC) vote... 126 2 2020 Summer Olympics Impact of the COVID-19 pandemic In January 2020, concerns were raised about th... 369 3 2020 Summer Olympics Qualifying event cancellation and postponement Concerns about the pandemic began to affect qu... 298 4 2020 Summer Olympics Effect on doping tests Mandatory doping tests were being severely res... 163 In\u00a0[4]: Copied! <pre>df.to_csv('olympics-data/olympics_sections.csv', index=False)\n</pre> df.to_csv('olympics-data/olympics_sections.csv', index=False) In\u00a0[5]: Copied! <pre>df.title.value_counts().head()\n</pre> df.title.value_counts().head() Out[5]: <pre>Concerns and controversies at the 2020 Summer Olympics    51\nUnited States at the 2020 Summer Olympics                 46\nGreat Britain at the 2020 Summer Olympics                 42\nCanada at the 2020 Summer Olympics                        39\nOlympic Games                                             39\nName: title, dtype: int64</pre> <p>There appear to be winter and summer Olympics 2020. We chose to leave a little ambiguity and noise in the dataset, even though we were interested in only Summer Olympics 2020.</p> In\u00a0[6]: Copied! <pre>df.title.str.contains('Summer').value_counts()\n</pre> df.title.str.contains('Summer').value_counts() Out[6]: <pre>True     3567\nFalse     305\nName: title, dtype: int64</pre> In\u00a0[7]: Copied! <pre>df.title.str.contains('Winter').value_counts()\n</pre> df.title.str.contains('Winter').value_counts() Out[7]: <pre>False    3774\nTrue       98\nName: title, dtype: int64</pre> In\u00a0[8]: Copied! <pre>import pandas as pd\nfrom matplotlib import pyplot as plt\n\ndf = pd.read_csv('olympics-data/olympics_sections.csv')\ndf[['tokens']].hist()\n# add axis descriptions and title\nplt.xlabel('Number of tokens')\nplt.ylabel('Number of Wikipedia sections')\nplt.title('Distribution of number of tokens in Wikipedia sections')\nplt.show()\n</pre> import pandas as pd from matplotlib import pyplot as plt  df = pd.read_csv('olympics-data/olympics_sections.csv') df[['tokens']].hist() # add axis descriptions and title plt.xlabel('Number of tokens') plt.ylabel('Number of Wikipedia sections') plt.title('Distribution of number of tokens in Wikipedia sections') plt.show() <p>We can see that the majority of section are fairly short (less than 500 tokens).</p>"},{"location":"examples/fine-tuned_qa/olympics-1-collect-data/#1.-Collect-Wikipedia-data-about-Olympic-Games-2020","title":"1. Collect Wikipedia data about Olympic Games 2020\u00b6","text":"<p>The idea of this project is to create a question answering model, based on a few paragraphs of provided text. Base GPT-3 models do a good job at answering questions when the answer is contained within the paragraph, however if the answer isn't contained, the base models tend to try their best to answer anyway, often leading to confabulated answers.</p> <p>To create a model which answers questions only if there is sufficient context for doing so, we first create a dataset of questions and answers based on paragraphs of text. In order to train the model to answer only when the answer is present, we also add adversarial examples, where the question doesn't match the context. In those cases, we ask the model to output \"No sufficient context for answering the question\".</p> <p>We will perform this task in three notebooks:</p> <ol> <li>The first (this) notebook focuses on collecting recent data, which GPT-3 didn't see during it's pre-training. We picked the topic of Olympic Games 2020 (which actually took place in the summer of 2021), and downloaded 713 unique pages. We organized the dataset by individual sections, which will serve as context for asking and answering the questions.</li> <li>The second notebook will utilize Davinci-instruct to ask a few questions based on a Wikipedia section, as well as answer those questions, based on that section.</li> <li>The third notebook will utilize the dataset of context, question and answer pairs to additionally create adversarial questions and context pairs, where the question was not generated on that context. In those cases the model will be prompted to answer \"No sufficient context for answering the question\". We will also train a discriminator model, which predicts whether the question can be answered based on the context or not.</li> </ol>"},{"location":"examples/fine-tuned_qa/olympics-1-collect-data/#1.1-Data-extraction-using-the-wikipedia-API","title":"1.1 Data extraction using the wikipedia API\u00b6","text":"<p>Extracting the data will take about half an hour, and processing will likely take about as much.</p>"},{"location":"examples/fine-tuned_qa/olympics-1-collect-data/#1.2-Filtering-the-Wikipedia-pages-and-splitting-them-into-sections-by-headings","title":"1.2 Filtering the Wikipedia pages and splitting them into sections by headings\u00b6","text":"<p>We remove sections unlikely to contain textual information, and ensure that each section is not longer than the token limit</p>"},{"location":"examples/fine-tuned_qa/olympics-1-collect-data/#1.2.1-We-create-a-dataset-and-filter-out-any-sections-with-fewer-than-40-tokens,-as-those-are-unlikely-to-contain-enough-context-to-ask-a-good-question.","title":"1.2.1 We create a dataset and filter out any sections with fewer than 40 tokens, as those are unlikely to contain enough context to ask a good question.\u00b6","text":""},{"location":"examples/fine-tuned_qa/olympics-1-collect-data/#Save-the-section-dataset","title":"Save the section dataset\u00b6","text":"<p>We will save the section dataset, for the next notebook</p>"},{"location":"examples/fine-tuned_qa/olympics-1-collect-data/#1.3-(Optional)-Exploring-the-data","title":"1.3 (Optional) Exploring the data\u00b6","text":""},{"location":"examples/fine-tuned_qa/olympics-2-create-qa/","title":"2. Creating a synthetic Q&amp;A dataset","text":"<p>Note: To answer questions based on text documents, we recommend the procedure in Question Answering using Embeddings. Some of the code below may rely on deprecated API endpoints.</p> In\u00a0[1]: Copied! <pre>import pandas as pd\ndf = pd.read_csv('olympics-data/olympics_sections.csv')\ndf['context'] = df.title + \"\\n\" + df.heading + \"\\n\\n\" + df.content\ndf.head()\n</pre> import pandas as pd df = pd.read_csv('olympics-data/olympics_sections.csv') df['context'] = df.title + \"\\n\" + df.heading + \"\\n\\n\" + df.content df.head() Out[1]: title heading content tokens context 0 2020 Summer Olympics Summary The 2020 Summer Olympics (Japanese: 2020\u5e74\u590f\u5b63\u30aa\u30ea\u30f3... 713 2020 Summer Olympics\\nSummary\\n\\nThe 2020 Summ... 1 2020 Summer Olympics Host city selection The International Olympic Committee (IOC) vote... 126 2020 Summer Olympics\\nHost city selection\\n\\nT... 2 2020 Summer Olympics Impact of the COVID-19 pandemic In January 2020, concerns were raised about th... 369 2020 Summer Olympics\\nImpact of the COVID-19 p... 3 2020 Summer Olympics Qualifying event cancellation and postponement Concerns about the pandemic began to affect qu... 298 2020 Summer Olympics\\nQualifying event cancell... 4 2020 Summer Olympics Effect on doping tests Mandatory doping tests were being severely res... 163 2020 Summer Olympics\\nEffect on doping tests\\n... In\u00a0[2]: Copied! <pre>import openai\n\ndef get_questions(context):\n    try:\n        response = openai.Completion.create(\n            engine=\"davinci-instruct-beta-v3\",\n            prompt=f\"Write questions based on the text below\\n\\nText: {context}\\n\\nQuestions:\\n1.\",\n            temperature=0,\n            max_tokens=257,\n            top_p=1,\n            frequency_penalty=0,\n            presence_penalty=0,\n            stop=[\"\\n\\n\"]\n        )\n        return response['choices'][0]['text']\n    except:\n        return \"\"\n\n\ndf['questions']= df.context.apply(get_questions)\ndf['questions'] = \"1.\" + df.questions\nprint(df[['questions']].values[0][0])\n</pre> import openai  def get_questions(context):     try:         response = openai.Completion.create(             engine=\"davinci-instruct-beta-v3\",             prompt=f\"Write questions based on the text below\\n\\nText: {context}\\n\\nQuestions:\\n1.\",             temperature=0,             max_tokens=257,             top_p=1,             frequency_penalty=0,             presence_penalty=0,             stop=[\"\\n\\n\"]         )         return response['choices'][0]['text']     except:         return \"\"   df['questions']= df.context.apply(get_questions) df['questions'] = \"1.\" + df.questions print(df[['questions']].values[0][0]) <pre>1. What is the 2020 Summer Olympics?\n2. When did the 2020 Summer Olympics take place?\n3. Who won the most medals at the 2020 Summer Olympics?\n4. Who won the most gold medals at the 2020 Summer Olympics?\n5. Who won the most medals at the 2020 Summer Olympics?\n</pre> <p>The prompt is designed to generate a number of questions. Example questions above were generated based on the summary section of the 2020 Summer Olympics page.</p> <p>We can observe that the questions 3 and 5 above repeat. Sometimes the generated questions could be ambiguous without the context. We will show that even despite these limitations we can create a successful model.</p> In\u00a0[3]: Copied! <pre>print(df.content.values[0])\n</pre> print(df.content.values[0]) <pre>The 2020 Summer Olympics (Japanese: 2020\u5e74\u590f\u5b63\u30aa\u30ea\u30f3\u30d4\u30c3\u30af, Hepburn: Nisen Nij\u016b-nen Kaki Orinpikku), officially the Games of the XXXII Olympiad (\u7b2c\u4e09\u5341\u4e8c\u56de\u30aa\u30ea\u30f3\u30d4\u30c3\u30af\u7af6\u6280\u5927\u4f1a, Dai Sanj\u016bni-kai Orinpikku Ky\u014dgi Taikai) and branded as Tokyo 2020 (\u6771\u4eac2020, T\u014dky\u014d Nii Zero Nii Zero), was an international multi-sport event held from 23 July to 8 August 2021 in Tokyo, Japan, with some preliminary events that began on 21 July.\nTokyo was selected as the host city during the 125th IOC Session in Buenos Aires, Argentina, on 7 September 2013. Originally scheduled to take place from 24 July to 9 August 2020, the event was postponed to 2021 in March 2020 as a result of the COVID-19 pandemic, the first such instance in the history of the Olympic Games (previous games had been cancelled but not rescheduled). However, the event retained the Tokyo 2020 name for marketing and branding purposes. It was largely held behind closed doors with no public spectators permitted due to the declaration of a state of emergency in the Greater Tokyo Area in response to the pandemic. The Summer Paralympics were held between 24 August and 5 September 2021, 16 days after the completion of the Olympics.The 2020 Games were the fourth Olympic Games to be held in Japan, following the Tokyo 1964 (Summer), Sapporo 1972 (Winter) and Nagano 1998 (Winter) games. Tokyo is the first city in Asia to hold the Summer Games twice. The 2020 Games were the second of three consecutive Olympics to be held in East Asia, following the 2018 Winter Olympics in Pyeongchang, South Korea and preceding the 2022 Winter Olympics in Beijing, China.\nNew events were introduced in existing sports for 2020, including 3x3 basketball, freestyle BMX and mixed gender team events in a number of existing sports, as well as the return of madison cycling for men and an introduction of the same event for women. New IOC policies also allowed the host organizing committee to add new sports to the Olympic program for just one Games. The disciplines added by the Japanese Olympic Committee were baseball and softball, karate, sport climbing, surfing and skateboarding, the last four of which made their Olympic debuts, and the last three of which will remain on the Olympic program.The United States topped the medal count by both total golds (39) and total medals (113), with China finishing second by both respects (38 and 88). Host nation Japan finished third, setting a record for the most gold medals and total medals ever won by their delegation at an Olympic Games with 27 and 58. Great Britain finished fourth, with a total of 22 gold and 65 medals, becoming the first nation at the Summer Olympics to increase or equal their total medals won in the two Games subsequent to hosting them. The Russian delegation competing as the ROC (not to be confused with the Republic of China (Taiwan) which competed as Chinese Taipei, not ROC) finished fifth with 20 gold medals and third in the overall medal count, with 71 medals. Bermuda, the Philippines and Qatar won their first-ever Olympic gold medals. Burkina Faso, San Marino and Turkmenistan won their first-ever Olympic medals.\n</pre> In\u00a0[4]: Copied! <pre>def get_answers(row):\n    try:\n        response = openai.Completion.create(\n            engine=\"davinci-instruct-beta-v3\",\n            prompt=f\"Write answer based on the text below\\n\\nText: {row.context}\\n\\nQuestions:\\n{row.questions}\\n\\nAnswers:\\n1.\",\n            temperature=0,\n            max_tokens=257,\n            top_p=1,\n            frequency_penalty=0,\n            presence_penalty=0\n        )\n        return response['choices'][0]['text']\n    except Exception as e:\n        print (e)\n        return \"\"\n\n\ndf['answers']= df.apply(get_answers, axis=1)\ndf['answers'] = \"1.\" + df.answers\ndf = df.dropna().reset_index().drop('index',axis=1)\nprint(df[['answers']].values[0][0])\n</pre> def get_answers(row):     try:         response = openai.Completion.create(             engine=\"davinci-instruct-beta-v3\",             prompt=f\"Write answer based on the text below\\n\\nText: {row.context}\\n\\nQuestions:\\n{row.questions}\\n\\nAnswers:\\n1.\",             temperature=0,             max_tokens=257,             top_p=1,             frequency_penalty=0,             presence_penalty=0         )         return response['choices'][0]['text']     except Exception as e:         print (e)         return \"\"   df['answers']= df.apply(get_answers, axis=1) df['answers'] = \"1.\" + df.answers df = df.dropna().reset_index().drop('index',axis=1) print(df[['answers']].values[0][0]) <pre>1. The 2020 Summer Olympics is an international multi-sport event held from 23 July to 8 August 2021 in Tokyo, Japan.\n2. The 2020 Summer Olympics took place from 23 July to 8 August 2021.\n3. The United States topped the medal count by both total golds (39) and total medals (113), with China finishing second by both respects (38 and 88).\n4. The United States topped the medal count by both total golds (39) and total medals (113), with China finishing second by both respects (38 and 88).\n5. The United States topped the medal count by both total golds (39) and total medals (113), with China finishing second by both respects (38 and 88).\n</pre> <p>These are the answers to the questions above based on the context around the host city selection.</p> <p>We can see that answers 3-5 contain the correct answer, but instead of answering the question directly, the answer is a verbatim extraction. Despite these occasional lower quality answers, we will show that the model can learn the task reasonably well, given a high number of examples.</p> In\u00a0[5]: Copied! <pre>df.to_csv('olympics-data/olympics_qa.csv', index=False)\n</pre> df.to_csv('olympics-data/olympics_qa.csv', index=False) In\u00a0[6]: Copied! <pre>df = df[df.tokens&lt;2000]\ndf[['context', 'tokens']].rename(columns={'context':'text','tokens':'metadata'}).to_json('olympics-data/olympics_search.jsonl', orient='records', lines=True)\n\nsearch_file = openai.File.create(\n  file=open(\"olympics-data/olympics_search.jsonl\"),\n  purpose='search'\n)\nolympics_search_fileid = search_file['id']\n</pre> df = df[df.tokens&lt;2000] df[['context', 'tokens']].rename(columns={'context':'text','tokens':'metadata'}).to_json('olympics-data/olympics_search.jsonl', orient='records', lines=True)  search_file = openai.File.create(   file=open(\"olympics-data/olympics_search.jsonl\"),   purpose='search' ) olympics_search_fileid = search_file['id'] In\u00a0[7]: Copied! <pre>from answers_with_ft import create_context, answer_question\nprint(create_context(\"Where did women's 4 x 100 metres relay event take place during the 2020 Summer Olympics?\", olympics_search_fileid, max_len=400))\n</pre> from answers_with_ft import create_context, answer_question print(create_context(\"Where did women's 4 x 100 metres relay event take place during the 2020 Summer Olympics?\", olympics_search_fileid, max_len=400)) <pre>Athletics at the 2020 Summer Olympics \u2013 Women's 4 \u00d7 100 metres relay\nSummary\n\nThe women's 4 \u00d7 100 metres relay event at the 2020 Summer Olympics took place on 5 and 6 August 2021 at the Japan National Stadium. There were 16 competing relay teams, with each team having 5 members from which 4 were selected in each round.\n\n###\n\nAthletics at the 2020 Summer Olympics \u2013 Men's 4 \u00d7 100 metres relay\nQualification\n\nNational Olympic Committees (NOCs) could qualify one relay team in one of three following ways:\nThe top 8 NOCs at the 2019 World Athletics Championships qualified a relay team.\nThe top 8 NOCs at the 2021 World Athletics Relays qualified a relay team.\nWhere an NOC placed in the top 8 at both the 2019 World Championships and the 2021 World Relays, the quota place was allocated to the world top list as of 29 June 2021. In this case, 4 teams did so, so there are 4 places available through the world rankings.A total of five athletes may be entered for a relay team. Should a NOC have also entered individual athletes in the corresponding individual event (100 m), the entered individual athletes must be included in the total of five (5) athletes entered for the relay event. In addition of five, NOCs can nominate a maximum of one alternate athlete for each team.\nThe qualifying period was originally from 1 May 2019 to 29 June 2020. Due to the COVID-19 pandemic, the period was suspended from 6 April 2020 to 30 November 2020, with the end date extended to 29 June 2021. The qualifying time standards could be obtained in various meets during the given period that have the approval of the IAAF. Both indoor and outdoor meets are eligible. The most recent Area Championships may be counted in the ranking, even if not during the qualifying period.\n</pre> In\u00a0[8]: Copied! <pre>answer_question(olympics_search_fileid, \"davinci-instruct-beta-v3\", \n            \"Where did women's 4 x 100 metres relay event take place during the 2020 Summer Olympics?\")\n</pre> answer_question(olympics_search_fileid, \"davinci-instruct-beta-v3\",              \"Where did women's 4 x 100 metres relay event take place during the 2020 Summer Olympics?\") Out[8]: <pre>' Japan National Stadium'</pre> <p>After we fine-tune the model for Q&amp;A we'll be able to use it instead of <code>davinci-instruct-beta-v3</code>, to obtain better answers when the question can't be answered based on the context. We see a downside of <code>davinci-instruct-beta-v3</code>, which always attempts to answer the question, regardless of the relevant context being present or not. (Note the second question is asking about a future event, set in 2024.)</p> In\u00a0[9]: Copied! <pre>answer_question(olympics_search_fileid, \"davinci-instruct-beta-v3\", \n            \"Where did women's 4 x 100 metres relay event take place during the 2048 Summer Olympics?\", max_len=1000)\n</pre> answer_question(olympics_search_fileid, \"davinci-instruct-beta-v3\",              \"Where did women's 4 x 100 metres relay event take place during the 2048 Summer Olympics?\", max_len=1000) Out[9]: <pre>' Japan National Stadium'</pre> <p>We can see that davinci has a tendency to answer the question, even if the question can't be answered given the context provided. Note the question asked regarding 2048 Summer Olympics, which didn't happen yet, and the retrieved content has only returned results for 2020.</p> In\u00a0[10]: Copied! <pre>def check_context(title, heading, question, max_len=1800, search_model='ada', max_rerank=10):\n\"\"\"\n    Evaluate the performance of the search model in retrieving the correct context\n\n    Parameters\n    ----------\n    title: str\n        The title of the Wikipedia page\n    heading: str\n        The heading of the Wikipedia section\n    qusetion: str\n        The question\n    max_len: int\n        The maximum length of the context\n    search_model: str\n        The search model to use - `ada` is most cost effective\n    max_rerank: int\n        The maximum number of reranking documents to use the search model on\n\n    Returns\n    -------\n    rank: int\n        The rank of the correct context\n    token_length: int\n        The number of tokens needed to obtain the correct context\n    \"\"\"\n    \n    try:\n        results = openai.Engine(search_model).search(\n            search_model=search_model, \n            query=question, \n            max_rerank=max_rerank,\n            file=olympics_search_fileid,\n            return_metadata=True\n        )\n        index=-1\n        returns = []\n        cur_len = 0\n        for result in results['data']:\n            cur_len += int(result['metadata']) + 4 # we add 4 tokens for the separator `\\n\\n###\\n\\n`\n            if cur_len &gt; max_len:\n                break\n            returns.append(result['text'])\n            res = result['text'].split('\\n')\n            if res[0] == title and res[1] == heading:\n                index = len(returns) - 1\n                break\n        return index, cur_len\n    except Exception as e:\n        #print (e)\n        return []\nprint(check_context(\"Athletics at the 2020 Summer Olympics \u2013 Women's 4 \u00d7 100 metres relay\", \"Summary\", \"Where did women's 4 x 100 metres relay event take place during the 2020 Summer Olympics?\", max_len=10000))\n</pre> def check_context(title, heading, question, max_len=1800, search_model='ada', max_rerank=10):     \"\"\"     Evaluate the performance of the search model in retrieving the correct context      Parameters     ----------     title: str         The title of the Wikipedia page     heading: str         The heading of the Wikipedia section     qusetion: str         The question     max_len: int         The maximum length of the context     search_model: str         The search model to use - `ada` is most cost effective     max_rerank: int         The maximum number of reranking documents to use the search model on      Returns     -------     rank: int         The rank of the correct context     token_length: int         The number of tokens needed to obtain the correct context     \"\"\"          try:         results = openai.Engine(search_model).search(             search_model=search_model,              query=question,              max_rerank=max_rerank,             file=olympics_search_fileid,             return_metadata=True         )         index=-1         returns = []         cur_len = 0         for result in results['data']:             cur_len += int(result['metadata']) + 4 # we add 4 tokens for the separator `\\n\\n###\\n\\n`             if cur_len &gt; max_len:                 break             returns.append(result['text'])             res = result['text'].split('\\n')             if res[0] == title and res[1] == heading:                 index = len(returns) - 1                 break         return index, cur_len     except Exception as e:         #print (e)         return [] print(check_context(\"Athletics at the 2020 Summer Olympics \u2013 Women's 4 \u00d7 100 metres relay\", \"Summary\", \"Where did women's 4 x 100 metres relay event take place during the 2020 Summer Olympics?\", max_len=10000)) <pre>(0, 58)\n</pre> <p>We utilize the generated questions based on context to estimate how often we can retrieve the original context. These questions are noisy, so this is not a perfect estimate.</p> <p>Our questions and answers are prefixed with numbered bullet points, however due to the way they were generated, they are missing the first number, hence we add \"1.\" to the list of questions (and answers).</p> <p>We calculate the rank of the section retrieved using ada search, and the number of tokens in the context needed to retrieve the relevant section in full.</p> In\u00a0[11]: Copied! <pre>ada_results = df.apply(lambda x: [\n                    check_context( x.title, \n                                   x.heading, \n                                   q[3:],     # remove the number prefix\n                                   max_len=1000000, # set a large number to get the full context \n                                   search_model='ada', \n                                   max_rerank=200,\n                                 ) \n                    for q in (x.questions).split('\\n') # split the questions\n                    if len(q) &gt;10 # remove the empty questions\n                ], axis=1)\nada_results.head()\n</pre> ada_results = df.apply(lambda x: [                     check_context( x.title,                                     x.heading,                                     q[3:],     # remove the number prefix                                    max_len=1000000, # set a large number to get the full context                                     search_model='ada',                                     max_rerank=200,                                  )                      for q in (x.questions).split('\\n') # split the questions                     if len(q) &gt;10 # remove the empty questions                 ], axis=1) ada_results.head() Out[11]: <pre>0    [(132, 27104), (-1, 22939), (8, 2151), (2, 121...\n1    [(4, 1737), (0, 130), (8, 744), (96, 17208), (...\n2          [(0, 373), (0, 373), (-1, 40610), (1, 570)]\n3            [(0, 302), (0, 302), (5, 968), (8, 1425)]\n4                      [(0, 167), (0, 167), (2, 1442)]\nName: ada, dtype: object</pre> In\u00a0[13]: Copied! <pre>out = pd.concat([ada_results], axis=1)\nout.columns = ['ada']\nout.to_csv('olympics-data/search_engine_results.csv')\n</pre> out = pd.concat([ada_results], axis=1) out.columns = ['ada'] out.to_csv('olympics-data/search_engine_results.csv') In\u00a0[14]: Copied! <pre>def expand_lists(out):\n\"\"\"\n    Expand a pandas series containing lists into a series, where each list element becomes a value on its own\n\n    Input is a row per paragraph, which has multiple questions\n    Output is a row per question\n    \"\"\"\n    cols = [pd.DataFrame(out[name].tolist()).stack().reset_index(level=1, drop=True).rename(name) for name in out.columns] \n    return pd.concat(cols, axis=1)\n\nout_expanded = expand_lists(out)\nout_expanded['rank'] = out_expanded.ada.apply(lambda x: x[0] if x != [] else -2)\nout_expanded['tokens'] = out_expanded.ada.apply(lambda x: x[1] if x != [] else -2)\n</pre> def expand_lists(out):     \"\"\"     Expand a pandas series containing lists into a series, where each list element becomes a value on its own      Input is a row per paragraph, which has multiple questions     Output is a row per question     \"\"\"     cols = [pd.DataFrame(out[name].tolist()).stack().reset_index(level=1, drop=True).rename(name) for name in out.columns]      return pd.concat(cols, axis=1)  out_expanded = expand_lists(out) out_expanded['rank'] = out_expanded.ada.apply(lambda x: x[0] if x != [] else -2) out_expanded['tokens'] = out_expanded.ada.apply(lambda x: x[1] if x != [] else -2)  In\u00a0[15]: Copied! <pre>within_2k = (out_expanded.tokens &lt; 2000).mean()\nprint(f\"{within_2k*100:.1f}% of relevant paragraphs are retrieved within the first 2k tokens\")\n</pre> within_2k = (out_expanded.tokens &lt; 2000).mean() print(f\"{within_2k*100:.1f}% of relevant paragraphs are retrieved within the first 2k tokens\") <pre>74.3% of relevant paragraphs are retrieved within the first 2k tokens\n</pre> <p>The relevant context can be obtained 74% of the time on this dataset</p> In\u00a0[16]: Copied! <pre>outside_200 = (out_expanded['rank'] == -1).mean()\nprint(f\"{outside_200*100:.1f}% of relevant paragraphs are not retrieved within the first 200 results\")\n</pre> outside_200 = (out_expanded['rank'] == -1).mean() print(f\"{outside_200*100:.1f}% of relevant paragraphs are not retrieved within the first 200 results\") <pre>7.4% of relevant paragraphs are not retrieved within the first 200 results\n</pre> <p>7.4% of the time, this is due to the keyword search part of the search algorithm not retrieving the relevant context within the first 200 results. 18.3% of the time this is due to the semantic search not placing the relevant context within the first 2000 tokens.</p> In\u00a0[17]: Copied! <pre>import matplotlib.pyplot as plt\n\n# plot a histogram, and add axis descriptions and title\nout_expanded[(out_expanded['rank'] &gt;=0)&amp;(out_expanded['rank'] &lt;30)]['rank'].hist(bins=29)\nplt.xlabel('rank')\nplt.ylabel('count')\nplt.title('Histogram of ranks of retrieved paragraphs')\nplt.show()\n</pre> import matplotlib.pyplot as plt  # plot a histogram, and add axis descriptions and title out_expanded[(out_expanded['rank'] &gt;=0)&amp;(out_expanded['rank'] &lt;30)]['rank'].hist(bins=29) plt.xlabel('rank') plt.ylabel('count') plt.title('Histogram of ranks of retrieved paragraphs') plt.show() In\u00a0[18]: Copied! <pre>out_expanded[(out_expanded.tokens&gt;=0)&amp;(out_expanded.tokens &lt; 2000)]['tokens'].hist(bins=29)\nplt.xlabel('tokens')\nplt.ylabel('count')\nplt.title('Histogram of the number of minimum tokens needed')\nplt.show()\n</pre> out_expanded[(out_expanded.tokens&gt;=0)&amp;(out_expanded.tokens &lt; 2000)]['tokens'].hist(bins=29) plt.xlabel('tokens') plt.ylabel('count') plt.title('Histogram of the number of minimum tokens needed') plt.show() <p>We can observe that the context is most likely to be returned as one of the first results, and most likely to be returned within the first 200-500 tokens.</p> In\u00a0[19]: Copied! <pre># normalized value_counts\nout_expanded['rank'].value_counts(normalize=True).sort_index()[:13]\n</pre> # normalized value_counts out_expanded['rank'].value_counts(normalize=True).sort_index()[:13] Out[19]: <pre>-2     0.000063\n-1     0.074428\n 0     0.453420\n 1     0.089515\n 2     0.047146\n 3     0.032437\n 4     0.024139\n 5     0.019676\n 6     0.015967\n 7     0.013452\n 8     0.011189\n 9     0.009869\n 10    0.009178\nName: rank, dtype: float64</pre> <p>probabilities of the relevant context being returned at each rank. (-2 means a processing error, -1 means the rank is &gt;200)</p>"},{"location":"examples/fine-tuned_qa/olympics-2-create-qa/#2.-Creating-a-synthetic-Q&A-dataset","title":"2. Creating a synthetic Q&amp;A dataset\u00b6","text":"<p>We use <code>davinci-instruct-beta-v3</code>, a model specialized in following instructions, to create questions based on the given context. Then we also use <code>davinci-instruct-beta-v3</code> to answer those questions, given the same context.</p> <p>This is expensive, and will also take a long time, as we call the davinci engine for each section. You can simply download the final dataset instead.</p> <p>We're using the dataset created using the previous notebook</p>"},{"location":"examples/fine-tuned_qa/olympics-2-create-qa/#2.1-Read-in-the-data,-and-create-a-context","title":"2.1 Read in the data, and create a context\u00b6","text":"<p>Create a context by concatenating the title, the heading and the content of that section</p>"},{"location":"examples/fine-tuned_qa/olympics-2-create-qa/#2.2-Create-questions-based-on-the-context","title":"2.2 Create questions based on the context\u00b6","text":"<p>Use davinci-instruct to generate a number of plausible questions relating to the Wikipedia section contents.</p> <p>Note: We have used temperature=0, but it may be beneficial to experiment with a higher temperature to get a higher diversity of questions.</p> <p>WARNING: This step will last a long time, and consume a lot of tokens, as it calls davinci-instruct for every section to generate a number of questions.</p>"},{"location":"examples/fine-tuned_qa/olympics-2-create-qa/#2.3-Create-answers-based-on-the-context","title":"2.3 Create answers based on the context\u00b6","text":"<p>Use davinci-instruct to answer the questions given the relevant Wikipedia section contents</p> <p>Note: We have used temperature=0, but it may be beneficial to experiment with a higher temperature to get a higher diversity of questions.</p> <p>WARNING: This step will last a long time, and consume a lot of tokens, as it calls davinci-instruct for every section to answer all the questions.</p>"},{"location":"examples/fine-tuned_qa/olympics-2-create-qa/#2.4-Save-the-Olympics-Q&A-dataset-based-on-Wikipedia-sections","title":"2.4 Save the Olympics Q&amp;A dataset based on Wikipedia sections\u00b6","text":"<p>We save the file for use in the next notebook</p>"},{"location":"examples/fine-tuned_qa/olympics-2-create-qa/#2.5-Search-file-(DEPRECATED)","title":"2.5 Search file (DEPRECATED)\u00b6","text":"<p>We create a search file (API reference), which can be used to retrieve the relevant context when a question is asked.</p> <p>DEPRECATED: The /search endpoint is deprecated in favour of using embeddings. Embeddings are cheaper, faster and can support a better search experience. See Question Answering Guide for a search implementation using the embeddings</p>"},{"location":"examples/fine-tuned_qa/olympics-2-create-qa/#2.6-Answer-questions-based-on-the-context-provided","title":"2.6 Answer questions based on the context provided\u00b6","text":"<p>We will use a simple implementation of the answers endpoint. This works by simply using the /search endpoint, which searches over an indexed file to obtain the relevant sections which can be included in the context, following by a question and answering prompt given a specified model.</p>"},{"location":"examples/fine-tuned_qa/olympics-2-create-qa/#2.7-(Optional)-Investigation-into-how-likely-the-search-endpoint-is-to-return-the-relevant-context","title":"2.7 (Optional) Investigation into how likely the search endpoint is to return the relevant context\u00b6","text":""},{"location":"examples/fine-tuned_qa/olympics-3-train-qa/","title":"3. Train a fine-tuning model specialized for Q&amp;A","text":"<p>Note: To answer questions based on text documents, we recommend the procedure in Question Answering using Embeddings. Some of the code below may rely on deprecated API endpoints.</p> In\u00a0[1]: Copied! <pre>import openai\nimport pandas as pd\ndf = pd.read_csv('olympics-data/olympics_qa.csv')\nolympics_search_fileid = \"file-c3shd8wqF3vSCKaukW4Jr1TT\"\ndf.head()\n</pre> import openai import pandas as pd df = pd.read_csv('olympics-data/olympics_qa.csv') olympics_search_fileid = \"file-c3shd8wqF3vSCKaukW4Jr1TT\" df.head() Out[1]: title heading content tokens context questions answers 0 2020 Summer Olympics Summary The 2020 Summer Olympics (Japanese: 2020\u5e74\u590f\u5b63\u30aa\u30ea\u30f3... 713 2020 Summer Olympics\\nSummary\\n\\nThe 2020 Summ... 1. What is the 2020 Summer Olympics?\\n2. When ... 1. The 2020 Summer Olympics is an internationa... 1 2020 Summer Olympics Host city selection The International Olympic Committee (IOC) vote... 126 2020 Summer Olympics\\nHost city selection\\n\\nT... 1. \\n2. \\n3. \\n4. 1. What is the International Olympic Committee... 2 2020 Summer Olympics Impact of the COVID-19 pandemic In January 2020, concerns were raised about th... 369 2020 Summer Olympics\\nImpact of the COVID-19 p... 1. What was the COVID-19 pandemic?\\n2. How did... 1. The COVID-19 pandemic was a pandemic that o... 3 2020 Summer Olympics Qualifying event cancellation and postponement Concerns about the pandemic began to affect qu... 298 2020 Summer Olympics\\nQualifying event cancell... 1. What was the original location of the Asia ... 1. The original location of the Asia &amp; Oceania... 4 2020 Summer Olympics Effect on doping tests Mandatory doping tests were being severely res... 163 2020 Summer Olympics\\nEffect on doping tests\\n... 1. What was the COVID-19 pandemic?\\n2. What di... 1. The COVID-19 pandemic was a pandemic that o... <p>Split the sections into a training and testing set</p> In\u00a0[2]: Copied! <pre>from sklearn.model_selection import train_test_split\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\nlen(train_df), len(test_df)\n</pre> from sklearn.model_selection import train_test_split train_df, test_df = train_test_split(df, test_size=0.2, random_state=42) len(train_df), len(test_df) Out[2]: <pre>(3014, 754)</pre> <p>we check that the separator we intend to use isn't present within the contexts</p> In\u00a0[3]: Copied! <pre>df.context.str.contains('-&gt;').sum()\n</pre> df.context.str.contains('-&gt;').sum() Out[3]: <pre>0</pre> In\u00a0[4]: Copied! <pre>import random\n\ndef get_random_similar_contexts(question, context, file_id=olympics_search_fileid, search_model='ada', max_rerank=10):\n\"\"\"\n    Find similar contexts to the given context using the search file\n    \"\"\"\n    try:\n        results = openai.Engine(search_model).search(\n            search_model=search_model, \n            query=question, \n            max_rerank=max_rerank,\n            file=file_id\n        )\n        candidates = []\n        for result in results['data'][:3]:\n            if result['text'] == context:\n                continue\n            candidates.append(result['text'])\n        random_candidate = random.choice(candidates)\n        return random_candidate\n    except Exception as e:\n        print(e)\n        return \"\"\n\ndef create_fine_tuning_dataset(df, discriminator=False, n_negative=1, add_related=False):\n\"\"\"\n    Create a dataset for fine tuning the OpenAI model; either for a discriminator model, \n    or a model specializing in Q&amp;A, where it says if no relevant context is found.\n\n    Parameters\n    ----------\n    df: pd.DataFrame\n        The dataframe containing the question, answer and context pairs\n    discriminator: bool\n        Whether to create a dataset for the discriminator\n    n_negative: int\n        The number of random negative samples to add (using a random context)\n    add_related: bool\n        Whether to add the related contexts to the correct context. These are hard negative examples\n\n    Returns\n    -------\n    pd.DataFrame\n        The dataframe containing the prompts and completions, ready for fine-tuning\n    \"\"\"\n    rows = []\n    for i, row in df.iterrows():\n        for q, a in zip((\"1.\" + row.questions).split('\\n'), (\"1.\" + row.answers).split('\\n')):\n            if len(q) &gt;10 and len(a) &gt;10:\n                if discriminator:\n                    rows.append({\"prompt\":f\"{row.context}\\nQuestion: {q[2:].strip()}\\n Related:\", \"completion\":f\" yes\"})\n                else:\n                    rows.append({\"prompt\":f\"{row.context}\\nQuestion: {q[2:].strip()}\\nAnswer:\", \"completion\":f\" {a[2:].strip()}\"})\n\n    for i, row in df.iterrows():\n        for q in (\"1.\" + row.questions).split('\\n'):\n            if len(q) &gt;10:\n                for j in range(n_negative + (2 if add_related else 0)):\n                    random_context = \"\"\n                    if j == 0 and add_related:\n                        # add the related contexts based on originating from the same wikipedia page\n                        subset = df[(df.title == row.title) &amp; (df.context != row.context)]\n                        \n                        if len(subset) &lt; 1:\n                            continue\n                        random_context = subset.sample(1).iloc[0].context\n                    if j == 1 and add_related:\n                        # add the related contexts based on the most similar contexts according to the search\n                        random_context = get_random_similar_contexts(q[2:].strip(), row.context, search_model='ada', max_rerank=10)\n                    else:\n                        while True:\n                            # add random context, which isn't the correct context\n                            random_context = df.sample(1).iloc[0].context\n                            if random_context != row.context:\n                                break\n                    if discriminator:\n                        rows.append({\"prompt\":f\"{random_context}\\nQuestion: {q[2:].strip()}\\n Related:\", \"completion\":f\" no\"})\n                    else:\n                        rows.append({\"prompt\":f\"{random_context}\\nQuestion: {q[2:].strip()}\\nAnswer:\", \"completion\":f\" No appropriate context found to answer the question.\"})\n\n    return pd.DataFrame(rows)\n</pre> import random  def get_random_similar_contexts(question, context, file_id=olympics_search_fileid, search_model='ada', max_rerank=10):     \"\"\"     Find similar contexts to the given context using the search file     \"\"\"     try:         results = openai.Engine(search_model).search(             search_model=search_model,              query=question,              max_rerank=max_rerank,             file=file_id         )         candidates = []         for result in results['data'][:3]:             if result['text'] == context:                 continue             candidates.append(result['text'])         random_candidate = random.choice(candidates)         return random_candidate     except Exception as e:         print(e)         return \"\"  def create_fine_tuning_dataset(df, discriminator=False, n_negative=1, add_related=False):     \"\"\"     Create a dataset for fine tuning the OpenAI model; either for a discriminator model,      or a model specializing in Q&amp;A, where it says if no relevant context is found.      Parameters     ----------     df: pd.DataFrame         The dataframe containing the question, answer and context pairs     discriminator: bool         Whether to create a dataset for the discriminator     n_negative: int         The number of random negative samples to add (using a random context)     add_related: bool         Whether to add the related contexts to the correct context. These are hard negative examples      Returns     -------     pd.DataFrame         The dataframe containing the prompts and completions, ready for fine-tuning     \"\"\"     rows = []     for i, row in df.iterrows():         for q, a in zip((\"1.\" + row.questions).split('\\n'), (\"1.\" + row.answers).split('\\n')):             if len(q) &gt;10 and len(a) &gt;10:                 if discriminator:                     rows.append({\"prompt\":f\"{row.context}\\nQuestion: {q[2:].strip()}\\n Related:\", \"completion\":f\" yes\"})                 else:                     rows.append({\"prompt\":f\"{row.context}\\nQuestion: {q[2:].strip()}\\nAnswer:\", \"completion\":f\" {a[2:].strip()}\"})      for i, row in df.iterrows():         for q in (\"1.\" + row.questions).split('\\n'):             if len(q) &gt;10:                 for j in range(n_negative + (2 if add_related else 0)):                     random_context = \"\"                     if j == 0 and add_related:                         # add the related contexts based on originating from the same wikipedia page                         subset = df[(df.title == row.title) &amp; (df.context != row.context)]                                                  if len(subset) &lt; 1:                             continue                         random_context = subset.sample(1).iloc[0].context                     if j == 1 and add_related:                         # add the related contexts based on the most similar contexts according to the search                         random_context = get_random_similar_contexts(q[2:].strip(), row.context, search_model='ada', max_rerank=10)                     else:                         while True:                             # add random context, which isn't the correct context                             random_context = df.sample(1).iloc[0].context                             if random_context != row.context:                                 break                     if discriminator:                         rows.append({\"prompt\":f\"{random_context}\\nQuestion: {q[2:].strip()}\\n Related:\", \"completion\":f\" no\"})                     else:                         rows.append({\"prompt\":f\"{random_context}\\nQuestion: {q[2:].strip()}\\nAnswer:\", \"completion\":f\" No appropriate context found to answer the question.\"})      return pd.DataFrame(rows)  <p>We apply the same process of dataset creation for both the discriminator, and the Q&amp;A answering model. We apply the process separately for the training and testing set, to ensure that the examples from the training set don't feature within the test set.</p> In\u00a0[5]: Copied! <pre>for name, is_disc in [('discriminator', True), ('qa', False)]:\n    for train_test, dt in [('train', train_df), ('test', test_df)]:\n        ft = create_fine_tuning_dataset(dt, discriminator=is_disc, n_negative=1, add_related=True)\n        ft.to_json(f'{name}_{train_test}.jsonl', orient='records', lines=True)\n</pre> for name, is_disc in [('discriminator', True), ('qa', False)]:     for train_test, dt in [('train', train_df), ('test', test_df)]:         ft = create_fine_tuning_dataset(dt, discriminator=is_disc, n_negative=1, add_related=True)         ft.to_json(f'{name}_{train_test}.jsonl', orient='records', lines=True) Out[5]: <pre></pre> <p>We formatted the data according to the recommendations from the fine-tuning tool, which is available using</p> <p>openai tools fine_tunes.prepare_data -f qa_train.jsonl</p> <p>We highly recommend that you use this tool, which suggests improvements in your data formatting for fine-tuning.</p> In\u00a0[6]: Copied! <pre>!openai api fine_tunes.create -t \"olympics-data/discriminator_train.jsonl\" -v \"olympics-data/discriminator_test.jsonl\" --batch_size 16  --compute_classification_metrics --classification_positive_class \" yes\" --model ada\n</pre> !openai api fine_tunes.create -t \"olympics-data/discriminator_train.jsonl\" -v \"olympics-data/discriminator_test.jsonl\" --batch_size 16  --compute_classification_metrics --classification_positive_class \" yes\" --model ada Out[6]: <pre></pre> In\u00a0[7]: Copied! <pre>!openai api fine_tunes.create -t \"olympics-data/qa_train.jsonl\" -v \"olympics-data/qa_test.jsonl\" --batch_size 16\n</pre> !openai api fine_tunes.create -t \"olympics-data/qa_train.jsonl\" -v \"olympics-data/qa_test.jsonl\" --batch_size 16 Out[7]: <pre></pre> In\u00a0[8]: Copied! <pre>ft_discriminator = \"curie:ft-openai-internal-2021-08-23-23-58-57\"\nft_qa = \"curie:ft-openai-internal-2021-08-23-17-54-10\"\n\ndef apply_ft_discriminator(context, question, discriminator_model):\n\"\"\"\n    Apply the fine tuned discriminator to a question, to assess whether it can be answered from the context.\n    \"\"\"\n    prompt = f\"{context}\\nQuestion: {question}\\n Related:\"\n    result = openai.Completion.create(model=discriminator_model, prompt=prompt, max_tokens=1, temperature=0, top_p=1, n=1, logprobs=2)\n    return result['choices'][0]['logprobs']['top_logprobs']\n\napply_ft_discriminator('The first human-made object in space was the Soviet Union satellite Sputnik 1 on 4 October 1957.', \n                        'What was the first human-made object in space?', ft_discriminator)\n</pre> ft_discriminator = \"curie:ft-openai-internal-2021-08-23-23-58-57\" ft_qa = \"curie:ft-openai-internal-2021-08-23-17-54-10\"  def apply_ft_discriminator(context, question, discriminator_model):     \"\"\"     Apply the fine tuned discriminator to a question, to assess whether it can be answered from the context.     \"\"\"     prompt = f\"{context}\\nQuestion: {question}\\n Related:\"     result = openai.Completion.create(model=discriminator_model, prompt=prompt, max_tokens=1, temperature=0, top_p=1, n=1, logprobs=2)     return result['choices'][0]['logprobs']['top_logprobs']  apply_ft_discriminator('The first human-made object in space was the Soviet Union satellite Sputnik 1 on 4 October 1957.',                          'What was the first human-made object in space?', ft_discriminator) Out[8]: <pre>[&lt;OpenAIObject at 0x7fe812e602b0&gt; JSON: {\n   \" no\": -10.819577,\n   \" yes\": -2.045765e-05\n }]</pre> <p>We can see that the model can generalize well to different contexts and questions.</p> In\u00a0[9]: Copied! <pre>def apply_ft_qa_answer(context, question, answering_model):\n\"\"\"\n    Apply the fine tuned discriminator to a question\n    \"\"\"\n    prompt = f\"{context}\\nQuestion: {question}\\nAnswer:\"\n    result = openai.Completion.create(model=answering_model, prompt=prompt, max_tokens=30, temperature=0, top_p=1, n=1, stop=['.','\\n'])\n    return result['choices'][0]['text']\n\napply_ft_qa_answer('The first human-made object in space was the Soviet Union satellite Sputnik 1 on 4 October 1957.', \n                    'What was the first human-made object in space?', ft_qa)\n</pre> def apply_ft_qa_answer(context, question, answering_model):     \"\"\"     Apply the fine tuned discriminator to a question     \"\"\"     prompt = f\"{context}\\nQuestion: {question}\\nAnswer:\"     result = openai.Completion.create(model=answering_model, prompt=prompt, max_tokens=30, temperature=0, top_p=1, n=1, stop=['.','\\n'])     return result['choices'][0]['text']  apply_ft_qa_answer('The first human-made object in space was the Soviet Union satellite Sputnik 1 on 4 October 1957.',                      'What was the first human-made object in space?', ft_qa)  Out[9]: <pre>' The first human-made object in space was the Soviet Union satellite Sputnik 1 on 4 October 1957'</pre> <p>We can see that the model can answer the question, when the context is appropriate.</p> In\u00a0[10]: Copied! <pre>apply_ft_qa_answer('The first human-made object in space was the Soviet Union satellite Sputnik 1 on 4 October 1957.',\n                    'What is impressive about the Soviet Union?', ft_qa)\n</pre> apply_ft_qa_answer('The first human-made object in space was the Soviet Union satellite Sputnik 1 on 4 October 1957.',                     'What is impressive about the Soviet Union?', ft_qa) Out[10]: <pre>' The Soviet Union was the first country to successfully launch a satellite into space'</pre> In\u00a0[11]: Copied! <pre>apply_ft_qa_answer('The first human-made object in space was the Soviet Union satellite Sputnik 1 on 4 October 1957.',\n                    'How many cars were produced in the Soviet Union in 1970?', ft_qa)\n</pre> apply_ft_qa_answer('The first human-made object in space was the Soviet Union satellite Sputnik 1 on 4 October 1957.',                     'How many cars were produced in the Soviet Union in 1970?', ft_qa) Out[11]: <pre>' No appropriate context found to answer the question'</pre> <p>We can see that the model knows when to answer the question, and when to say that insufficient context is present to answer the question.</p> <p>We can also combine a discriminator and a base model, or a fine-tuned Q&amp;A model. Discriminator can essentially serve as a decision whether the question can be answered given the context or not.</p> In\u00a0[12]: Copied! <pre>def answer_question_conditionally(answering_model, discriminator_model, context, question, discriminator_logprob_yes_modifier=0):\n    logprobs = apply_ft_discriminator(context, question, discriminator_model)\n    yes_logprob = logprobs[' yes'] if ' yes' in logprobs else -100\n    no_logprob = logprobs[' no'] if ' no' in logprobs else -100\n    if yes_logprob + discriminator_logprob_yes_modifier &lt; no_logprob:\n        return \" No appropriate context found to answer the question based on the discriminator.\"\n    return apply_ft_qa_answer(context, question, answering_model)\nanswer_question_conditionally(ft_qa, ft_discriminator, \n                                \"Crowdless games are a rare although not unheard-of occurrence in sports. \\\n                                 When they do occur, it is usually the result of events beyond the control \\\n                                 of the teams or fans, such as weather-related concerns, public health concerns, \\\n                                 or wider civil disturbances unrelated to the game. For instance, \\\n                                 the COVID-19 pandemic caused many sports leagues around the world \\\n                                 to be played behind closed doors.\",\n                                \"Could weather cause a sport event to have no crowd?\")\n</pre> def answer_question_conditionally(answering_model, discriminator_model, context, question, discriminator_logprob_yes_modifier=0):     logprobs = apply_ft_discriminator(context, question, discriminator_model)     yes_logprob = logprobs[' yes'] if ' yes' in logprobs else -100     no_logprob = logprobs[' no'] if ' no' in logprobs else -100     if yes_logprob + discriminator_logprob_yes_modifier &lt; no_logprob:         return \" No appropriate context found to answer the question based on the discriminator.\"     return apply_ft_qa_answer(context, question, answering_model) answer_question_conditionally(ft_qa, ft_discriminator,                                  \"Crowdless games are a rare although not unheard-of occurrence in sports. \\                                  When they do occur, it is usually the result of events beyond the control \\                                  of the teams or fans, such as weather-related concerns, public health concerns, \\                                  or wider civil disturbances unrelated to the game. For instance, \\                                  the COVID-19 pandemic caused many sports leagues around the world \\                                  to be played behind closed doors.\",                                 \"Could weather cause a sport event to have no crowd?\") Out[12]: <pre>' Weather could cause a sport event to have no crowd'</pre> <p>The above function illustrates how to potentially combine a discriminator and a fine-tuned Q&amp;A model. This gives a more fine-grained control over how certain we want the model to be before it answers the question.</p> <p>We'll now take a look on how answers endpoint works - combining search to retrieve the relevant context from a knowledge base, and then using the fine-tuned Q&amp;A model to answer the question.</p> In\u00a0[13]: Copied! <pre>from answers_with_ft import answer_question\nanswer_question(olympics_search_fileid, ft_qa, \"Which country won the Women's football tournament at the 2020 Olympic games?\")\n</pre> from answers_with_ft import answer_question answer_question(olympics_search_fileid, ft_qa, \"Which country won the Women's football tournament at the 2020 Olympic games?\") Out[13]: <pre>\" Canada won the Women's football tournament at the 2020 Olympic games\"</pre>"},{"location":"examples/fine-tuned_qa/olympics-3-train-qa/#3.-Train-a-fine-tuning-model-specialized-for-Q&A","title":"3. Train a fine-tuning model specialized for Q&amp;A\u00b6","text":"<p>This notebook will utilize the dataset of context, question and answer pairs to additionally create adversarial questions and context pairs, where the question was not generated on that context. In those cases the model will be prompted to answer \"No sufficient context for answering the question\". We will also train a discriminator model, which predicts whether the question can be answered based on the context or not.</p> <p>We will add hard adversarial examples as well, which will be based either on semantically similar sections, or neighbouring sections, originating from the same article.</p>"},{"location":"examples/fine-tuned_qa/olympics-3-train-qa/#3.1-Create-the-fine-tuning-datasets-for-Q&A-and-discriminator-models","title":"3.1 Create the fine-tuning datasets for Q&amp;A and discriminator models\u00b6","text":"<p>The fine-tuning dataset is created in the following way. For every corresponding question, answer and context pair we create:</p> <ul> <li>Positive example: correct question, answer, context pair</li> <li>Negative examples:<ul> <li>random negative example, where the random context is paired with the question</li> <li>two hard negative examples<ul> <li>one originating from the same wikipedia article</li> <li>another, which is most similar to the correct context</li> </ul> </li> </ul> </li> </ul> <p>This process is noisy, as sometimes the question might be answerable given a different context, but on average we hope this won't affect the peformance too much.</p> <p>We apply the same process of dataset creation for both the discriminator, and the Q&amp;A answering model. We apply the process separately for the training and testing set, to ensure that the examples from the training set don't feature within the test set.</p>"},{"location":"examples/fine-tuned_qa/olympics-3-train-qa/#3.2-Submit-the-datasets-for-fine-tuning","title":"3.2 Submit the datasets for fine-tuning\u00b6","text":""},{"location":"examples/fine-tuned_qa/olympics-3-train-qa/#3.3-Using-the-fine-tuned-models","title":"3.3 Using the fine-tuned models\u00b6","text":"<p>We will now use the fine-tuned discriminator and the fine-tuned Q&amp;A model. By requesting logprobs, we can see how certain the discriminator is in a <code>yes</code> vs <code>no</code> answer.</p>"},{"location":"examples/fine-tuned_qa/olympics-3-train-qa/#3.4-Answering-the-question-based-on-a-knowledge-base","title":"3.4 Answering the question based on a knowledge base\u00b6","text":"<p>Finally we can use a logic similar to the /answers endpoint, where we first search for the relevant context, and then ask a Q&amp;A model to answer the question given that context. If you'd like to see the implementation details, check out the <code>answers_with_ft.py</code> file.</p>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/","title":"Using Vector Databases for Embeddings Search","text":"In\u00a0[\u00a0]: Copied! <pre># We'll need to install the clients for all vector databases\n!pip install pinecone-client\n!pip install weaviate-client\n!pip install pymilvus\n!pip install qdrant-client\n!pip install redis\n\n#Install wget to pull zip file\n!pip install wget\n</pre> # We'll need to install the clients for all vector databases !pip install pinecone-client !pip install weaviate-client !pip install pymilvus !pip install qdrant-client !pip install redis  #Install wget to pull zip file !pip install wget In\u00a0[2]: Copied! <pre>import openai\n\nimport tiktoken\nfrom typing import List, Iterator\nimport pandas as pd\nimport numpy as np\nimport os\nimport wget\nfrom ast import literal_eval\n\n# Redis client library for Python\nimport redis\n\n# Pinecone's client library for Python\nimport pinecone\n\n# Weaviate's client library for Python\nimport weaviate\n\n# Milvus's client library for Python\nimport pymilvus\n\n# Qdrant's client library for Python\nimport qdrant_client\n\n# I've set this to our new embeddings model, this can be changed to the embedding model of your choice\nEMBEDDING_MODEL = \"text-embedding-ada-002\"\n\n# Ignore unclosed SSL socket warnings - optional in case you get these errors\nimport warnings\n\nwarnings.filterwarnings(action=\"ignore\", message=\"unclosed\", category=ResourceWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n</pre> import openai  import tiktoken from typing import List, Iterator import pandas as pd import numpy as np import os import wget from ast import literal_eval  # Redis client library for Python import redis  # Pinecone's client library for Python import pinecone  # Weaviate's client library for Python import weaviate  # Milvus's client library for Python import pymilvus  # Qdrant's client library for Python import qdrant_client  # I've set this to our new embeddings model, this can be changed to the embedding model of your choice EMBEDDING_MODEL = \"text-embedding-ada-002\"  # Ignore unclosed SSL socket warnings - optional in case you get these errors import warnings  warnings.filterwarnings(action=\"ignore\", message=\"unclosed\", category=ResourceWarning) warnings.filterwarnings(\"ignore\", category=DeprecationWarning)  In\u00a0[3]: Copied! <pre>embeddings_url = 'https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip'\n\n# The file is ~700 MB so this will take some time\nwget.download(embeddings_url)\n</pre> embeddings_url = 'https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip'  # The file is ~700 MB so this will take some time wget.download(embeddings_url) Out[3]: <pre>'vector_database_wikipedia_articles_embedded.zip'</pre> In\u00a0[4]: Copied! <pre>import zipfile\nwith zipfile.ZipFile(\"vector_database_wikipedia_articles_embedded.zip\",\"r\") as zip_ref:\n    zip_ref.extractall(\"../data\")\n</pre> import zipfile with zipfile.ZipFile(\"vector_database_wikipedia_articles_embedded.zip\",\"r\") as zip_ref:     zip_ref.extractall(\"../data\") In\u00a0[\u00a0]: Copied! <pre>article_df = pd.read_csv('../data/vector_database_wikipedia_articles_embedded.csv')\n</pre> article_df = pd.read_csv('../data/vector_database_wikipedia_articles_embedded.csv') In\u00a0[6]: Copied! <pre>article_df.head()\n</pre> article_df.head() Out[6]: id url title text title_vector content_vector vector_id 0 1 https://simple.wikipedia.org/wiki/April April April is the fourth month of the year in the J... [0.001009464613161981, -0.020700545981526375, ... [-0.011253940872848034, -0.013491976074874401,... 0 1 2 https://simple.wikipedia.org/wiki/August August August (Aug.) is the eighth month of the year ... [0.0009286514250561595, 0.000820168002974242, ... [0.0003609954728744924, 0.007262262050062418, ... 1 2 6 https://simple.wikipedia.org/wiki/Art Art Art is a creative activity that expresses imag... [0.003393713850528002, 0.0061537534929811954, ... [-0.004959689453244209, 0.015772193670272827, ... 2 3 8 https://simple.wikipedia.org/wiki/A A A or a is the first letter of the English alph... [0.0153952119871974, -0.013759135268628597, 0.... [0.024894846603274345, -0.022186409682035446, ... 3 4 9 https://simple.wikipedia.org/wiki/Air Air Air refers to the Earth's atmosphere. Air is a... [0.02224554680287838, -0.02044147066771984, -0... [0.021524671465158463, 0.018522677943110466, -... 4 In\u00a0[7]: Copied! <pre># Read vectors from strings back into a list\narticle_df['title_vector'] = article_df.title_vector.apply(literal_eval)\narticle_df['content_vector'] = article_df.content_vector.apply(literal_eval)\n\n# Set vector_id to be a string\narticle_df['vector_id'] = article_df['vector_id'].apply(str)\n</pre> # Read vectors from strings back into a list article_df['title_vector'] = article_df.title_vector.apply(literal_eval) article_df['content_vector'] = article_df.content_vector.apply(literal_eval)  # Set vector_id to be a string article_df['vector_id'] = article_df['vector_id'].apply(str) In\u00a0[11]: Copied! <pre>article_df.info(show_counts=True)\n</pre> article_df.info(show_counts=True) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 25000 entries, 0 to 24999\nData columns (total 7 columns):\n #   Column          Non-Null Count  Dtype \n---  ------          --------------  ----- \n 0   id              25000 non-null  int64 \n 1   url             25000 non-null  object\n 2   title           25000 non-null  object\n 3   text            25000 non-null  object\n 4   title_vector    25000 non-null  object\n 5   content_vector  25000 non-null  object\n 6   vector_id       25000 non-null  object\ndtypes: int64(1), object(6)\nmemory usage: 1.3+ MB\n</pre> In\u00a0[\u00a0]: Copied! <pre>api_key = os.getenv(\"PINECONE_API_KEY\")\npinecone.init(api_key=api_key)\n</pre> api_key = os.getenv(\"PINECONE_API_KEY\") pinecone.init(api_key=api_key) In\u00a0[\u00a0]: Copied! <pre># Models a simple batch generator that make chunks out of an input DataFrame\nclass BatchGenerator:\n    \n    \n    def __init__(self, batch_size: int = 10) -&gt; None:\n        self.batch_size = batch_size\n    \n    # Makes chunks out of an input DataFrame\n    def to_batches(self, df: pd.DataFrame) -&gt; Iterator[pd.DataFrame]:\n        splits = self.splits_num(df.shape[0])\n        if splits &lt;= 1:\n            yield df\n        else:\n            for chunk in np.array_split(df, splits):\n                yield chunk\n\n    # Determines how many chunks DataFrame contains\n    def splits_num(self, elements: int) -&gt; int:\n        return round(elements / self.batch_size)\n    \n    __call__ = to_batches\n\ndf_batcher = BatchGenerator(300)\n</pre> # Models a simple batch generator that make chunks out of an input DataFrame class BatchGenerator:               def __init__(self, batch_size: int = 10) -&gt; None:         self.batch_size = batch_size          # Makes chunks out of an input DataFrame     def to_batches(self, df: pd.DataFrame) -&gt; Iterator[pd.DataFrame]:         splits = self.splits_num(df.shape[0])         if splits &lt;= 1:             yield df         else:             for chunk in np.array_split(df, splits):                 yield chunk      # Determines how many chunks DataFrame contains     def splits_num(self, elements: int) -&gt; int:         return round(elements / self.batch_size)          __call__ = to_batches  df_batcher = BatchGenerator(300) In\u00a0[\u00a0]: Copied! <pre># Pick a name for the new index\nindex_name = 'wikipedia-articles'\n\n# Check whether the index with the same name already exists - if so, delete it\nif index_name in pinecone.list_indexes():\n    pinecone.delete_index(index_name)\n    \n# Creates new index\npinecone.create_index(name=index_name, dimension=len(article_df['content_vector'][0]))\nindex = pinecone.Index(index_name=index_name)\n\n# Confirm our index was created\npinecone.list_indexes()\n</pre> # Pick a name for the new index index_name = 'wikipedia-articles'  # Check whether the index with the same name already exists - if so, delete it if index_name in pinecone.list_indexes():     pinecone.delete_index(index_name)      # Creates new index pinecone.create_index(name=index_name, dimension=len(article_df['content_vector'][0])) index = pinecone.Index(index_name=index_name)  # Confirm our index was created pinecone.list_indexes() In\u00a0[\u00a0]: Copied! <pre># Upsert content vectors in content namespace - this can take a few minutes\nprint(\"Uploading vectors to content namespace..\")\nfor batch_df in df_batcher(article_df):\n    index.upsert(vectors=zip(batch_df.vector_id, batch_df.content_vector), namespace='content')\n</pre> # Upsert content vectors in content namespace - this can take a few minutes print(\"Uploading vectors to content namespace..\") for batch_df in df_batcher(article_df):     index.upsert(vectors=zip(batch_df.vector_id, batch_df.content_vector), namespace='content') In\u00a0[\u00a0]: Copied! <pre># Upsert title vectors in title namespace - this can also take a few minutes\nprint(\"Uploading vectors to title namespace..\")\nfor batch_df in df_batcher(article_df):\n    index.upsert(vectors=zip(batch_df.vector_id, batch_df.title_vector), namespace='title')\n</pre> # Upsert title vectors in title namespace - this can also take a few minutes print(\"Uploading vectors to title namespace..\") for batch_df in df_batcher(article_df):     index.upsert(vectors=zip(batch_df.vector_id, batch_df.title_vector), namespace='title') In\u00a0[\u00a0]: Copied! <pre># Check index size for each namespace to confirm all of our docs have loaded\nindex.describe_index_stats()\n</pre> # Check index size for each namespace to confirm all of our docs have loaded index.describe_index_stats() In\u00a0[\u00a0]: Copied! <pre># First we'll create dictionaries mapping vector IDs to their outputs so we can retrieve the text for our search results\ntitles_mapped = dict(zip(article_df.vector_id,article_df.title))\ncontent_mapped = dict(zip(article_df.vector_id,article_df.text))\n</pre> # First we'll create dictionaries mapping vector IDs to their outputs so we can retrieve the text for our search results titles_mapped = dict(zip(article_df.vector_id,article_df.title)) content_mapped = dict(zip(article_df.vector_id,article_df.text)) In\u00a0[\u00a0]: Copied! <pre>def query_article(query, namespace, top_k=5):\n'''Queries an article using its title in the specified\n     namespace and prints results.'''\n\n    # Create vector embeddings based on the title column\n    embedded_query = openai.Embedding.create(\n                                            input=query,\n                                            model=EMBEDDING_MODEL,\n                                            )[\"data\"][0]['embedding']\n\n    # Query namespace passed as parameter using title vector\n    query_result = index.query(embedded_query, \n                                      namespace=namespace, \n                                      top_k=top_k)\n\n    # Print query results \n    print(f'\\nMost similar results to {query} in \"{namespace}\" namespace:\\n')\n    if not query_result.matches:\n        print('no query result')\n    \n    matches = query_result.matches\n    ids = [res.id for res in matches]\n    scores = [res.score for res in matches]\n    df = pd.DataFrame({'id':ids, \n                       'score':scores,\n                       'title': [titles_mapped[_id] for _id in ids],\n                       'content': [content_mapped[_id] for _id in ids],\n                       })\n    \n    counter = 0\n    for k,v in df.iterrows():\n        counter += 1\n        print(f'{v.title} (score = {v.score})')\n    \n    print('\\n')\n\n    return df\n</pre> def query_article(query, namespace, top_k=5):     '''Queries an article using its title in the specified      namespace and prints results.'''      # Create vector embeddings based on the title column     embedded_query = openai.Embedding.create(                                             input=query,                                             model=EMBEDDING_MODEL,                                             )[\"data\"][0]['embedding']      # Query namespace passed as parameter using title vector     query_result = index.query(embedded_query,                                        namespace=namespace,                                        top_k=top_k)      # Print query results      print(f'\\nMost similar results to {query} in \"{namespace}\" namespace:\\n')     if not query_result.matches:         print('no query result')          matches = query_result.matches     ids = [res.id for res in matches]     scores = [res.score for res in matches]     df = pd.DataFrame({'id':ids,                         'score':scores,                        'title': [titles_mapped[_id] for _id in ids],                        'content': [content_mapped[_id] for _id in ids],                        })          counter = 0     for k,v in df.iterrows():         counter += 1         print(f'{v.title} (score = {v.score})')          print('\\n')      return df In\u00a0[\u00a0]: Copied! <pre>query_output = query_article('modern art in Europe','title')\n</pre> query_output = query_article('modern art in Europe','title') In\u00a0[\u00a0]: Copied! <pre>content_query_output = query_article(\"Famous battles in Scottish history\",'content')\n</pre> content_query_output = query_article(\"Famous battles in Scottish history\",'content') In\u00a0[\u00a0]: Copied! <pre># Option #1 - Self-hosted - Weaviate Open Source \nclient = weaviate.Client(\n    url=\"http://localhost:8080\",\n    additional_headers={\n        \"X-OpenAI-Api-Key\": os.getenv(\"OPENAI_API_KEY\")\n    }\n)\n</pre> # Option #1 - Self-hosted - Weaviate Open Source  client = weaviate.Client(     url=\"http://localhost:8080\",     additional_headers={         \"X-OpenAI-Api-Key\": os.getenv(\"OPENAI_API_KEY\")     } ) In\u00a0[\u00a0]: Copied! <pre># Option #2 - SaaS - (Weaviate Cloud Service)\nclient = weaviate.Client(\n    url=\"https://your-wcs-instance-name.weaviate.network\",\n    additional_headers={\n        \"X-OpenAI-Api-Key\": os.getenv(\"OPENAI_API_KEY\")\n    }\n)\n</pre> # Option #2 - SaaS - (Weaviate Cloud Service) client = weaviate.Client(     url=\"https://your-wcs-instance-name.weaviate.network\",     additional_headers={         \"X-OpenAI-Api-Key\": os.getenv(\"OPENAI_API_KEY\")     } ) In\u00a0[\u00a0]: Copied! <pre>client.is_ready()\n</pre> client.is_ready() In\u00a0[\u00a0]: Copied! <pre># Clear up the schema, so that we can recreate it\nclient.schema.delete_all()\nclient.schema.get()\n\n# Define the Schema object to use `text-embedding-ada-002` on `title` and `content`, but skip it for `url`\narticle_schema = {\n    \"class\": \"Article\",\n    \"description\": \"A collection of articles\",\n    \"vectorizer\": \"text2vec-openai\",\n    \"moduleConfig\": {\n        \"text2vec-openai\": {\n          \"model\": \"ada\",\n          \"modelVersion\": \"002\",\n          \"type\": \"text\"\n        }\n    },\n    \"properties\": [{\n        \"name\": \"title\",\n        \"description\": \"Title of the article\",\n        \"dataType\": [\"string\"]\n    },\n    {\n        \"name\": \"content\",\n        \"description\": \"Contents of the article\",\n        \"dataType\": [\"text\"],\n        \"moduleConfig\": { \"text2vec-openai\": { \"skip\": True } }\n    }]\n}\n\n# add the Article schema\nclient.schema.create_class(article_schema)\n\n# get the schema to make sure it worked\nclient.schema.get()\n</pre> # Clear up the schema, so that we can recreate it client.schema.delete_all() client.schema.get()  # Define the Schema object to use `text-embedding-ada-002` on `title` and `content`, but skip it for `url` article_schema = {     \"class\": \"Article\",     \"description\": \"A collection of articles\",     \"vectorizer\": \"text2vec-openai\",     \"moduleConfig\": {         \"text2vec-openai\": {           \"model\": \"ada\",           \"modelVersion\": \"002\",           \"type\": \"text\"         }     },     \"properties\": [{         \"name\": \"title\",         \"description\": \"Title of the article\",         \"dataType\": [\"string\"]     },     {         \"name\": \"content\",         \"description\": \"Contents of the article\",         \"dataType\": [\"text\"],         \"moduleConfig\": { \"text2vec-openai\": { \"skip\": True } }     }] }  # add the Article schema client.schema.create_class(article_schema)  # get the schema to make sure it worked client.schema.get() In\u00a0[\u00a0]: Copied! <pre>### Step 1 - configure Weaviate Batch, which optimizes CRUD operations in bulk\n# - starting batch size of 100\n# - dynamically increase/decrease based on performance\n# - add timeout retries if something goes wrong\n\nclient.batch.configure(\n    batch_size=100,\n    dynamic=True,\n    timeout_retries=3,\n)\n</pre> ### Step 1 - configure Weaviate Batch, which optimizes CRUD operations in bulk # - starting batch size of 100 # - dynamically increase/decrease based on performance # - add timeout retries if something goes wrong  client.batch.configure(     batch_size=100,     dynamic=True,     timeout_retries=3, ) In\u00a0[\u00a0]: Copied! <pre>### Step 2 - import data\n\nprint(\"Uploading data with vectors to Article schema..\")\n\ncounter=0\n\nwith client.batch as batch:\n    for k,v in article_df.iterrows():\n        \n        # print update message every 100 objects        \n        if (counter %100 == 0):\n            print(f\"Import {counter} / {len(article_df)} \")\n        \n        properties = {\n            \"title\": v[\"title\"],\n            \"content\": v[\"text\"]\n        }\n        \n        vector = v[\"title_vector\"]\n        \n        batch.add_data_object(properties, \"Article\", None, vector)\n        counter = counter+1\n\nprint(f\"Importing ({len(article_df)}) Articles complete\")\n</pre> ### Step 2 - import data  print(\"Uploading data with vectors to Article schema..\")  counter=0  with client.batch as batch:     for k,v in article_df.iterrows():                  # print update message every 100 objects                 if (counter %100 == 0):             print(f\"Import {counter} / {len(article_df)} \")                  properties = {             \"title\": v[\"title\"],             \"content\": v[\"text\"]         }                  vector = v[\"title_vector\"]                  batch.add_data_object(properties, \"Article\", None, vector)         counter = counter+1  print(f\"Importing ({len(article_df)}) Articles complete\")   In\u00a0[\u00a0]: Copied! <pre># Test that all data has loaded \u2013 get object count\nresult = (\n    client.query.aggregate(\"Article\")\n    .with_fields(\"meta { count }\")\n    .do()\n)\nprint(\"Object count: \", result[\"data\"][\"Aggregate\"][\"Article\"])\n</pre> # Test that all data has loaded \u2013 get object count result = (     client.query.aggregate(\"Article\")     .with_fields(\"meta { count }\")     .do() ) print(\"Object count: \", result[\"data\"][\"Aggregate\"][\"Article\"]) In\u00a0[\u00a0]: Copied! <pre># Test one article has worked by checking one object\ntest_article = (\n    client.query\n    .get(\"Article\", [\"title\", \"content\", \"_additional {id}\"])\n    .with_limit(1)\n    .do()\n)[\"data\"][\"Get\"][\"Article\"][0]\n\nprint(test_article[\"_additional\"][\"id\"])\nprint(test_article[\"title\"])\nprint(test_article[\"content\"])\n</pre> # Test one article has worked by checking one object test_article = (     client.query     .get(\"Article\", [\"title\", \"content\", \"_additional {id}\"])     .with_limit(1)     .do() )[\"data\"][\"Get\"][\"Article\"][0]  print(test_article[\"_additional\"][\"id\"]) print(test_article[\"title\"]) print(test_article[\"content\"]) In\u00a0[\u00a0]: Copied! <pre>def query_weaviate(query, collection_name, top_k=20):\n\n    # Creates embedding vector from user query\n    embedded_query = openai.Embedding.create(\n        input=query,\n        model=EMBEDDING_MODEL,\n    )[\"data\"][0]['embedding']\n    \n    near_vector = {\"vector\": embedded_query}\n\n    # Queries input schema with vectorised user query\n    query_result = (\n        client.query\n        .get(collection_name, [\"title\", \"content\", \"_additional {certainty distance}\"])\n        .with_near_vector(near_vector)\n        .with_limit(top_k)\n        .do()\n    )\n    \n    return query_result\n</pre> def query_weaviate(query, collection_name, top_k=20):      # Creates embedding vector from user query     embedded_query = openai.Embedding.create(         input=query,         model=EMBEDDING_MODEL,     )[\"data\"][0]['embedding']          near_vector = {\"vector\": embedded_query}      # Queries input schema with vectorised user query     query_result = (         client.query         .get(collection_name, [\"title\", \"content\", \"_additional {certainty distance}\"])         .with_near_vector(near_vector)         .with_limit(top_k)         .do()     )          return query_result In\u00a0[\u00a0]: Copied! <pre>query_result = query_weaviate(\"modern art in Europe\", \"Article\")\ncounter = 0\nfor article in query_result[\"data\"][\"Get\"][\"Article\"]:\n    counter += 1\n    print(f\"{counter}. { article['title']} (Certainty: {round(article['_additional']['certainty'],3) }) (Distance: {round(article['_additional']['distance'],3) })\")\n</pre> query_result = query_weaviate(\"modern art in Europe\", \"Article\") counter = 0 for article in query_result[\"data\"][\"Get\"][\"Article\"]:     counter += 1     print(f\"{counter}. { article['title']} (Certainty: {round(article['_additional']['certainty'],3) }) (Distance: {round(article['_additional']['distance'],3) })\") In\u00a0[\u00a0]: Copied! <pre>query_result = query_weaviate(\"Famous battles in Scottish history\", \"Article\")\ncounter = 0\nfor article in query_result[\"data\"][\"Get\"][\"Article\"]:\n    counter += 1\n    print(f\"{counter}. {article['title']} (Score: {round(article['_additional']['certainty'],3) })\")\n</pre> query_result = query_weaviate(\"Famous battles in Scottish history\", \"Article\") counter = 0 for article in query_result[\"data\"][\"Get\"][\"Article\"]:     counter += 1     print(f\"{counter}. {article['title']} (Score: {round(article['_additional']['certainty'],3) })\") In\u00a0[\u00a0]: Copied! <pre>def near_text_weaviate(query, collection_name):\n    \n    nearText = {\n        \"concepts\": [query],\n        \"distance\": 0.7,\n    }\n\n    properties = [\n        \"title\", \"content\",\n        \"_additional {certainty distance}\"\n    ]\n\n    query_result = (\n        client.query\n        .get(collection_name, properties)\n        .with_near_text(nearText)\n        .with_limit(20)\n        .do()\n    )[\"data\"][\"Get\"][collection_name]\n    \n    print (f\"Objects returned: {len(query_result)}\")\n    \n    return query_result\n</pre> def near_text_weaviate(query, collection_name):          nearText = {         \"concepts\": [query],         \"distance\": 0.7,     }      properties = [         \"title\", \"content\",         \"_additional {certainty distance}\"     ]      query_result = (         client.query         .get(collection_name, properties)         .with_near_text(nearText)         .with_limit(20)         .do()     )[\"data\"][\"Get\"][collection_name]          print (f\"Objects returned: {len(query_result)}\")          return query_result In\u00a0[\u00a0]: Copied! <pre>query_result = near_text_weaviate(\"modern art in Europe\",\"Article\")\ncounter = 0\nfor article in query_result:\n    counter += 1\n    print(f\"{counter}. { article['title']} (Certainty: {round(article['_additional']['certainty'],3) }) (Distance: {round(article['_additional']['distance'],3) })\")\n</pre> query_result = near_text_weaviate(\"modern art in Europe\",\"Article\") counter = 0 for article in query_result:     counter += 1     print(f\"{counter}. { article['title']} (Certainty: {round(article['_additional']['certainty'],3) }) (Distance: {round(article['_additional']['distance'],3) })\") In\u00a0[\u00a0]: Copied! <pre>query_result = near_text_weaviate(\"Famous battles in Scottish history\",\"Article\")\ncounter = 0\nfor article in query_result:\n    counter += 1\n    print(f\"{counter}. { article['title']} (Certainty: {round(article['_additional']['certainty'],3) }) (Distance: {round(article['_additional']['distance'],3) })\")\n</pre> query_result = near_text_weaviate(\"Famous battles in Scottish history\",\"Article\") counter = 0 for article in query_result:     counter += 1     print(f\"{counter}. { article['title']} (Certainty: {round(article['_additional']['certainty'],3) }) (Distance: {round(article['_additional']['distance'],3) })\") In\u00a0[\u00a0]: Copied! <pre>from pymilvus import connections\n\nconnections.connect(host='localhost', port=19530)  # Local instance defaults to port 19530\n</pre> from pymilvus import connections  connections.connect(host='localhost', port=19530)  # Local instance defaults to port 19530 In\u00a0[\u00a0]: Copied! <pre>from pymilvus import utility, Collection, FieldSchema, CollectionSchema, DataType\n\n# Remove the collection if it already exists.\nif utility.has_collection('articles'):\n    utility.drop_collection('articles')\n\nfields = [\n    FieldSchema(name='id', dtype=DataType.INT64),\n    FieldSchema(name='url', dtype=DataType.VARCHAR, max_length=1000),  # Strings have to specify a max length [1, 65535]\n    FieldSchema(name='title', dtype=DataType.VARCHAR, max_length=1000),\n    FieldSchema(name='text', dtype=DataType.VARCHAR, max_length=1000),\n    FieldSchema(name='content_vector', dtype=DataType.FLOAT_VECTOR, dim=len(article_df['content_vector'][0])),\n    FieldSchema(name='vector_id', dtype=DataType.INT64, is_primary=True, auto_id=False),\n]\n\ncol_schema = CollectionSchema(fields)\n\ncol = Collection('articles', col_schema)\n\n# Using a basic HNSW index for this example\nindex = {\n    'index_type': 'HNSW',\n    'metric_type': 'L2',\n    'params': {\n        'M': 8,\n        'efConstruction': 64\n    },\n}\n\ncol.create_index('content_vector', index)\ncol.load()\n</pre> from pymilvus import utility, Collection, FieldSchema, CollectionSchema, DataType  # Remove the collection if it already exists. if utility.has_collection('articles'):     utility.drop_collection('articles')  fields = [     FieldSchema(name='id', dtype=DataType.INT64),     FieldSchema(name='url', dtype=DataType.VARCHAR, max_length=1000),  # Strings have to specify a max length [1, 65535]     FieldSchema(name='title', dtype=DataType.VARCHAR, max_length=1000),     FieldSchema(name='text', dtype=DataType.VARCHAR, max_length=1000),     FieldSchema(name='content_vector', dtype=DataType.FLOAT_VECTOR, dim=len(article_df['content_vector'][0])),     FieldSchema(name='vector_id', dtype=DataType.INT64, is_primary=True, auto_id=False), ]  col_schema = CollectionSchema(fields)  col = Collection('articles', col_schema)  # Using a basic HNSW index for this example index = {     'index_type': 'HNSW',     'metric_type': 'L2',     'params': {         'M': 8,         'efConstruction': 64     }, }  col.create_index('content_vector', index) col.load() In\u00a0[\u00a0]: Copied! <pre># Using the above provided batching function from Pinecone\ndef to_batches(df: pd.DataFrame, batch_size: int) -&gt; Iterator[pd.DataFrame]:\n    splits = df.shape[0] / batch_size\n    if splits &lt;= 1:\n        yield df\n    else:\n        for chunk in np.array_split(df, splits):\n            yield chunk\n\n# Since we are storing the text within Milvus we need to clip any that are over our set limit.\n# We can also set the limit to be higher, but that slows down the search requests as more info \n# needs to be sent back.\ndef shorten_text(text):\n    if len(text) &gt;= 996:\n        return text[:996] + '...'\n    else:\n        return text\n\nfor batch in to_batches(article_df, 1000):\n    batch = batch.drop(columns = ['title_vector'])\n    batch['text'] = batch.text.apply(shorten_text)\n    # Due to the vector_id being converted to a string for compatiblity for other vector dbs,\n    # we want to swap it back to its original form.\n    batch['vector_id'] = batch.vector_id.apply(int)\n    col.insert(batch) \n\ncol.flush()\n</pre> # Using the above provided batching function from Pinecone def to_batches(df: pd.DataFrame, batch_size: int) -&gt; Iterator[pd.DataFrame]:     splits = df.shape[0] / batch_size     if splits &lt;= 1:         yield df     else:         for chunk in np.array_split(df, splits):             yield chunk  # Since we are storing the text within Milvus we need to clip any that are over our set limit. # We can also set the limit to be higher, but that slows down the search requests as more info  # needs to be sent back. def shorten_text(text):     if len(text) &gt;= 996:         return text[:996] + '...'     else:         return text  for batch in to_batches(article_df, 1000):     batch = batch.drop(columns = ['title_vector'])     batch['text'] = batch.text.apply(shorten_text)     # Due to the vector_id being converted to a string for compatiblity for other vector dbs,     # we want to swap it back to its original form.     batch['vector_id'] = batch.vector_id.apply(int)     col.insert(batch)   col.flush()    In\u00a0[\u00a0]: Copied! <pre>def query_article(query, top_k=5):\n    # Generate the embedding with openai\n    embedded_query = openai.Embedding.create(\n        input=query,\n        model=EMBEDDING_MODEL,\n    )[\"data\"][0]['embedding']\n\n    # Using some basic params for HNSW\n    search_param = {\n        'ef': max(64, top_k)\n    }\n\n    # Perform the search.\n    res = col.search([embedded_query], 'content_vector', search_param, output_fields = ['title', 'url'], limit = top_k)\n\n    ret = []\n    for hit in res[0]:\n        # Get the id, distance, and title for the results\n        ret.append({'vector_id': hit.id, 'distance': hit.score, 'title': hit.entity.get('title'), 'url': hit.entity.get('url')})\n    return ret\n    \n\nfor x in query_article('fastest plane ever made', 3):\n    print(x.items())\n</pre> def query_article(query, top_k=5):     # Generate the embedding with openai     embedded_query = openai.Embedding.create(         input=query,         model=EMBEDDING_MODEL,     )[\"data\"][0]['embedding']      # Using some basic params for HNSW     search_param = {         'ef': max(64, top_k)     }      # Perform the search.     res = col.search([embedded_query], 'content_vector', search_param, output_fields = ['title', 'url'], limit = top_k)      ret = []     for hit in res[0]:         # Get the id, distance, and title for the results         ret.append({'vector_id': hit.id, 'distance': hit.score, 'title': hit.entity.get('title'), 'url': hit.entity.get('url')})     return ret       for x in query_article('fastest plane ever made', 3):     print(x.items())  In\u00a0[\u00a0]: Copied! <pre>qdrant = qdrant_client.QdrantClient(host='localhost', prefer_grpc=True)\n</pre> qdrant = qdrant_client.QdrantClient(host='localhost', prefer_grpc=True) In\u00a0[\u00a0]: Copied! <pre>qdrant.get_collections()\n</pre> qdrant.get_collections() In\u00a0[\u00a0]: Copied! <pre>from qdrant_client.http import models as rest\n</pre> from qdrant_client.http import models as rest In\u00a0[\u00a0]: Copied! <pre>vector_size = len(article_df['content_vector'][0])\n\nqdrant.recreate_collection(\n    collection_name='Articles',\n    vectors_config={\n        'title': rest.VectorParams(\n            distance=rest.Distance.COSINE,\n            size=vector_size,\n        ),\n        'content': rest.VectorParams(\n            distance=rest.Distance.COSINE,\n            size=vector_size,\n        ),\n    }\n)\n</pre> vector_size = len(article_df['content_vector'][0])  qdrant.recreate_collection(     collection_name='Articles',     vectors_config={         'title': rest.VectorParams(             distance=rest.Distance.COSINE,             size=vector_size,         ),         'content': rest.VectorParams(             distance=rest.Distance.COSINE,             size=vector_size,         ),     } ) In\u00a0[\u00a0]: Copied! <pre>qdrant.upsert(\n    collection_name='Articles',\n    points=[\n        rest.PointStruct(\n            id=k,\n            vector={\n                'title': v['title_vector'],\n                'content': v['content_vector'],\n            },\n            payload=v.to_dict(),\n        )\n        for k, v in article_df.iterrows()\n    ],\n)\n</pre> qdrant.upsert(     collection_name='Articles',     points=[         rest.PointStruct(             id=k,             vector={                 'title': v['title_vector'],                 'content': v['content_vector'],             },             payload=v.to_dict(),         )         for k, v in article_df.iterrows()     ], ) In\u00a0[\u00a0]: Copied! <pre># Check the collection size to make sure all the points have been stored\nqdrant.count(collection_name='Articles')\n</pre> # Check the collection size to make sure all the points have been stored qdrant.count(collection_name='Articles') In\u00a0[\u00a0]: Copied! <pre>def query_qdrant(query, collection_name, vector_name='title', top_k=20):\n\n    # Creates embedding vector from user query\n    embedded_query = openai.Embedding.create(\n        input=query,\n        model=EMBEDDING_MODEL,\n    )['data'][0]['embedding']\n    \n    query_results = qdrant.search(\n        collection_name=collection_name,\n        query_vector=(\n            vector_name, embedded_query\n        ),\n        limit=top_k,\n    )\n    \n    return query_results\n</pre> def query_qdrant(query, collection_name, vector_name='title', top_k=20):      # Creates embedding vector from user query     embedded_query = openai.Embedding.create(         input=query,         model=EMBEDDING_MODEL,     )['data'][0]['embedding']          query_results = qdrant.search(         collection_name=collection_name,         query_vector=(             vector_name, embedded_query         ),         limit=top_k,     )          return query_results In\u00a0[\u00a0]: Copied! <pre>query_results = query_qdrant('modern art in Europe', 'Articles')\nfor i, article in enumerate(query_results):\n    print(f'{i + 1}. {article.payload[\"title\"]} (Score: {round(article.score, 3)})')\n</pre> query_results = query_qdrant('modern art in Europe', 'Articles') for i, article in enumerate(query_results):     print(f'{i + 1}. {article.payload[\"title\"]} (Score: {round(article.score, 3)})') In\u00a0[\u00a0]: Copied! <pre># This time we'll query using content vector\nquery_results = query_qdrant('Famous battles in Scottish history', 'Articles', 'content')\nfor i, article in enumerate(query_results):\n    print(f'{i + 1}. {article.payload[\"title\"]} (Score: {round(article.score, 3)})')\n</pre> # This time we'll query using content vector query_results = query_qdrant('Famous battles in Scottish history', 'Articles', 'content') for i, article in enumerate(query_results):     print(f'{i + 1}. {article.payload[\"title\"]} (Score: {round(article.score, 3)})') In\u00a0[134]: Copied! <pre>import redis\nfrom redis.commands.search.indexDefinition import (\n    IndexDefinition,\n    IndexType\n)\nfrom redis.commands.search.query import Query\nfrom redis.commands.search.field import (\n    TextField,\n    VectorField\n)\n\nREDIS_HOST =  \"localhost\"\nREDIS_PORT = 6379\nREDIS_PASSWORD = \"\" # default for passwordless Redis\n\n# Connect to Redis\nredis_client = redis.Redis(\n    host=REDIS_HOST,\n    port=REDIS_PORT,\n    password=REDIS_PASSWORD\n)\nredis_client.ping()\n</pre> import redis from redis.commands.search.indexDefinition import (     IndexDefinition,     IndexType ) from redis.commands.search.query import Query from redis.commands.search.field import (     TextField,     VectorField )  REDIS_HOST =  \"localhost\" REDIS_PORT = 6379 REDIS_PASSWORD = \"\" # default for passwordless Redis  # Connect to Redis redis_client = redis.Redis(     host=REDIS_HOST,     port=REDIS_PORT,     password=REDIS_PASSWORD ) redis_client.ping() Out[134]: <pre>True</pre> In\u00a0[135]: Copied! <pre># Constants\nVECTOR_DIM = len(article_df['title_vector'][0]) # length of the vectors\nVECTOR_NUMBER = len(article_df)                 # initial number of vectors\nINDEX_NAME = \"embeddings-index\"                 # name of the search index\nPREFIX = \"doc\"                                  # prefix for the document keys\nDISTANCE_METRIC = \"COSINE\"                      # distance metric for the vectors (ex. COSINE, IP, L2)\n</pre> # Constants VECTOR_DIM = len(article_df['title_vector'][0]) # length of the vectors VECTOR_NUMBER = len(article_df)                 # initial number of vectors INDEX_NAME = \"embeddings-index\"                 # name of the search index PREFIX = \"doc\"                                  # prefix for the document keys DISTANCE_METRIC = \"COSINE\"                      # distance metric for the vectors (ex. COSINE, IP, L2) In\u00a0[136]: Copied! <pre># Define RediSearch fields for each of the columns in the dataset\ntitle = TextField(name=\"title\")\nurl = TextField(name=\"url\")\ntext = TextField(name=\"text\")\ntitle_embedding = VectorField(\"title_vector\",\n    \"FLAT\", {\n        \"TYPE\": \"FLOAT32\",\n        \"DIM\": VECTOR_DIM,\n        \"DISTANCE_METRIC\": DISTANCE_METRIC,\n        \"INITIAL_CAP\": VECTOR_NUMBER,\n    }\n)\ntext_embedding = VectorField(\"content_vector\",\n    \"FLAT\", {\n        \"TYPE\": \"FLOAT32\",\n        \"DIM\": VECTOR_DIM,\n        \"DISTANCE_METRIC\": DISTANCE_METRIC,\n        \"INITIAL_CAP\": VECTOR_NUMBER,\n    }\n)\nfields = [title, url, text, title_embedding, text_embedding]\n</pre> # Define RediSearch fields for each of the columns in the dataset title = TextField(name=\"title\") url = TextField(name=\"url\") text = TextField(name=\"text\") title_embedding = VectorField(\"title_vector\",     \"FLAT\", {         \"TYPE\": \"FLOAT32\",         \"DIM\": VECTOR_DIM,         \"DISTANCE_METRIC\": DISTANCE_METRIC,         \"INITIAL_CAP\": VECTOR_NUMBER,     } ) text_embedding = VectorField(\"content_vector\",     \"FLAT\", {         \"TYPE\": \"FLOAT32\",         \"DIM\": VECTOR_DIM,         \"DISTANCE_METRIC\": DISTANCE_METRIC,         \"INITIAL_CAP\": VECTOR_NUMBER,     } ) fields = [title, url, text, title_embedding, text_embedding] In\u00a0[137]: Copied! <pre># Check if index exists\ntry:\n    redis_client.ft(INDEX_NAME).info()\n    print(\"Index already exists\")\nexcept:\n    # Create RediSearch Index\n    redis_client.ft(INDEX_NAME).create_index(\n        fields = fields,\n        definition = IndexDefinition(prefix=[PREFIX], index_type=IndexType.HASH)\n    )\n</pre> # Check if index exists try:     redis_client.ft(INDEX_NAME).info()     print(\"Index already exists\") except:     # Create RediSearch Index     redis_client.ft(INDEX_NAME).create_index(         fields = fields,         definition = IndexDefinition(prefix=[PREFIX], index_type=IndexType.HASH)     ) In\u00a0[138]: Copied! <pre>def index_documents(client: redis.Redis, prefix: str, documents: pd.DataFrame):\n    records = documents.to_dict(\"records\")\n    for doc in records:\n        key = f\"{prefix}:{str(doc['id'])}\"\n\n        # create byte vectors for title and content\n        title_embedding = np.array(doc[\"title_vector\"], dtype=np.float32).tobytes()\n        content_embedding = np.array(doc[\"content_vector\"], dtype=np.float32).tobytes()\n\n        # replace list of floats with byte vectors\n        doc[\"title_vector\"] = title_embedding\n        doc[\"content_vector\"] = content_embedding\n\n        client.hset(key, mapping = doc)\n</pre> def index_documents(client: redis.Redis, prefix: str, documents: pd.DataFrame):     records = documents.to_dict(\"records\")     for doc in records:         key = f\"{prefix}:{str(doc['id'])}\"          # create byte vectors for title and content         title_embedding = np.array(doc[\"title_vector\"], dtype=np.float32).tobytes()         content_embedding = np.array(doc[\"content_vector\"], dtype=np.float32).tobytes()          # replace list of floats with byte vectors         doc[\"title_vector\"] = title_embedding         doc[\"content_vector\"] = content_embedding          client.hset(key, mapping = doc) In\u00a0[139]: Copied! <pre>index_documents(redis_client, PREFIX, article_df)\nprint(f\"Loaded {redis_client.info()['db0']['keys']} documents in Redis search index with name: {INDEX_NAME}\")\n</pre> index_documents(redis_client, PREFIX, article_df) print(f\"Loaded {redis_client.info()['db0']['keys']} documents in Redis search index with name: {INDEX_NAME}\") <pre>Loaded 25000 documents in Redis search index with name: embeddings-index\n</pre> In\u00a0[140]: Copied! <pre>def search_redis(\n    redis_client: redis.Redis,\n    user_query: str,\n    index_name: str = \"embeddings-index\",\n    vector_field: str = \"title_vector\",\n    return_fields: list = [\"title\", \"url\", \"text\", \"vector_score\"],\n    hybrid_fields = \"*\",\n    k: int = 20,\n) -&gt; List[dict]:\n\n    # Creates embedding vector from user query\n    embedded_query = openai.Embedding.create(input=user_query,\n                                            model=EMBEDDING_MODEL,\n                                            )[\"data\"][0]['embedding']\n\n    # Prepare the Query\n    base_query = f'{hybrid_fields}=&gt;[KNN {k} @{vector_field} $vector AS vector_score]'\n    query = (\n        Query(base_query)\n         .return_fields(*return_fields)\n         .sort_by(\"vector_score\")\n         .paging(0, k)\n         .dialect(2)\n    )\n    params_dict = {\"vector\": np.array(embedded_query).astype(dtype=np.float32).tobytes()}\n\n    # perform vector search\n    results = redis_client.ft(index_name).search(query, params_dict)\n    for i, article in enumerate(results.docs):\n        score = 1 - float(article.vector_score)\n        print(f\"{i}. {article.title} (Score: {round(score ,3) })\")\n    return results.docs\n</pre> def search_redis(     redis_client: redis.Redis,     user_query: str,     index_name: str = \"embeddings-index\",     vector_field: str = \"title_vector\",     return_fields: list = [\"title\", \"url\", \"text\", \"vector_score\"],     hybrid_fields = \"*\",     k: int = 20, ) -&gt; List[dict]:      # Creates embedding vector from user query     embedded_query = openai.Embedding.create(input=user_query,                                             model=EMBEDDING_MODEL,                                             )[\"data\"][0]['embedding']      # Prepare the Query     base_query = f'{hybrid_fields}=&gt;[KNN {k} @{vector_field} $vector AS vector_score]'     query = (         Query(base_query)          .return_fields(*return_fields)          .sort_by(\"vector_score\")          .paging(0, k)          .dialect(2)     )     params_dict = {\"vector\": np.array(embedded_query).astype(dtype=np.float32).tobytes()}      # perform vector search     results = redis_client.ft(index_name).search(query, params_dict)     for i, article in enumerate(results.docs):         score = 1 - float(article.vector_score)         print(f\"{i}. {article.title} (Score: {round(score ,3) })\")     return results.docs In\u00a0[142]: Copied! <pre># For using OpenAI to generate query embedding\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\", \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\nresults = search_redis(redis_client, 'modern art in Europe', k=10)\n</pre> # For using OpenAI to generate query embedding openai.api_key = os.getenv(\"OPENAI_API_KEY\", \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\") results = search_redis(redis_client, 'modern art in Europe', k=10) <pre>0. Museum of Modern Art (Score: 0.875)\n1. Western Europe (Score: 0.867)\n2. Renaissance art (Score: 0.864)\n3. Pop art (Score: 0.86)\n4. Northern Europe (Score: 0.855)\n5. Hellenistic art (Score: 0.853)\n6. Modernist literature (Score: 0.847)\n7. Art film (Score: 0.843)\n8. Central Europe (Score: 0.843)\n9. European (Score: 0.841)\n</pre> In\u00a0[143]: Copied! <pre>results = search_redis(redis_client, 'Famous battles in Scottish history', vector_field='content_vector', k=10)\n</pre> results = search_redis(redis_client, 'Famous battles in Scottish history', vector_field='content_vector', k=10) <pre>0. Battle of Bannockburn (Score: 0.869)\n1. Wars of Scottish Independence (Score: 0.861)\n2. 1651 (Score: 0.853)\n3. First War of Scottish Independence (Score: 0.85)\n4. Robert I of Scotland (Score: 0.846)\n5. 841 (Score: 0.844)\n6. 1716 (Score: 0.844)\n7. 1314 (Score: 0.837)\n8. 1263 (Score: 0.836)\n9. William Wallace (Score: 0.835)\n</pre> In\u00a0[23]: Copied! <pre>def create_hybrid_field(field_name: str, value: str) -&gt; str:\n    return f'@{field_name}:\"{value}\"'\n</pre> def create_hybrid_field(field_name: str, value: str) -&gt; str:     return f'@{field_name}:\"{value}\"' In\u00a0[24]: Copied! <pre># search the content vector for articles about famous battles in Scottish history and only include results with Scottish in the title\nresults = search_redis(redis_client,\n                       \"Famous battles in Scottish history\",\n                       vector_field=\"title_vector\",\n                       k=5,\n                       hybrid_fields=create_hybrid_field(\"title\", \"Scottish\")\n                       )\n</pre> # search the content vector for articles about famous battles in Scottish history and only include results with Scottish in the title results = search_redis(redis_client,                        \"Famous battles in Scottish history\",                        vector_field=\"title_vector\",                        k=5,                        hybrid_fields=create_hybrid_field(\"title\", \"Scottish\")                        ) <pre>0. First War of Scottish Independence (Score: 0.892)\n1. Wars of Scottish Independence (Score: 0.889)\n2. Second War of Scottish Independence (Score: 0.879)\n3. List of Scottish monarchs (Score: 0.873)\n4. Scottish Borders (Score: 0.863)\n</pre> In\u00a0[25]: Copied! <pre># run a hybrid query for articles about Art in the title vector and only include results with the phrase \"Leonardo da Vinci\" in the text\nresults = search_redis(redis_client,\n                       \"Art\",\n                       vector_field=\"title_vector\",\n                       k=5,\n                       hybrid_fields=create_hybrid_field(\"text\", \"Leonardo da Vinci\")\n                       )\n\n# find specific mention of Leonardo da Vinci in the text that our full-text-search query returned\nmention = [sentence for sentence in results[0].text.split(\"\\n\") if \"Leonardo da Vinci\" in sentence][0]\nmention\n</pre> # run a hybrid query for articles about Art in the title vector and only include results with the phrase \"Leonardo da Vinci\" in the text results = search_redis(redis_client,                        \"Art\",                        vector_field=\"title_vector\",                        k=5,                        hybrid_fields=create_hybrid_field(\"text\", \"Leonardo da Vinci\")                        )  # find specific mention of Leonardo da Vinci in the text that our full-text-search query returned mention = [sentence for sentence in results[0].text.split(\"\\n\") if \"Leonardo da Vinci\" in sentence][0] mention <pre>0. Art (Score: 1.0)\n1. Paint (Score: 0.896)\n2. Renaissance art (Score: 0.88)\n3. Painting (Score: 0.874)\n4. Renaissance (Score: 0.846)\n</pre> Out[25]: <pre>'In Europe, after the Middle Ages, there was a \"Renaissance\" which means \"rebirth\". People rediscovered science and artists were allowed to paint subjects other than religious subjects. People like Michelangelo and Leonardo da Vinci still painted religious pictures, but they also now could paint mythological pictures too. These artists also invented perspective where things in the distance look smaller in the picture. This was new because in the Middle Ages people would paint all the figures close up and just overlapping each other. These artists used nudity regularly in their art.'</pre> <p>For more example with Redis as a vector database, see the README and examples within the <code>vector_databases/redis</code> directory of this repository</p> <p>Thanks for following along, you're now equipped to set up your own vector databases and use embeddings to do all kinds of cool things - enjoy! For more complex use cases please continue to work through other cookbook examples in this repo.</p>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/#Using-Vector-Databases-for-Embeddings-Search","title":"Using Vector Databases for Embeddings Search\u00b6","text":"<p>This notebook takes you through a simple flow to download some data, embed it, and then index and search it using a selection of vector databases. This is a common requirement for customers who want to store and search our embeddings with their own data in a secure environment to support production use cases such as chatbots, topic modelling and more.</p>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/#What-is-a-Vector-Database","title":"What is a Vector Database\u00b6","text":"<p>A vector database is a database made to store, manage and search embedding vectors. The use of embeddings to encode unstructured data (text, audio, video and more) as vectors for consumption by machine-learning models has exploded in recent years, due to the increasing effectiveness of AI in solving use cases involving natural language, image recognition and other unstructured forms of data. Vector databases have emerged as an effective solution for enterprises to deliver and scale these use cases.</p>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/#Why-use-a-Vector-Database","title":"Why use a Vector Database\u00b6","text":"<p>Vector databases enable enterprises to take many of the embeddings use cases we've shared in this repo (question and answering, chatbot and recommendation services, for example), and make use of them in a secure, scalable environment. Many of our customers make embeddings solve their problems at small scale but performance and security hold them back from going into production - we see vector databases as a key component in solving that, and in this guide we'll walk through the basics of embedding text data, storing it in a vector database and using it for semantic search.</p>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/#Demo-Flow","title":"Demo Flow\u00b6","text":"<p>The demo flow is:</p> <ul> <li>Setup: Import packages and set any required variables</li> <li>Load data: Load a dataset and embed it using OpenAI embeddings</li> <li>Pinecone<ul> <li>Setup: Here we'll set up the Python client for Pinecone. For more details go here</li> <li>Index Data: We'll create an index with namespaces for titles and content</li> <li>Search Data: We'll test out both namespaces with search queries to confirm it works</li> </ul> </li> <li>Weaviate<ul> <li>Setup: Here we'll set up the Python client for Weaviate. For more details go here</li> <li>Index Data: We'll create an index with title search vectors in it</li> <li>Search Data: We'll run a few searches to confirm it works</li> </ul> </li> <li>Milvus<ul> <li>Setup: Here we'll set up the Python client for Milvus. For more details go here</li> <li>Index Data We'll create a collection and index it for both titles and content</li> <li>Search Data: We'll test out both collections with search queries to confirm it works</li> </ul> </li> <li>Qdrant<ul> <li>Setup: Here we'll set up the Python client for Qdrant. For more details go here</li> <li>Index Data: We'll create a collection with vectors for titles and content</li> <li>Search Data: We'll run a few searches to confirm it works</li> </ul> </li> <li>Redis<ul> <li>Setup: Set up the Redis-Py client. For more details go here</li> <li>Index Data: Create the search index for vector search and hybrid search (vector + full-text search) on all available fields.</li> <li>Search Data: Run a few example queries with various goals in mind.</li> </ul> </li> </ul> <p>Once you've run through this notebook you should have a basic understanding of how to setup and use vector databases, and can move on to more complex use cases making use of our embeddings.</p>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/#Setup","title":"Setup\u00b6","text":"<p>Import the required libraries and set the embedding model that we'd like to use.</p>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/#Load-data","title":"Load data\u00b6","text":"<p>In this section we'll load embedded data that we've prepared previous to this session.</p>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/#Pinecone","title":"Pinecone\u00b6","text":"<p>We'll index these embedded documents in a vector database and search them. The first option we'll look at is Pinecone, a managed vector database which offers a cloud-native option.</p> <p>Before you proceed with this step you'll need to navigate to Pinecone, sign up and then save your API key as an environment variable titled <code>PINECONE_API_KEY</code>.</p> <p>For section we will:</p> <ul> <li>Create an index with multiple namespaces for article titles and content</li> <li>Store our data in the index with separate searchable \"namespaces\" for article titles and content</li> <li>Fire some similarity search queries to verify our setup is working</li> </ul>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/#Create-Index","title":"Create Index\u00b6","text":"<p>First we will need to create an index, which we'll call <code>wikipedia-articles</code>. Once we have an index, we can create multiple namespaces, which can make a single index searchable for various use cases. For more details, consult Pinecone documentation.</p> <p>If you want to batch insert to your index in parallel to increase insertion speed then there is a great guide in the Pinecone documentation on batch inserts in parallel.</p>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/#Search-data","title":"Search data\u00b6","text":"<p>Now we'll enter some dummy searches and check we get decent results back</p>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/#Weaviate","title":"Weaviate\u00b6","text":"<p>Another vector database option we'll explore is Weaviate, which offers both a managed, SaaS option, as well as a self-hosted open source option. As we've already looked at a cloud vector database, we'll try the self-hosted option here.</p> <p>For this we will:</p> <ul> <li>Set up a local deployment of Weaviate</li> <li>Create indices in Weaviate</li> <li>Store our data there</li> <li>Fire some similarity search queries</li> <li>Try a real use case</li> </ul>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/#Bring-your-own-vectors-approach","title":"Bring your own vectors approach\u00b6","text":"<p>In this cookbook, we provide the data with already generated vectors. This is a good approach for scenarios, where your data is already vectorized.</p>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/#Automated-vectorization-with-OpenAI-module","title":"Automated vectorization with OpenAI module\u00b6","text":"<p>For scenarios, where your data is not vectorized yet, you can delegate the vectorization task with OpenAI to Weaviate. Weaviate offers a built-in module text2vec-openai, which takes care of the vectorization for you at:</p> <ul> <li>import</li> <li>for any CRUD operations</li> <li>for semantic search</li> </ul> <p>Check out the Getting Started with Weaviate and OpenAI module cookbook to learn step by step how to import and vectorize data in one step.</p>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/#Setup","title":"Setup\u00b6","text":"<p>To run Weaviate locally, you'll need Docker. Following the instructions contained in the Weaviate documentation here, we created an example docker-compose.yml file in this repo saved at ./weaviate/docker-compose.yml.</p> <p>After starting Docker, you can start Weaviate locally by navigating to the <code>examples/vector_databases/weaviate/</code> directory and running <code>docker-compose up -d</code>.</p>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/#SaaS","title":"SaaS\u00b6","text":"<p>Alternatively you can use Weaviate Cloud Service (WCS) to create a free Weaviate cluster.</p> <ol> <li>create a free account and/or login to WCS</li> <li>create a <code>Weaviate Cluster</code> with the following settings:<ul> <li>Sandbox: <code>Sandbox Free</code></li> <li>Weaviate Version: Use default (latest)</li> <li>OIDC Authentication: <code>Disabled</code></li> </ul> </li> <li>your instance should be ready in a minute or two</li> <li>make a note of the <code>Cluster Id</code>. The link will take you to the full path of your cluster (you will need it later to connect to it). It should be something like: <code>https://your-project-name-suffix.weaviate.network</code></li> </ol>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/#Index-data","title":"Index data\u00b6","text":"<p>In Weaviate you create schemas to capture each of the entities you will be searching.</p> <p>In this case we'll create a schema called Article with the title vector from above included for us to search by.</p> <p>The next few steps closely follow the documentation Weaviate provides here.</p>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/#Search-data","title":"Search data\u00b6","text":"<p>As above, we'll fire some queries at our new Index and get back results based on the closeness to our existing vectors</p>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/#Let-Weaviate-handle-vector-embeddings","title":"Let Weaviate handle vector embeddings\u00b6","text":"<p>Weaviate has a built-in module for OpenAI, which takes care of the steps required to generate a vector embedding for your queries and any CRUD operations.</p> <p>This allows you to run a vector query with the <code>with_near_text</code> filter, which uses your <code>OPEN_API_KEY</code>.</p>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/#Milvus","title":"Milvus\u00b6","text":"<p>The next vector database we will take a look at is Milvus, which also offers a SaaS option like the previous two, as well as self-hosted options using either helm or docker-compose. Sticking to the idea of open source, we will show our self-hosted example here.</p> <p>In this example we will:</p> <ul> <li>Set up a local docker-compose based deployment</li> <li>Create the title and content collections</li> <li>Store our data</li> <li>Test out our system with real world searches</li> </ul>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/#Setup","title":"Setup\u00b6","text":"<p>There are many ways to run Milvus (take a look here), but for now we will stick to a simple standalone Milvus instance with docker-compose.</p> <p>A simple docker-file can be found at <code>./milvus/docker-compose.yaml</code> and can be run using <code>docker-compose up</code> if within that mentioned directory or using <code>docker-compose -f path/to/file up</code></p>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/#Index-data","title":"Index data\u00b6","text":"<p>In Milvus data is stored in the form of collections, with each collection being able to store the vectors and any attributes that come with them.</p> <p>In this case we'll create a collection called articles which contains the url, title, text and the content_embedding.</p> <p>In addition to this we will also create an index on the content embedding. Milvus allows for the use of many SOTA indexing methods, but in this case, we are going to use HNSW.</p>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/#Search","title":"Search\u00b6","text":"<p>Once the data is inserted into Milvus we can perform searches. For this example the search function takes one argument, top_k, how many closest matches to return.</p>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/#Qdrant","title":"Qdrant\u00b6","text":"<p>The last vector database we'll consider is Qdrant. This is a high-performant vector search database written in Rust. It offers both on-premise and cloud version, but for the purposes of that example we're going to use the local deployment mode.</p> <p>Setting everything up will require:</p> <ul> <li>Spinning up a local instance of Qdrant</li> <li>Configuring the collection and storing the data in it</li> <li>Trying out with some queries</li> </ul>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/#Setup","title":"Setup\u00b6","text":"<p>For the local deployment, we are going to use Docker, according to the Qdrant documentation: https://qdrant.tech/documentation/quick_start/. Qdrant requires just a single container, but an example of the docker-compose.yaml file is available at <code>./qdrant/docker-compose.yaml</code> in this repo.</p> <p>You can start Qdrant instance locally by navigating to this directory and running <code>docker-compose up -d</code></p>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/#Index-data","title":"Index data\u00b6","text":"<p>Qdrant stores data in collections where each object is described by at least one vector and may contain an additional metadata called payload. Our collection will be called Articles and each object will be described by both title and content vectors.</p> <p>We'll be using an official qdrant-client package that has all the utility methods already built-in.</p>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/#Search-Data","title":"Search Data\u00b6","text":"<p>Once the data is put into Qdrant we will start querying the collection for the closest vectors. We may provide an additional parameter <code>vector_name</code> to switch from title to content based search.</p>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/#Redis","title":"Redis\u00b6","text":"<p>The last vector database covered in this tutorial is Redis. You most likely already know Redis. What you might not be aware of is the RediSearch module. Enterprises have been using Redis with the RediSearch module for years now across all major cloud providers, Redis Cloud, and on premise. Recently, the Redis team added vector storage and search capability to this module in addition to the features RediSearch already had.</p> <p>Given the large ecosystem around Redis, there are most likely client libraries in the language you need. You can use any standard Redis client library to run RediSearch commands, but it's easiest to use a library that wraps the RediSearch API. Below are a few examples, but you can find more client libraries here.</p> Project Language License Author Stars jedis Java MIT Redis redis-py Python MIT Redis node-redis Node.js MIT Redis nredisstack .NET MIT Redis redisearch-go Go BSD Redis redisearch-api-rs Rust BSD Redis <p>In the below cells, we will walk you through using Redis as a vector database. Since many of you are likely already used to the Redis API, this should be familiar to most.</p>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/#Setup","title":"Setup\u00b6","text":"<p>There are many ways to deploy Redis with RediSearch. The easiest way to get started is to use Docker, but there are are many potential options for deployment. For other deployment options, see the redis directory in this repo.</p> <p>For this tutorial, we will use Redis Stack on Docker.</p> <p>Start a version of Redis with RediSearch (Redis Stack) by running the following docker command</p> <pre>$ cd redis\n$ docker compose up -d\n</pre> <p>This also includes the RedisInsight GUI for managing your Redis database which you can view at http://localhost:8001 once you start the docker container.</p> <p>You're all set up and ready to go! Next, we import and create our client for communicating with the Redis database we just created.</p>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/#Creating-a-Search-Index","title":"Creating a Search Index\u00b6","text":"<p>The below cells will show how to specify and create a search index in Redis. We will</p> <ol> <li>Set some constants for defining our index like the distance metric and the index name</li> <li>Define the index schema with RediSearch fields</li> <li>Create the index</li> </ol>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/#Load-Documents-into-the-Index","title":"Load Documents into the Index\u00b6","text":"<p>Now that we have a search index, we can load documents into it. We will use the same documents we used in the previous examples. In Redis, either the Hash or JSON (if using RedisJSON in addition to RediSearch) data types can be used to store documents. We will use the HASH data type in this example. The below cells will show how to load documents into the index.</p>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/#Running-Search-Queries","title":"Running Search Queries\u00b6","text":"<p>Now that we have a search index and documents loaded into it, we can run search queries. Below we will provide a function that will run a search query and return the results. Using this function we run a few queries that will show how you can utilize Redis as a vector database. Each example will demonstrate specific features to keep in mind when developing your search application with Redis.</p> <ol> <li>Return Fields: You can specify which fields you want to return in the search results. This is useful if you only want to return a subset of the fields in your documents and doesn't require a separate call to retrieve documents. In the below example, we will only return the <code>title</code> field in the search results.</li> <li>Hybrid Search: You can combine vector search with any of the other RediSearch fields for hybrid search such as full text search, tag, geo, and numeric. In the below example, we will combine vector search with full text search.</li> </ol>"},{"location":"examples/vector_databases/Using_vector_databases_for_embeddings_search/#Hybrid-Queries-with-Redis","title":"Hybrid Queries with Redis\u00b6","text":"<p>The previous examples showed how run vector search queries with RediSearch. In this section, we will show how to combine vector search with other RediSearch fields for hybrid search. In the below example, we will combine vector search with full text search.</p>"},{"location":"examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI/","title":"Using Qdrant as a vector database for OpenAI embeddings","text":"In\u00a0[1]: Copied! <pre>! docker-compose up -d\n</pre> ! docker-compose up -d <pre>qdrant_qdrant_1 is up-to-date\r\n</pre> <p>We might validate if the server was launched successfully by running a simple curl command:</p> In\u00a0[2]: Copied! <pre>! curl http://localhost:6333\n</pre> ! curl http://localhost:6333 <pre>{\"title\":\"qdrant - vector search engine\",\"version\":\"1.0.1\"}</pre> In\u00a0[\u00a0]: Copied! <pre>! pip install openai qdrant-client pandas wget\n</pre> ! pip install openai qdrant-client pandas wget In\u00a0[4]: Copied! <pre># Test that your OpenAI API key is correctly set as an environment variable\n# Note. if you run this notebook locally, you will need to reload your terminal and the notebook for the env variables to be live.\nimport os\n\n# Note. alternatively you can set a temporary env variable like this:\n# os.environ[\"OPENAI_API_KEY\"] = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n\nif os.getenv(\"OPENAI_API_KEY\") is not None:\n    print(\"OPENAI_API_KEY is ready\")\nelse:\n    print(\"OPENAI_API_KEY environment variable not found\")\n</pre> # Test that your OpenAI API key is correctly set as an environment variable # Note. if you run this notebook locally, you will need to reload your terminal and the notebook for the env variables to be live. import os  # Note. alternatively you can set a temporary env variable like this: # os.environ[\"OPENAI_API_KEY\"] = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"  if os.getenv(\"OPENAI_API_KEY\") is not None:     print(\"OPENAI_API_KEY is ready\") else:     print(\"OPENAI_API_KEY environment variable not found\") <pre>OPENAI_API_KEY is ready\n</pre> In\u00a0[5]: Copied! <pre>import qdrant_client\n\nclient = qdrant_client.QdrantClient(\n    host=\"localhost\",\n    prefer_grpc=True,\n)\n</pre> import qdrant_client  client = qdrant_client.QdrantClient(     host=\"localhost\",     prefer_grpc=True, ) <p>We can test the connection by running any available method:</p> In\u00a0[6]: Copied! <pre>client.get_collections()\n</pre> client.get_collections() Out[6]: <pre>CollectionsResponse(collections=[])</pre> In\u00a0[7]: Copied! <pre>import wget\n\nembeddings_url = \"https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip\"\n\n# The file is ~700 MB so this will take some time\nwget.download(embeddings_url)\n</pre> import wget  embeddings_url = \"https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip\"  # The file is ~700 MB so this will take some time wget.download(embeddings_url) <pre>100% [......................................................................] 698933052 / 698933052</pre> Out[7]: <pre>'vector_database_wikipedia_articles_embedded.zip'</pre> <p>The downloaded file has to be then extracted:</p> In\u00a0[8]: Copied! <pre>import zipfile\n\nwith zipfile.ZipFile(\"vector_database_wikipedia_articles_embedded.zip\",\"r\") as zip_ref:\n    zip_ref.extractall(\"../data\")\n</pre> import zipfile  with zipfile.ZipFile(\"vector_database_wikipedia_articles_embedded.zip\",\"r\") as zip_ref:     zip_ref.extractall(\"../data\") <p>And we can finally load it from the provided CSV file:</p> In\u00a0[9]: Copied! <pre>import pandas as pd\n\nfrom ast import literal_eval\n\narticle_df = pd.read_csv('../data/vector_database_wikipedia_articles_embedded.csv')\n# Read vectors from strings back into a list\narticle_df[\"title_vector\"] = article_df.title_vector.apply(literal_eval)\narticle_df[\"content_vector\"] = article_df.content_vector.apply(literal_eval)\narticle_df.head()\n</pre> import pandas as pd  from ast import literal_eval  article_df = pd.read_csv('../data/vector_database_wikipedia_articles_embedded.csv') # Read vectors from strings back into a list article_df[\"title_vector\"] = article_df.title_vector.apply(literal_eval) article_df[\"content_vector\"] = article_df.content_vector.apply(literal_eval) article_df.head() Out[9]: id url title text title_vector content_vector vector_id 0 1 https://simple.wikipedia.org/wiki/April April April is the fourth month of the year in the J... [0.001009464613161981, -0.020700545981526375, ... [-0.011253940872848034, -0.013491976074874401,... 0 1 2 https://simple.wikipedia.org/wiki/August August August (Aug.) is the eighth month of the year ... [0.0009286514250561595, 0.000820168002974242, ... [0.0003609954728744924, 0.007262262050062418, ... 1 2 6 https://simple.wikipedia.org/wiki/Art Art Art is a creative activity that expresses imag... [0.003393713850528002, 0.0061537534929811954, ... [-0.004959689453244209, 0.015772193670272827, ... 2 3 8 https://simple.wikipedia.org/wiki/A A A or a is the first letter of the English alph... [0.0153952119871974, -0.013759135268628597, 0.... [0.024894846603274345, -0.022186409682035446, ... 3 4 9 https://simple.wikipedia.org/wiki/Air Air Air refers to the Earth's atmosphere. Air is a... [0.02224554680287838, -0.02044147066771984, -0... [0.021524671465158463, 0.018522677943110466, -... 4 In\u00a0[10]: Copied! <pre>from qdrant_client.http import models as rest\n\nvector_size = len(article_df[\"content_vector\"][0])\n\nclient.recreate_collection(\n    collection_name=\"Articles\",\n    vectors_config={\n        \"title\": rest.VectorParams(\n            distance=rest.Distance.COSINE,\n            size=vector_size,\n        ),\n        \"content\": rest.VectorParams(\n            distance=rest.Distance.COSINE,\n            size=vector_size,\n        ),\n    }\n)\n</pre> from qdrant_client.http import models as rest  vector_size = len(article_df[\"content_vector\"][0])  client.recreate_collection(     collection_name=\"Articles\",     vectors_config={         \"title\": rest.VectorParams(             distance=rest.Distance.COSINE,             size=vector_size,         ),         \"content\": rest.VectorParams(             distance=rest.Distance.COSINE,             size=vector_size,         ),     } ) Out[10]: <pre>True</pre> In\u00a0[11]: Copied! <pre>client.upsert(\n    collection_name=\"Articles\",\n    points=[\n        rest.PointStruct(\n            id=k,\n            vector={\n                \"title\": v[\"title_vector\"],\n                \"content\": v[\"content_vector\"],\n            },\n            payload=v.to_dict(),\n        )\n        for k, v in article_df.iterrows()\n    ],\n)\n</pre> client.upsert(     collection_name=\"Articles\",     points=[         rest.PointStruct(             id=k,             vector={                 \"title\": v[\"title_vector\"],                 \"content\": v[\"content_vector\"],             },             payload=v.to_dict(),         )         for k, v in article_df.iterrows()     ], ) Out[11]: <pre>UpdateResult(operation_id=0, status=&lt;UpdateStatus.COMPLETED: 'completed'&gt;)</pre> In\u00a0[16]: Copied! <pre># Check the collection size to make sure all the points have been stored\nclient.count(collection_name=\"Articles\")\n</pre> # Check the collection size to make sure all the points have been stored client.count(collection_name=\"Articles\") Out[16]: <pre>CountResult(count=25000)</pre> In\u00a0[13]: Copied! <pre>import openai\n\ndef query_qdrant(query, collection_name, vector_name=\"title\", top_k=20):\n    # Creates embedding vector from user query\n    embedded_query = openai.Embedding.create(\n        input=query,\n        model=\"text-embedding-ada-002\",\n    )[\"data\"][0][\"embedding\"]\n\n    query_results = client.search(\n        collection_name=collection_name,\n        query_vector=(\n            vector_name, embedded_query\n        ),\n        limit=top_k,\n    )\n\n    return query_results\n</pre> import openai  def query_qdrant(query, collection_name, vector_name=\"title\", top_k=20):     # Creates embedding vector from user query     embedded_query = openai.Embedding.create(         input=query,         model=\"text-embedding-ada-002\",     )[\"data\"][0][\"embedding\"]      query_results = client.search(         collection_name=collection_name,         query_vector=(             vector_name, embedded_query         ),         limit=top_k,     )      return query_results In\u00a0[14]: Copied! <pre>query_results = query_qdrant(\"modern art in Europe\", \"Articles\")\nfor i, article in enumerate(query_results):\n    print(f\"{i + 1}. {article.payload['title']} (Score: {round(article.score, 3)})\")\n</pre> query_results = query_qdrant(\"modern art in Europe\", \"Articles\") for i, article in enumerate(query_results):     print(f\"{i + 1}. {article.payload['title']} (Score: {round(article.score, 3)})\") <pre>1. Museum of Modern Art (Score: 0.875)\n2. Western Europe (Score: 0.868)\n3. Renaissance art (Score: 0.864)\n4. Pop art (Score: 0.86)\n5. Northern Europe (Score: 0.855)\n6. Hellenistic art (Score: 0.853)\n7. Modernist literature (Score: 0.847)\n8. Art film (Score: 0.843)\n9. Central Europe (Score: 0.843)\n10. European (Score: 0.841)\n11. Art (Score: 0.841)\n12. Byzantine art (Score: 0.841)\n13. Postmodernism (Score: 0.84)\n14. Eastern Europe (Score: 0.839)\n15. Europe (Score: 0.839)\n16. Cubism (Score: 0.839)\n17. Impressionism (Score: 0.838)\n18. Bauhaus (Score: 0.838)\n19. Surrealism (Score: 0.837)\n20. Expressionism (Score: 0.837)\n</pre> In\u00a0[15]: Copied! <pre># This time we'll query using content vector\nquery_results = query_qdrant(\"Famous battles in Scottish history\", \"Articles\", \"content\")\nfor i, article in enumerate(query_results):\n    print(f\"{i + 1}. {article.payload['title']} (Score: {round(article.score, 3)})\")\n</pre> # This time we'll query using content vector query_results = query_qdrant(\"Famous battles in Scottish history\", \"Articles\", \"content\") for i, article in enumerate(query_results):     print(f\"{i + 1}. {article.payload['title']} (Score: {round(article.score, 3)})\") <pre>1. Battle of Bannockburn (Score: 0.869)\n2. Wars of Scottish Independence (Score: 0.861)\n3. 1651 (Score: 0.853)\n4. First War of Scottish Independence (Score: 0.85)\n5. Robert I of Scotland (Score: 0.846)\n6. 841 (Score: 0.844)\n7. 1716 (Score: 0.844)\n8. 1314 (Score: 0.837)\n9. 1263 (Score: 0.836)\n10. William Wallace (Score: 0.835)\n11. Stirling (Score: 0.831)\n12. 1306 (Score: 0.831)\n13. 1746 (Score: 0.83)\n14. 1040s (Score: 0.828)\n15. 1106 (Score: 0.827)\n16. 1304 (Score: 0.827)\n17. David II of Scotland (Score: 0.825)\n18. Braveheart (Score: 0.824)\n19. 1124 (Score: 0.824)\n20. July 27 (Score: 0.823)\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI/#Using-Qdrant-as-a-vector-database-for-OpenAI-embeddings","title":"Using Qdrant as a vector database for OpenAI embeddings\u00b6","text":"<p>This notebook guides you step by step on using Qdrant as a vector database for OpenAI embeddings. Qdrant is a high-performant vector search database written in Rust. It offers REST and gRPC APIs to manage your embeddings. There is an official Python qdrant-client that eases the integration with your apps.</p> <p>This notebook presents an end-to-end process of:</p> <ol> <li>Using precomputed embeddings created by OpenAI API.</li> <li>Storing the embeddings in a local instance of Qdrant.</li> <li>Converting raw text query to an embedding with OpenAI API.</li> <li>Using Qdrant to perform the nearest neighbour search in the created collection.</li> </ol>"},{"location":"examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI/#What-is-Qdrant","title":"What is Qdrant\u00b6","text":"<p>Qdrant is an Open Source vector database that allows storing neural embeddings along with the metadata, a.k.a payload. Payloads are not only available for keeping some additional attributes of a particular point, but might be also used for filtering. Qdrant offers a unique filtering mechanism which is built-in into the vector search phase, what makes it really efficient.</p>"},{"location":"examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI/#Deployment-options","title":"Deployment options\u00b6","text":"<p>Qdrant might be launched in various ways, depending on the target load on the application it might be hosted:</p> <ul> <li>Locally or on premise, with Docker containers</li> <li>On Kubernetes cluster, with the Helm chart</li> <li>Using Qdrant Cloud</li> </ul>"},{"location":"examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI/#Integration","title":"Integration\u00b6","text":"<p>Qdrant provides both REST and gRPC APIs which makes integration easy, no matter the programming language you use. However, there are some official clients for the most popular languages available, and if you use Python then the Python Qdrant client library might be the best choice.</p>"},{"location":"examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI/#Prerequisites","title":"Prerequisites\u00b6","text":"<p>For the purposes of this exercise we need to prepare a couple of things:</p> <ol> <li>Qdrant server instance. In our case a local Docker container.</li> <li>The qdrant-client library to interact with the vector database.</li> <li>An OpenAI API key.</li> </ol>"},{"location":"examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI/#Start-Qdrant-server","title":"Start Qdrant server\u00b6","text":"<p>We're going to use a local Qdrant instance running in a Docker container. The easiest way to launch it is to use the attached [docker-compose.yaml] file and run the following command:</p>"},{"location":"examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI/#Install-requirements","title":"Install requirements\u00b6","text":"<p>This notebook obviously requires the <code>openai</code> and <code>qdrant-client</code> packages, but there are also some other additional libraries we will use. The following command installs them all:</p>"},{"location":"examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI/#Prepare-your-OpenAI-API-key","title":"Prepare your OpenAI API key\u00b6","text":"<p>The OpenAI API key is used for vectorization of the documents and queries.</p> <p>If you don't have an OpenAI API key, you can get one from https://beta.openai.com/account/api-keys.</p> <p>Once you get your key, please add it to your environment variables as <code>OPENAI_API_KEY</code>.</p>"},{"location":"examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI/#Connect-to-Qdrant","title":"Connect to Qdrant\u00b6","text":"<p>Connecting to a running instance of Qdrant server is easy with the official Python library:</p>"},{"location":"examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI/#Load-data","title":"Load data\u00b6","text":"<p>In this section we are going to load the data prepared previous to this session, so you don't have to recompute the embeddings of Wikipedia articles with your own credits.</p>"},{"location":"examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI/#Index-data","title":"Index data\u00b6","text":"<p>Qdrant stores data in collections where each object is described by at least one vector and may contain an additional metadata called payload. Our collection will be called Articles and each object will be described by both title and content vectors. Qdrant does not require you to set up any kind of schema beforehand, so you can freely put points to the collection with a simple setup only.</p> <p>We will start with creating a collection, and then we will fill it with our precomputed embeddings.</p>"},{"location":"examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI/#Search-data","title":"Search data\u00b6","text":"<p>Once the data is put into Qdrant we will start querying the collection for the closest vectors. We may provide an additional parameter <code>vector_name</code> to switch from title to content based search. Since the precomputed embeddings were created with <code>text-embedding-ada-002</code> OpenAI model we also have to use it during search.</p>"},{"location":"examples/vector_databases/qdrant/QA_with_Langchain_Qdrant_and_OpenAI/","title":"Question Answering with Langchain, Qdrant and OpenAI","text":"In\u00a0[1]: Copied! <pre>! docker-compose up -d\n</pre> ! docker-compose up -d <pre>Starting qdrant_qdrant_1 ... \nting qdrant_qdrant_1 ... done</pre> <p>We might validate if the server was launched successfully by running a simple curl command:</p> In\u00a0[2]: Copied! <pre>! curl http://localhost:6333\n</pre> ! curl http://localhost:6333 <pre>{\"title\":\"qdrant - vector search engine\",\"version\":\"1.0.1\"}</pre> In\u00a0[\u00a0]: Copied! <pre>! pip install openai qdrant-client \"langchain==0.0.100\" wget\n</pre> ! pip install openai qdrant-client \"langchain==0.0.100\" wget In\u00a0[4]: Copied! <pre># Test that your OpenAI API key is correctly set as an environment variable\n# Note. if you run this notebook locally, you will need to reload your terminal and the notebook for the env variables to be live.\nimport os\n\n# Note. alternatively you can set a temporary env variable like this:\n# os.environ[\"OPENAI_API_KEY\"] = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n\nif os.getenv(\"OPENAI_API_KEY\") is not None:\n    print(\"OPENAI_API_KEY is ready\")\nelse:\n    print(\"OPENAI_API_KEY environment variable not found\")\n</pre> # Test that your OpenAI API key is correctly set as an environment variable # Note. if you run this notebook locally, you will need to reload your terminal and the notebook for the env variables to be live. import os  # Note. alternatively you can set a temporary env variable like this: # os.environ[\"OPENAI_API_KEY\"] = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"  if os.getenv(\"OPENAI_API_KEY\") is not None:     print(\"OPENAI_API_KEY is ready\") else:     print(\"OPENAI_API_KEY environment variable not found\") <pre>OPENAI_API_KEY is ready\n</pre> In\u00a0[5]: Copied! <pre>import wget\n\n# All the examples come from https://ai.google.com/research/NaturalQuestions\n# This is a sample of the training set that we download and extract for some\n# futher processing.\nwget.download(\"https://storage.googleapis.com/dataset-natural-questions/questions.json\")\nwget.download(\"https://storage.googleapis.com/dataset-natural-questions/answers.json\")\n</pre> import wget  # All the examples come from https://ai.google.com/research/NaturalQuestions # This is a sample of the training set that we download and extract for some # futher processing. wget.download(\"https://storage.googleapis.com/dataset-natural-questions/questions.json\") wget.download(\"https://storage.googleapis.com/dataset-natural-questions/answers.json\") <pre>100% [..............................................................................] 95372 / 95372</pre> Out[5]: <pre>'answers.json'</pre> In\u00a0[6]: Copied! <pre>import json\n\nwith open(\"questions.json\", \"r\") as fp:\n    questions = json.load(fp)\n\nwith open(\"answers.json\", \"r\") as fp:\n    answers = json.load(fp)\n</pre> import json  with open(\"questions.json\", \"r\") as fp:     questions = json.load(fp)  with open(\"answers.json\", \"r\") as fp:     answers = json.load(fp) In\u00a0[7]: Copied! <pre>print(questions[0])\n</pre> print(questions[0]) <pre>when is the last episode of season 8 of the walking dead\n</pre> In\u00a0[8]: Copied! <pre>print(answers[0])\n</pre> print(answers[0]) <pre>No . overall No. in season Title Directed by Written by Original air date U.S. viewers ( millions ) 100 `` Mercy '' Greg Nicotero Scott M. Gimple October 22 , 2017 ( 2017 - 10 - 22 ) 11.44 Rick , Maggie , and Ezekiel rally their communities together to take down Negan . Gregory attempts to have the Hilltop residents side with Negan , but they all firmly stand behind Maggie . The group attacks the Sanctuary , taking down its fences and flooding the compound with walkers . With the Sanctuary defaced , everyone leaves except Gabriel , who reluctantly stays to save Gregory , but is left behind when Gregory abandons him . Surrounded by walkers , Gabriel hides in a trailer , where he is trapped inside with Negan . 101 `` The Damned '' Rosemary Rodriguez Matthew Negrete &amp; Channing Powell October 29 , 2017 ( 2017 - 10 - 29 ) 8.92 Rick 's forces split into separate parties to attack several of the Saviors ' outposts , during which many members of the group are killed ; Eric is critically injured and rushed away by Aaron . Jesus stops Tara and Morgan from executing a group of surrendered Saviors . While clearing an outpost with Daryl , Rick is confronted and held at gunpoint by Morales , a survivor he met in the initial Atlanta camp , who is now with the Saviors . 102 `` Monsters '' Greg Nicotero Matthew Negrete &amp; Channing Powell November 5 , 2017 ( 2017 - 11 - 05 ) 8.52 Daryl finds Morales threatening Rick and kills him ; the duo then pursue a group of Saviors who are transporting weapons to another outpost . Gregory returns to Hilltop , and after a heated argument , Maggie ultimately allows him back in the community . Eric dies from his injuries , leaving Aaron distraught . Despite Tara and Morgan 's objections , Jesus leads the group of surrendered Saviors to Hilltop . Ezekiel 's group attacks another Savior compound , during which several Kingdommers are shot while protecting Ezekiel . 103 `` Some Guy '' Dan Liu David Leslie Johnson November 12 , 2017 ( 2017 - 11 - 12 ) 8.69 Ezekiel 's group is overwhelmed by the Saviors , who kill all of them except for Ezekiel himself and Jerry . Carol clears the inside of the compound , killing all but two Saviors , who almost escape but are eventually caught by Rick and Daryl . En route to the Kingdom , Ezekiel , Jerry , and Carol are surrounded by walkers , but Shiva sacrifices herself to save them . The trio returns to the Kingdom , where Ezekiel 's confidence in himself as a leader has diminished . 104 5 `` The Big Scary U '' Michael E. Satrazemis Story by : Scott M. Gimple &amp; David Leslie Johnson &amp; Angela Kang Teleplay by : David Leslie Johnson &amp; Angela Kang November 19 , 2017 ( 2017 - 11 - 19 ) 7.85 After confessing their sins to each other , Gabriel and Negan manage to escape from the trailer . Simon and the other lieutenants grow suspicious of each other , knowing that Rick 's forces must have inside information . The workers in the Sanctuary become increasingly frustrated with their living conditions , and a riot nearly ensues , until Negan returns and restores order . Gabriel is locked in a cell , where Eugene discovers him sick and suffering . Meanwhile , Rick and Daryl argue over how to take out the Saviors , leading Daryl to abandon Rick . 105 6 `` The King , the Widow , and Rick '' John Polson Angela Kang &amp; Corey Reed November 26 , 2017 ( 2017 - 11 - 26 ) 8.28 Rick visits Jadis in hopes of convincing her to turn against Negan ; Jadis refuses , and locks Rick in a shipping container . Carl encounters Siddiq in the woods and recruits him to Alexandria . Daryl and Tara plot to deviate from Rick 's plans by destroying the Sanctuary . Ezekiel isolates himself at the Kingdom , where Carol tries to encourage him to be the leader his people need . Maggie has the group of captured Saviors placed in a holding area and forces Gregory to join them as punishment for betraying Hilltop . 106 7 `` Time for After '' Larry Teng Matthew Negrete &amp; Corey Reed December 3 , 2017 ( 2017 - 12 - 03 ) 7.47 After learning of Dwight 's association with Rick 's group , Eugene affirms his loyalty to Negan and outlines a plan to get rid of the walkers surrounding the Sanctuary . With help from Morgan and Tara , Daryl drives a truck through the Sanctuary 's walls , flooding its interior with walkers , killing many Saviors . Rick finally convinces Jadis and the Scavengers to align with him , and they plan to force the Saviors to surrender . However , when they arrive at the Sanctuary , Rick is horrified to see the breached walls and no sign of the walker herd . 107 8 `` How It 's Gotta Be '' Michael E. Satrazemis David Leslie Johnson &amp; Angela Kang December 10 , 2017 ( 2017 - 12 - 10 ) 7.89 Eugene 's plan allows the Saviors to escape , and separately , the Saviors waylay the Alexandria , Hilltop , and Kingdom forces . The Scavengers abandon Rick , after which he returns to Alexandria . Ezekiel ensures that the Kingdom residents are able to escape before locking himself in the community with the Saviors . Eugene aids Gabriel and Doctor Carson in escaping the Sanctuary in order to ease his conscience . Negan attacks Alexandria , but Carl devises a plan to allow the Alexandria residents to escape into the sewers . Carl reveals he was bitten by a walker while escorting Siddiq to Alexandria . 108 9 `` Honor '' Greg Nicotero Matthew Negrete &amp; Channing Powell February 25 , 2018 ( 2018 - 02 - 25 ) 8.28 After the Saviors leave Alexandria , the survivors make for the Hilltop while Rick and Michonne stay behind to say their final goodbyes to a dying Carl , who pleads with Rick to build a better future alongside the Saviors before killing himself . In the Kingdom , Morgan and Carol launch a rescue mission for Ezekiel . Although they are successful and retake the Kingdom , the Saviors ' lieutenant Gavin is killed by Benjamin 's vengeful brother Henry . 109 10 `` The Lost and the Plunderers '' TBA TBA March 4 , 2018 ( 2018 - 03 - 04 ) TBD 110 11 `` Dead or Alive Or '' TBA TBA March 11 , 2018 ( 2018 - 03 - 11 ) TBD 111 12 `` The Key '' TBA TBA March 18 , 2018 ( 2018 - 03 - 18 ) TBD\n</pre> In\u00a0[9]: Copied! <pre>from langchain.vectorstores import Qdrant\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain import VectorDBQA, OpenAI\n\nembeddings = OpenAIEmbeddings()\ndoc_store = Qdrant.from_texts(\n    answers, embeddings, host=\"localhost\" \n)\n</pre> from langchain.vectorstores import Qdrant from langchain.embeddings import OpenAIEmbeddings from langchain import VectorDBQA, OpenAI  embeddings = OpenAIEmbeddings() doc_store = Qdrant.from_texts(     answers, embeddings, host=\"localhost\"  ) <p>At this stage all the possible answers are already stored in Qdrant, so we can define the whole QA chain.</p> In\u00a0[10]: Copied! <pre>llm = OpenAI()\nqa = VectorDBQA.from_chain_type(\n    llm=llm, \n    chain_type=\"stuff\", \n    vectorstore=doc_store,\n    return_source_documents=False,\n)\n</pre> llm = OpenAI() qa = VectorDBQA.from_chain_type(     llm=llm,      chain_type=\"stuff\",      vectorstore=doc_store,     return_source_documents=False, ) In\u00a0[11]: Copied! <pre>import random\n\nrandom.seed(52)\nselected_questions = random.choices(questions, k=5)\n</pre> import random  random.seed(52) selected_questions = random.choices(questions, k=5) In\u00a0[12]: Copied! <pre>for question in selected_questions:\n    print(\"&gt;\", question)\n    print(qa.run(question), end=\"\\n\\n\")\n</pre> for question in selected_questions:     print(\"&gt;\", question)     print(qa.run(question), end=\"\\n\\n\") <pre>&gt; where do frankenstein and the monster first meet\n Victor and the Creature first meet in the mountains.\n\n&gt; who are the actors in fast and furious\n The actors in the Fast and Furious films are Vin Diesel, Paul Walker, Michelle Rodriguez, Jordana Brewster, Tyrese Gibson, Ludacris, Lucas Black, Sung Kang, Gal Gadot, Dwayne Johnson, Matt Schulze, Chad Lindberg, Johnny Strong, Eva Mendes, Devon Aoki, Nathalie Kelley, Bow Wow, Tego Calder\u00f3n, Don Omar, Elsa Pataky, Kurt Russell, Nathalie Emmanuel, Scott Eastwood, Noel Gugliemi, Ja Rule, Thom Barry, Ted Levine, Minka Kelly, James Remar, Amaury Nolasco, Michael Ealy, MC Jin, Brian Goodman, Lynda Boyd, Jason Tobin, Neela, Liza Lapira, Alimi Ballard, Yorgo Constantine, Geoff Meed, Jeimy Osorio, Max William Crane, Charlie &amp; Miller Kimsey, Eden Estrella, Romeo Santos, John Brotherton, Helen Mirren, Celestino Cornielle, Janmarco Santiago, Carlos De La Hoz, James Ayoub, Rick Yune, Cole Hauser, Brian Tee, John Ortiz, Luke Evans, Jason Statham, Charlize Theron, Reggie Lee, Mo Gallini, Roberto Sanchez, Leonardo\n\n&gt; properties of red black tree in data structure\n Red black trees are a type of binary tree with a special set of properties. Each node is either red or black, the root is black, and if a node is red, then both its children are black. Every path from a given node to any of its descendant NIL nodes contains the same number of black nodes. The number of black nodes from the root to a node is the node's black depth, and the uniform number of black nodes in all paths from root to the leaves is called the black-height of the red-black tree.\n\n&gt; who designed the national coat of arms of south africa\n Iaan Bekker\n\n&gt; caravaggio's death of the virgin pamela askew\n I don't know.\n\n</pre> In\u00a0[13]: Copied! <pre>from langchain.prompts import PromptTemplate\n</pre> from langchain.prompts import PromptTemplate In\u00a0[14]: Copied! <pre>custom_prompt = \"\"\"\nUse the following pieces of context to answer the question at the end. Please provide\na short single-sentence summary answer only. If you don't know the answer or if it's \nnot present in given context, don't try to make up an answer, but suggest me a random \nunrelated song title I could listen to. \nContext: {context}\nQuestion: {question}\nHelpful Answer:\n\"\"\"\n</pre> custom_prompt = \"\"\" Use the following pieces of context to answer the question at the end. Please provide a short single-sentence summary answer only. If you don't know the answer or if it's  not present in given context, don't try to make up an answer, but suggest me a random  unrelated song title I could listen to.  Context: {context} Question: {question} Helpful Answer: \"\"\" In\u00a0[15]: Copied! <pre>custom_prompt_template = PromptTemplate(\n    template=custom_prompt, input_variables=[\"context\", \"question\"]\n)\n</pre> custom_prompt_template = PromptTemplate(     template=custom_prompt, input_variables=[\"context\", \"question\"] ) In\u00a0[16]: Copied! <pre>custom_qa = VectorDBQA.from_chain_type(\n    llm=llm, \n    chain_type=\"stuff\", \n    vectorstore=doc_store,\n    return_source_documents=False,\n    chain_type_kwargs={\"prompt\": custom_prompt_template},\n)\n</pre> custom_qa = VectorDBQA.from_chain_type(     llm=llm,      chain_type=\"stuff\",      vectorstore=doc_store,     return_source_documents=False,     chain_type_kwargs={\"prompt\": custom_prompt_template}, ) In\u00a0[17]: Copied! <pre>random.seed(41)\nfor question in random.choices(questions, k=5):\n    print(\"&gt;\", question)\n    print(custom_qa.run(question), end=\"\\n\\n\")\n</pre> random.seed(41) for question in random.choices(questions, k=5):     print(\"&gt;\", question)     print(custom_qa.run(question), end=\"\\n\\n\") <pre>&gt; what was uncle jesse's original last name on full house\nUncle Jesse's original last name on Full House was Cochran.\n\n&gt; when did the volcano erupt in indonesia 2018\nNo volcanic eruption is mentioned in the given context. Suggested Song: \"Ring of Fire\" by Johnny Cash.\n\n&gt; what does a dualist way of thinking mean\nDualist way of thinking means that the mind and body are separate entities, with the mind being a non-physical substance.\n\n&gt; the first civil service commission in india was set up on the basis of recommendation of\nThe first Civil Service Commission in India was not set up on the basis of a recommendation.\n\n&gt; how old do you have to be to get a tattoo in utah\nIn Utah, you must be at least 18 years old to get a tattoo.\n\n</pre>"},{"location":"examples/vector_databases/qdrant/QA_with_Langchain_Qdrant_and_OpenAI/#Question-Answering-with-Langchain,-Qdrant-and-OpenAI","title":"Question Answering with Langchain, Qdrant and OpenAI\u00b6","text":"<p>This notebook presents how to implement a Question Answering system with Langchain, Qdrant as a knowledge based and OpenAI embeddings. If you are not familiar with Qdrant, it's better to check out the Getting_started_with_Qdrant_and_OpenAI.ipynb notebook.</p> <p>This notebook presents an end-to-end process of:</p> <ol> <li>Calculating the embeddings with OpenAI API.</li> <li>Storing the embeddings in a local instance of Qdrant to build a knowledge base.</li> <li>Converting raw text query to an embedding with OpenAI API.</li> <li>Using Qdrant to perform the nearest neighbour search in the created collection to find some context.</li> <li>Asking LLM to find the answer in a given context.</li> </ol> <p>All the steps will be simplified to calling some corresponding Langchain methods.</p>"},{"location":"examples/vector_databases/qdrant/QA_with_Langchain_Qdrant_and_OpenAI/#Prerequisites","title":"Prerequisites\u00b6","text":"<p>For the purposes of this exercise we need to prepare a couple of things:</p> <ol> <li>Qdrant server instance. In our case a local Docker container.</li> <li>The qdrant-client library to interact with the vector database.</li> <li>Langchain as a framework.</li> <li>An OpenAI API key.</li> </ol>"},{"location":"examples/vector_databases/qdrant/QA_with_Langchain_Qdrant_and_OpenAI/#Start-Qdrant-server","title":"Start Qdrant server\u00b6","text":"<p>We're going to use a local Qdrant instance running in a Docker container. The easiest way to launch it is to use the attached [docker-compose.yaml] file and run the following command:</p>"},{"location":"examples/vector_databases/qdrant/QA_with_Langchain_Qdrant_and_OpenAI/#Install-requirements","title":"Install requirements\u00b6","text":"<p>This notebook obviously requires the <code>openai</code>, <code>langchain</code> and <code>qdrant-client</code> packages.</p>"},{"location":"examples/vector_databases/qdrant/QA_with_Langchain_Qdrant_and_OpenAI/#Prepare-your-OpenAI-API-key","title":"Prepare your OpenAI API key\u00b6","text":"<p>The OpenAI API key is used for vectorization of the documents and queries.</p> <p>If you don't have an OpenAI API key, you can get one from https://beta.openai.com/account/api-keys.</p> <p>Once you get your key, please add it to your environment variables as <code>OPENAI_API_KEY</code>.</p>"},{"location":"examples/vector_databases/qdrant/QA_with_Langchain_Qdrant_and_OpenAI/#Load-data","title":"Load data\u00b6","text":"<p>In this section we are going to load the data containing some natural questions and answers to them. All the data will be used to create a Langchain application with Qdrant being the knowledge base.</p>"},{"location":"examples/vector_databases/qdrant/QA_with_Langchain_Qdrant_and_OpenAI/#Chain-definition","title":"Chain definition\u00b6","text":"<p>Langchain is already integrated with Qdrant and performs all the indexing for given list of documents. In our case we are going to store the set of answers we have.</p>"},{"location":"examples/vector_databases/qdrant/QA_with_Langchain_Qdrant_and_OpenAI/#Search-data","title":"Search data\u00b6","text":"<p>Once the data is put into Qdrant we can start asking some questions. A question will be automatically vectorized by OpenAI model, and the created vector will be used to find some possibly matching answers in Qdrant. Once retrieved, the most similar answers will be incorporated into the prompt sent to OpenAI Large Language Model. The communication between all the services is shown on a graph:</p> <p></p>"},{"location":"examples/vector_databases/qdrant/QA_with_Langchain_Qdrant_and_OpenAI/#Custom-prompt-templates","title":"Custom prompt templates\u00b6","text":"<p>The <code>stuff</code> chain type in Langchain uses a specific prompt with question and context documents incorporated. This is what the default prompt looks like:</p> <pre>Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n{context}\nQuestion: {question}\nHelpful Answer:\n</pre> <p>We can, however, provide our prompt template and change the behaviour of the OpenAI LLM, while still using the <code>stuff</code> chain type. It is important to keep <code>{context}</code> and <code>{question}</code> as placeholders.</p>"},{"location":"examples/vector_databases/qdrant/QA_with_Langchain_Qdrant_and_OpenAI/#Experimenting-with-custom-prompts","title":"Experimenting with custom prompts\u00b6","text":"<p>We can try using a different prompt template, so the model:</p> <ol> <li>Responds with a single-sentence answer if it knows it.</li> <li>Suggests a random song title if it doesn't know the answer to our question.</li> </ol>"},{"location":"examples/vector_databases/redis/","title":"Redis","text":"<p>Redis has first-class support for vector search through the RediSearch module. RediSearch is a Redis module that provides querying, secondary indexing, full-text search and vector search for Redis. To use RediSearch, you first declare indexes on your Redis data. You can then use the RediSearch query language to query that data.</p>"},{"location":"examples/vector_databases/redis/#features","title":"Features","text":"<p>RediSearch uses compressed, inverted indexes for fast indexing with a low memory footprint. RediSearch indexes enhance Redis by providing exact-phrase matching, fuzzy search, and numeric filtering, among many other features. Such as:</p> <ul> <li>Full-Text indexing of multiple fields in Redis hashes</li> <li>Incremental indexing without performance loss</li> <li>Vector similarity search</li> <li>Document ranking (using tf-idf, with optional user-provided weights)</li> <li>Field weighting</li> <li>Complex boolean queries with AND, OR, and NOT operators</li> <li>Prefix matching, fuzzy matching, and exact-phrase queries</li> <li>Support for double-metaphone phonetic matching</li> <li>Auto-complete suggestions (with fuzzy prefix suggestions)</li> <li>Stemming-based query expansion in many languages (using Snowball)</li> <li>Support for Chinese-language tokenization and querying (using Friso)</li> <li>Numeric filters and ranges</li> <li>Geospatial searches using Redis geospatial indexing</li> <li>A powerful aggregations engine</li> <li>Supports for all utf-8 encoded text</li> <li>Retrieve full documents, selected fields, or only the document IDs</li> <li>Sorting results (for example, by creation date)</li> <li>JSON support through RedisJSON</li> </ul>"},{"location":"examples/vector_databases/redis/#clients","title":"Clients","text":"<p>Given the large ecosystem around Redis, there are most likely client libraries in the language you need. You can use any standard Redis client library to run RediSearch commands, but it's easiest to use a library that wraps the RediSearch API. Below are a few examples, but you can find more client libraries here.</p> Project Language License Author Stars jedis Java MIT Redis redis-py Python MIT Redis node-redis Node.js MIT Redis nredisstack .NET MIT Redis redisearch-go Go BSD Redis redisearch-api-rs Rust BSD Redis"},{"location":"examples/vector_databases/redis/#deployment-options","title":"Deployment Options","text":"<p>There are many ways to deploy Redis with RediSearch. The easiest way to get started is to use Docker, but there are are many potential options for deployment such as</p> <ul> <li>Redis Cloud</li> <li>Cloud marketplaces: AWS Marketplace, Google Marketplace, or Azure Marketplace</li> <li>On-premise: Redis Enterprise Software</li> <li>Kubernetes: Redis Enterprise Software on Kubernetes</li> <li>Docker (RediSearch)</li> <li>Docker (Redis Stack)</li> </ul>"},{"location":"examples/vector_databases/redis/#cluster-support","title":"Cluster support","text":"<p>RediSearch has a distributed cluster version that scales to billions of documents across hundreds of servers. At the moment, distributed RediSearch is available as part of Redis Enterprise Cloud and Redis Enterprise Software.</p> <p>See RediSearch on Redis Enterprise for more information.</p>"},{"location":"examples/vector_databases/redis/#examples","title":"Examples","text":"<ul> <li>Product Search - eCommerce product search (with image and text)</li> <li>Product Recommendations with DocArray / Jina - Content-based product recommendations example with Redis and DocArray.</li> <li>Redis VSS in RecSys - 3 end-to-end Redis &amp; NVIDIA Merlin Recommendation System Architectures.</li> <li>Azure OpenAI Embeddings Q&amp;A - OpenAI and Redis as a Q&amp;A service on Azure.</li> <li>ArXiv Paper Search - Semantic search over arXiv scholarly papers</li> </ul>"},{"location":"examples/vector_databases/redis/#more-resources","title":"More Resources","text":"<p>For more information on how to use Redis as a vector database, check out the following resources:</p> <ul> <li>Redis Vector Similarity Docs - Redis official docs for Vector Search.</li> <li>Redis-py Search Docs - Redis-py client library docs for RediSearch.</li> <li>Vector Similarity Search: From Basics to Production - Introductory blog post to VSS and Redis as a VectorDB.</li> <li>AI-Powered Document Search - Blog post covering AI Powered Document Search Use Cases &amp; Architectures.</li> <li>Vector Database Benchmarks - Jina AI VectorDB benchmarks comparing Redis against others.</li> </ul>"},{"location":"examples/vector_databases/redis/getting-started-with-redis-and-openai/","title":"Using Redis as a Vector Database with OpenAI","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install redis wget pandas openai\n</pre> !pip install redis wget pandas openai <p>===========================================================</p> In\u00a0[2]: Copied! <pre># Test that your OpenAI API key is correctly set as an environment variable\n# Note. if you run this notebook locally, you will need to reload your terminal and the notebook for the env variables to be live.\nimport os\nimport openai\n\n# Note. alternatively you can set a temporary env variable like this:\n# os.environ[\"OPENAI_API_KEY\"] = 'sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'\n\nif os.getenv(\"OPENAI_API_KEY\") is not None:\n    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n    print (\"OPENAI_API_KEY is ready\")\nelse:\n    print (\"OPENAI_API_KEY environment variable not found\")\n</pre> # Test that your OpenAI API key is correctly set as an environment variable # Note. if you run this notebook locally, you will need to reload your terminal and the notebook for the env variables to be live. import os import openai  # Note. alternatively you can set a temporary env variable like this: # os.environ[\"OPENAI_API_KEY\"] = 'sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'  if os.getenv(\"OPENAI_API_KEY\") is not None:     openai.api_key = os.getenv(\"OPENAI_API_KEY\")     print (\"OPENAI_API_KEY is ready\") else:     print (\"OPENAI_API_KEY environment variable not found\") <pre>OPENAI_API_KEY is ready\n</pre> In\u00a0[3]: Copied! <pre>import sys\nimport numpy as np\nimport pandas as pd\nfrom typing import List\n\n# use helper function in nbutils.py to download and read the data\n# this should take from 5-10 min to run\nif os.getcwd() not in sys.path:\n    sys.path.append(os.getcwd())\nimport nbutils\n\nnbutils.download_wikipedia_data()\ndata = nbutils.read_wikipedia_data()\n\ndata.head()\n</pre> import sys import numpy as np import pandas as pd from typing import List  # use helper function in nbutils.py to download and read the data # this should take from 5-10 min to run if os.getcwd() not in sys.path:     sys.path.append(os.getcwd()) import nbutils  nbutils.download_wikipedia_data() data = nbutils.read_wikipedia_data()  data.head() <pre>File Downloaded\n</pre> Out[3]: id url title text title_vector content_vector vector_id 0 1 https://simple.wikipedia.org/wiki/April April April is the fourth month of the year in the J... [0.001009464613161981, -0.020700545981526375, ... [-0.011253940872848034, -0.013491976074874401,... 0 1 2 https://simple.wikipedia.org/wiki/August August August (Aug.) is the eighth month of the year ... [0.0009286514250561595, 0.000820168002974242, ... [0.0003609954728744924, 0.007262262050062418, ... 1 2 6 https://simple.wikipedia.org/wiki/Art Art Art is a creative activity that expresses imag... [0.003393713850528002, 0.0061537534929811954, ... [-0.004959689453244209, 0.015772193670272827, ... 2 3 8 https://simple.wikipedia.org/wiki/A A A or a is the first letter of the English alph... [0.0153952119871974, -0.013759135268628597, 0.... [0.024894846603274345, -0.022186409682035446, ... 3 4 9 https://simple.wikipedia.org/wiki/Air Air Air refers to the Earth's atmosphere. Air is a... [0.02224554680287838, -0.02044147066771984, -0... [0.021524671465158463, 0.018522677943110466, -... 4 In\u00a0[4]: Copied! <pre>import redis\nfrom redis.commands.search.indexDefinition import (\n    IndexDefinition,\n    IndexType\n)\nfrom redis.commands.search.query import Query\nfrom redis.commands.search.field import (\n    TextField,\n    VectorField\n)\n\nREDIS_HOST =  \"localhost\"\nREDIS_PORT = 6379\nREDIS_PASSWORD = \"\" # default for passwordless Redis\n\n# Connect to Redis\nredis_client = redis.Redis(\n    host=REDIS_HOST,\n    port=REDIS_PORT,\n    password=REDIS_PASSWORD\n)\nredis_client.ping()\n</pre> import redis from redis.commands.search.indexDefinition import (     IndexDefinition,     IndexType ) from redis.commands.search.query import Query from redis.commands.search.field import (     TextField,     VectorField )  REDIS_HOST =  \"localhost\" REDIS_PORT = 6379 REDIS_PASSWORD = \"\" # default for passwordless Redis  # Connect to Redis redis_client = redis.Redis(     host=REDIS_HOST,     port=REDIS_PORT,     password=REDIS_PASSWORD ) redis_client.ping() Out[4]: <pre>True</pre> In\u00a0[5]: Copied! <pre># Constants\nVECTOR_DIM = len(data['title_vector'][0]) # length of the vectors\nVECTOR_NUMBER = len(data)                 # initial number of vectors\nINDEX_NAME = \"embeddings-index\"           # name of the search index\nPREFIX = \"doc\"                            # prefix for the document keys\nDISTANCE_METRIC = \"COSINE\"                # distance metric for the vectors (ex. COSINE, IP, L2)\n</pre> # Constants VECTOR_DIM = len(data['title_vector'][0]) # length of the vectors VECTOR_NUMBER = len(data)                 # initial number of vectors INDEX_NAME = \"embeddings-index\"           # name of the search index PREFIX = \"doc\"                            # prefix for the document keys DISTANCE_METRIC = \"COSINE\"                # distance metric for the vectors (ex. COSINE, IP, L2) In\u00a0[6]: Copied! <pre># Define RediSearch fields for each of the columns in the dataset\ntitle = TextField(name=\"title\")\nurl = TextField(name=\"url\")\ntext = TextField(name=\"text\")\ntitle_embedding = VectorField(\"title_vector\",\n    \"FLAT\", {\n        \"TYPE\": \"FLOAT32\",\n        \"DIM\": VECTOR_DIM,\n        \"DISTANCE_METRIC\": DISTANCE_METRIC,\n        \"INITIAL_CAP\": VECTOR_NUMBER,\n    }\n)\ntext_embedding = VectorField(\"content_vector\",\n    \"FLAT\", {\n        \"TYPE\": \"FLOAT32\",\n        \"DIM\": VECTOR_DIM,\n        \"DISTANCE_METRIC\": DISTANCE_METRIC,\n        \"INITIAL_CAP\": VECTOR_NUMBER,\n    }\n)\nfields = [title, url, text, title_embedding, text_embedding]\n</pre> # Define RediSearch fields for each of the columns in the dataset title = TextField(name=\"title\") url = TextField(name=\"url\") text = TextField(name=\"text\") title_embedding = VectorField(\"title_vector\",     \"FLAT\", {         \"TYPE\": \"FLOAT32\",         \"DIM\": VECTOR_DIM,         \"DISTANCE_METRIC\": DISTANCE_METRIC,         \"INITIAL_CAP\": VECTOR_NUMBER,     } ) text_embedding = VectorField(\"content_vector\",     \"FLAT\", {         \"TYPE\": \"FLOAT32\",         \"DIM\": VECTOR_DIM,         \"DISTANCE_METRIC\": DISTANCE_METRIC,         \"INITIAL_CAP\": VECTOR_NUMBER,     } ) fields = [title, url, text, title_embedding, text_embedding] In\u00a0[7]: Copied! <pre># Check if index exists\ntry:\n    redis_client.ft(INDEX_NAME).info()\n    print(\"Index already exists\")\nexcept:\n    # Create RediSearch Index\n    redis_client.ft(INDEX_NAME).create_index(\n        fields = fields,\n        definition = IndexDefinition(prefix=[PREFIX], index_type=IndexType.HASH)\n)\n</pre> # Check if index exists try:     redis_client.ft(INDEX_NAME).info()     print(\"Index already exists\") except:     # Create RediSearch Index     redis_client.ft(INDEX_NAME).create_index(         fields = fields,         definition = IndexDefinition(prefix=[PREFIX], index_type=IndexType.HASH) ) In\u00a0[8]: Copied! <pre>def index_documents(client: redis.Redis, prefix: str, documents: pd.DataFrame):\n    records = documents.to_dict(\"records\")\n    for doc in records:\n        key = f\"{prefix}:{str(doc['id'])}\"\n\n        # create byte vectors for title and content\n        title_embedding = np.array(doc[\"title_vector\"], dtype=np.float32).tobytes()\n        content_embedding = np.array(doc[\"content_vector\"], dtype=np.float32).tobytes()\n\n        # replace list of floats with byte vectors\n        doc[\"title_vector\"] = title_embedding\n        doc[\"content_vector\"] = content_embedding\n\n        client.hset(key, mapping = doc)\n</pre> def index_documents(client: redis.Redis, prefix: str, documents: pd.DataFrame):     records = documents.to_dict(\"records\")     for doc in records:         key = f\"{prefix}:{str(doc['id'])}\"          # create byte vectors for title and content         title_embedding = np.array(doc[\"title_vector\"], dtype=np.float32).tobytes()         content_embedding = np.array(doc[\"content_vector\"], dtype=np.float32).tobytes()          # replace list of floats with byte vectors         doc[\"title_vector\"] = title_embedding         doc[\"content_vector\"] = content_embedding          client.hset(key, mapping = doc) In\u00a0[9]: Copied! <pre>index_documents(redis_client, PREFIX, data)\nprint(f\"Loaded {redis_client.info()['db0']['keys']} documents in Redis search index with name: {INDEX_NAME}\")\n</pre> index_documents(redis_client, PREFIX, data) print(f\"Loaded {redis_client.info()['db0']['keys']} documents in Redis search index with name: {INDEX_NAME}\") <pre>Loaded 25000 documents in Redis search index with name: embeddings-index\n</pre> In\u00a0[10]: Copied! <pre>def search_redis(\n    redis_client: redis.Redis,\n    user_query: str,\n    index_name: str = \"embeddings-index\",\n    vector_field: str = \"title_vector\",\n    return_fields: list = [\"title\", \"url\", \"text\", \"vector_score\"],\n    hybrid_fields = \"*\",\n    k: int = 20,\n    print_results: bool = True,\n) -&gt; List[dict]:\n\n    # Creates embedding vector from user query\n    embedded_query = openai.Embedding.create(input=user_query,\n                                            model=\"text-embedding-ada-002\",\n                                            )[\"data\"][0]['embedding']\n\n    # Prepare the Query\n    base_query = f'{hybrid_fields}=&gt;[KNN {k} @{vector_field} $vector AS vector_score]'\n    query = (\n        Query(base_query)\n         .return_fields(*return_fields)\n         .sort_by(\"vector_score\")\n         .paging(0, k)\n         .dialect(2)\n    )\n    params_dict = {\"vector\": np.array(embedded_query).astype(dtype=np.float32).tobytes()}\n\n    # perform vector search\n    results = redis_client.ft(index_name).search(query, params_dict)\n    if print_results:\n        for i, article in enumerate(results.docs):\n            score = 1 - float(article.vector_score)\n            print(f\"{i}. {article.title} (Score: {round(score ,3) })\")\n    return results.docs\n</pre> def search_redis(     redis_client: redis.Redis,     user_query: str,     index_name: str = \"embeddings-index\",     vector_field: str = \"title_vector\",     return_fields: list = [\"title\", \"url\", \"text\", \"vector_score\"],     hybrid_fields = \"*\",     k: int = 20,     print_results: bool = True, ) -&gt; List[dict]:      # Creates embedding vector from user query     embedded_query = openai.Embedding.create(input=user_query,                                             model=\"text-embedding-ada-002\",                                             )[\"data\"][0]['embedding']      # Prepare the Query     base_query = f'{hybrid_fields}=&gt;[KNN {k} @{vector_field} $vector AS vector_score]'     query = (         Query(base_query)          .return_fields(*return_fields)          .sort_by(\"vector_score\")          .paging(0, k)          .dialect(2)     )     params_dict = {\"vector\": np.array(embedded_query).astype(dtype=np.float32).tobytes()}      # perform vector search     results = redis_client.ft(index_name).search(query, params_dict)     if print_results:         for i, article in enumerate(results.docs):             score = 1 - float(article.vector_score)             print(f\"{i}. {article.title} (Score: {round(score ,3) })\")     return results.docs In\u00a0[11]: Copied! <pre># For using OpenAI to generate query embedding\nresults = search_redis(redis_client, 'modern art in Europe', k=10)\n</pre> # For using OpenAI to generate query embedding results = search_redis(redis_client, 'modern art in Europe', k=10) <pre>0. Museum of Modern Art (Score: 0.875)\n1. Western Europe (Score: 0.868)\n2. Renaissance art (Score: 0.864)\n3. Pop art (Score: 0.86)\n4. Northern Europe (Score: 0.855)\n5. Hellenistic art (Score: 0.853)\n6. Modernist literature (Score: 0.847)\n7. Art film (Score: 0.843)\n8. Central Europe (Score: 0.843)\n9. European (Score: 0.841)\n</pre> In\u00a0[12]: Copied! <pre>results = search_redis(redis_client, 'Famous battles in Scottish history', vector_field='content_vector', k=10)\n</pre> results = search_redis(redis_client, 'Famous battles in Scottish history', vector_field='content_vector', k=10) <pre>0. Battle of Bannockburn (Score: 0.869)\n1. Wars of Scottish Independence (Score: 0.861)\n2. 1651 (Score: 0.853)\n3. First War of Scottish Independence (Score: 0.85)\n4. Robert I of Scotland (Score: 0.846)\n5. 841 (Score: 0.844)\n6. 1716 (Score: 0.844)\n7. 1314 (Score: 0.837)\n8. 1263 (Score: 0.836)\n9. William Wallace (Score: 0.835)\n</pre> In\u00a0[13]: Copied! <pre>def create_hybrid_field(field_name: str, value: str) -&gt; str:\n    return f'@{field_name}:\"{value}\"'\n\n# search the content vector for articles about famous battles in Scottish history and only include results with Scottish in the title\nresults = search_redis(redis_client,\n                       \"Famous battles in Scottish history\",\n                       vector_field=\"title_vector\",\n                       k=5,\n                       hybrid_fields=create_hybrid_field(\"title\", \"Scottish\")\n                       )\n</pre> def create_hybrid_field(field_name: str, value: str) -&gt; str:     return f'@{field_name}:\"{value}\"'  # search the content vector for articles about famous battles in Scottish history and only include results with Scottish in the title results = search_redis(redis_client,                        \"Famous battles in Scottish history\",                        vector_field=\"title_vector\",                        k=5,                        hybrid_fields=create_hybrid_field(\"title\", \"Scottish\")                        ) <pre>0. First War of Scottish Independence (Score: 0.892)\n1. Wars of Scottish Independence (Score: 0.889)\n2. Second War of Scottish Independence (Score: 0.879)\n3. List of Scottish monarchs (Score: 0.873)\n4. Scottish Borders (Score: 0.863)\n</pre> In\u00a0[14]: Copied! <pre># run a hybrid query for articles about Art in the title vector and only include results with the phrase \"Leonardo da Vinci\" in the text\nresults = search_redis(redis_client,\n                       \"Art\",\n                       vector_field=\"title_vector\",\n                       k=5,\n                       hybrid_fields=create_hybrid_field(\"text\", \"Leonardo da Vinci\")\n                       )\n\n# find specific mention of Leonardo da Vinci in the text that our full-text-search query returned\nmention = [sentence for sentence in results[0].text.split(\"\\n\") if \"Leonardo da Vinci\" in sentence][0]\nmention\n</pre> # run a hybrid query for articles about Art in the title vector and only include results with the phrase \"Leonardo da Vinci\" in the text results = search_redis(redis_client,                        \"Art\",                        vector_field=\"title_vector\",                        k=5,                        hybrid_fields=create_hybrid_field(\"text\", \"Leonardo da Vinci\")                        )  # find specific mention of Leonardo da Vinci in the text that our full-text-search query returned mention = [sentence for sentence in results[0].text.split(\"\\n\") if \"Leonardo da Vinci\" in sentence][0] mention <pre>0. Art (Score: 1.0)\n1. Paint (Score: 0.896)\n2. Renaissance art (Score: 0.88)\n3. Painting (Score: 0.874)\n4. Renaissance (Score: 0.846)\n</pre> Out[14]: <pre>'In Europe, after the Middle Ages, there was a \"Renaissance\" which means \"rebirth\". People rediscovered science and artists were allowed to paint subjects other than religious subjects. People like Michelangelo and Leonardo da Vinci still painted religious pictures, but they also now could paint mythological pictures too. These artists also invented perspective where things in the distance look smaller in the picture. This was new because in the Middle Ages people would paint all the figures close up and just overlapping each other. These artists used nudity regularly in their art.'</pre> In\u00a0[15]: Copied! <pre># re-define RediSearch vector fields to use HNSW index\ntitle_embedding = VectorField(\"title_vector\",\n    \"HNSW\", {\n        \"TYPE\": \"FLOAT32\",\n        \"DIM\": VECTOR_DIM,\n        \"DISTANCE_METRIC\": DISTANCE_METRIC,\n        \"INITIAL_CAP\": VECTOR_NUMBER\n    }\n)\ntext_embedding = VectorField(\"content_vector\",\n    \"HNSW\", {\n        \"TYPE\": \"FLOAT32\",\n        \"DIM\": VECTOR_DIM,\n        \"DISTANCE_METRIC\": DISTANCE_METRIC,\n        \"INITIAL_CAP\": VECTOR_NUMBER\n    }\n)\nfields = [title, url, text, title_embedding, text_embedding]\n</pre> # re-define RediSearch vector fields to use HNSW index title_embedding = VectorField(\"title_vector\",     \"HNSW\", {         \"TYPE\": \"FLOAT32\",         \"DIM\": VECTOR_DIM,         \"DISTANCE_METRIC\": DISTANCE_METRIC,         \"INITIAL_CAP\": VECTOR_NUMBER     } ) text_embedding = VectorField(\"content_vector\",     \"HNSW\", {         \"TYPE\": \"FLOAT32\",         \"DIM\": VECTOR_DIM,         \"DISTANCE_METRIC\": DISTANCE_METRIC,         \"INITIAL_CAP\": VECTOR_NUMBER     } ) fields = [title, url, text, title_embedding, text_embedding] In\u00a0[16]: Copied! <pre>import time\n# Check if index exists\nHNSW_INDEX_NAME = INDEX_NAME+ \"_HNSW\"\n\ntry:\n    redis_client.ft(HNSW_INDEX_NAME).info()\n    print(\"Index already exists\")\nexcept:\n    # Create RediSearch Index\n    redis_client.ft(HNSW_INDEX_NAME).create_index(\n        fields = fields,\n        definition = IndexDefinition(prefix=[PREFIX], index_type=IndexType.HASH)\n    )\n\n# since RediSearch creates the index in the background for existing documents, we will wait until\n# indexing is complete before running our queries. Although this is not necessary for the first query,\n# some queries may take longer to run if the index is not fully built. In general, Redis will perform\n# best when adding new documents to existing indices rather than new indices on existing documents.\nwhile redis_client.ft(HNSW_INDEX_NAME).info()[\"indexing\"] == \"1\":\n    time.sleep(5)\n</pre> import time # Check if index exists HNSW_INDEX_NAME = INDEX_NAME+ \"_HNSW\"  try:     redis_client.ft(HNSW_INDEX_NAME).info()     print(\"Index already exists\") except:     # Create RediSearch Index     redis_client.ft(HNSW_INDEX_NAME).create_index(         fields = fields,         definition = IndexDefinition(prefix=[PREFIX], index_type=IndexType.HASH)     )  # since RediSearch creates the index in the background for existing documents, we will wait until # indexing is complete before running our queries. Although this is not necessary for the first query, # some queries may take longer to run if the index is not fully built. In general, Redis will perform # best when adding new documents to existing indices rather than new indices on existing documents. while redis_client.ft(HNSW_INDEX_NAME).info()[\"indexing\"] == \"1\":     time.sleep(5) In\u00a0[17]: Copied! <pre>results = search_redis(redis_client, 'modern art in Europe', index_name=HNSW_INDEX_NAME, k=10)\n</pre> results = search_redis(redis_client, 'modern art in Europe', index_name=HNSW_INDEX_NAME, k=10) <pre>0. Western Europe (Score: 0.868)\n1. Northern Europe (Score: 0.855)\n2. Central Europe (Score: 0.843)\n3. European (Score: 0.841)\n4. Eastern Europe (Score: 0.839)\n5. Europe (Score: 0.839)\n6. Western European Union (Score: 0.837)\n7. Southern Europe (Score: 0.831)\n8. Western civilization (Score: 0.83)\n9. Council of Europe (Score: 0.827)\n</pre> In\u00a0[18]: Copied! <pre># compare the results of the HNSW index to the FLAT index and time both queries\ndef time_queries(iterations: int = 10):\n    print(\" ----- Flat Index ----- \")\n    t0 = time.time()\n    for i in range(iterations):\n        results_flat = search_redis(redis_client, 'modern art in Europe', k=10, print_results=False)\n    t0 = (time.time() - t0) / iterations\n    results_flat = search_redis(redis_client, 'modern art in Europe', k=10, print_results=True)\n    print(f\"Flat index query time: {round(t0, 3)} seconds\\n\")\n    time.sleep(1)\n    print(\" ----- HNSW Index ------ \")\n    t1 = time.time()\n    for i in range(iterations):\n        results_hnsw = search_redis(redis_client, 'modern art in Europe', index_name=HNSW_INDEX_NAME, k=10, print_results=False)\n    t1 = (time.time() - t1) / iterations\n    results_hnsw = search_redis(redis_client, 'modern art in Europe', index_name=HNSW_INDEX_NAME, k=10, print_results=True)\n    print(f\"HNSW index query time: {round(t1, 3)} seconds\")\n    print(\" ------------------------ \")\ntime_queries()\n</pre> # compare the results of the HNSW index to the FLAT index and time both queries def time_queries(iterations: int = 10):     print(\" ----- Flat Index ----- \")     t0 = time.time()     for i in range(iterations):         results_flat = search_redis(redis_client, 'modern art in Europe', k=10, print_results=False)     t0 = (time.time() - t0) / iterations     results_flat = search_redis(redis_client, 'modern art in Europe', k=10, print_results=True)     print(f\"Flat index query time: {round(t0, 3)} seconds\\n\")     time.sleep(1)     print(\" ----- HNSW Index ------ \")     t1 = time.time()     for i in range(iterations):         results_hnsw = search_redis(redis_client, 'modern art in Europe', index_name=HNSW_INDEX_NAME, k=10, print_results=False)     t1 = (time.time() - t1) / iterations     results_hnsw = search_redis(redis_client, 'modern art in Europe', index_name=HNSW_INDEX_NAME, k=10, print_results=True)     print(f\"HNSW index query time: {round(t1, 3)} seconds\")     print(\" ------------------------ \") time_queries() <pre> ----- Flat Index ----- \n0. Museum of Modern Art (Score: 0.875)\n1. Western Europe (Score: 0.867)\n2. Renaissance art (Score: 0.864)\n3. Pop art (Score: 0.861)\n4. Northern Europe (Score: 0.855)\n5. Hellenistic art (Score: 0.853)\n6. Modernist literature (Score: 0.847)\n7. Art film (Score: 0.843)\n8. Central Europe (Score: 0.843)\n9. Art (Score: 0.842)\nFlat index query time: 0.263 seconds\n\n ----- HNSW Index ------ \n0. Western Europe (Score: 0.867)\n1. Northern Europe (Score: 0.855)\n2. Central Europe (Score: 0.843)\n3. European (Score: 0.841)\n4. Eastern Europe (Score: 0.839)\n5. Europe (Score: 0.839)\n6. Western European Union (Score: 0.837)\n7. Southern Europe (Score: 0.831)\n8. Western civilization (Score: 0.83)\n9. Council of Europe (Score: 0.827)\nHNSW index query time: 0.129 seconds\n ------------------------ \n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/vector_databases/redis/getting-started-with-redis-and-openai/#Using-Redis-as-a-Vector-Database-with-OpenAI","title":"Using Redis as a Vector Database with OpenAI\u00b6","text":"<p>This notebook provides an introduction to using Redis as a vector database with OpenAI embeddings. Redis is a scalable, real-time database that can be used as a vector database when using the RediSearch Module. The RediSearch module allows you to index and search for vectors in Redis. This notebook will show you how to use the RediSearch module to index and search for vectors created by using the OpenAI API and stored in Redis.</p>"},{"location":"examples/vector_databases/redis/getting-started-with-redis-and-openai/#What-is-Redis?","title":"What is Redis?\u00b6","text":"<p>Most developers from a web services background are probably familiar with Redis. At it's core, Redis is an open-source key-value store that can be used as a cache, message broker, and database. Developers choice Redis because it is fast, has a large ecosystem of client libraries, and has been deployed by major enterprises for years.</p> <p>In addition to the traditional uses of Redis. Redis also provides Redis Modules which are a way to extend Redis with new data types and commands. Example modules include RedisJSON, RedisTimeSeries, RedisBloom and RediSearch.</p>"},{"location":"examples/vector_databases/redis/getting-started-with-redis-and-openai/#What-is-RediSearch?","title":"What is RediSearch?\u00b6","text":"<p>RediSearch is a Redis module that provides querying, secondary indexing, full-text search and vector search for Redis. To use RediSearch, you first declare indexes on your Redis data. You can then use the RediSearch clients to query that data. For more information on the feature set of RediSearch, see the README or the RediSearch documentation.</p>"},{"location":"examples/vector_databases/redis/getting-started-with-redis-and-openai/#Deployment-options","title":"Deployment options\u00b6","text":"<p>There are a number of ways to deploy Redis. For local development, the quickest method is to use the Redis Stack docker container which we will use here. Redis Stack contains a number of Redis modules that can be used together to create a fast, multi-model data store and query engine.</p> <p>For production use cases, The easiest way to get started is to use the Redis Cloud service. Redis Cloud is a fully managed Redis service. You can also deploy Redis on your own infrastructure using Redis Enterprise. Redis Enterprise is a fully managed Redis service that can be deployed in kubernetes, on-premises or in the cloud.</p> <p>Additionally, every major cloud provider (AWS Marketplace, Google Marketplace, or Azure Marketplace) offers Redis Enterprise in a marketplace offering.</p>"},{"location":"examples/vector_databases/redis/getting-started-with-redis-and-openai/#Prerequisites","title":"Prerequisites\u00b6","text":"<p>Before we start this project, we need setup the following:</p> <ul> <li>start a Redis database with RediSearch (redis-stack)</li> <li>install libraries<ul> <li>Redis-py</li> </ul> </li> <li>get your OpenAI API key</li> </ul> <p>===========================================================</p>"},{"location":"examples/vector_databases/redis/getting-started-with-redis-and-openai/#Start-Redis","title":"Start Redis\u00b6","text":"<p>To keep this example simple, we will use the Redis Stack docker container which we can start as follows</p> <pre>$ docker compose up -d\n</pre> <p>This also includes the RedisInsight GUI for managing your Redis database which you can view at http://localhost:8001 once you start the docker container.</p> <p>You're all set up and ready to go! Next, we import and create our client for communicating with the Redis database we just created.</p>"},{"location":"examples/vector_databases/redis/getting-started-with-redis-and-openai/#Install-Requirements","title":"Install Requirements\u00b6","text":"<p>Redis-Py is the python client for communicating with Redis. We will use this to communicate with our Redis-stack database.</p>"},{"location":"examples/vector_databases/redis/getting-started-with-redis-and-openai/#Prepare-your-OpenAI-API-key","title":"Prepare your OpenAI API key\u00b6","text":"<p>The <code>OpenAI API key</code> is used for vectorization of query data.</p> <p>If you don't have an OpenAI API key, you can get one from https://beta.openai.com/account/api-keys.</p> <p>Once you get your key, please add it to your environment variables as <code>OPENAI_API_KEY</code>.</p>"},{"location":"examples/vector_databases/redis/getting-started-with-redis-and-openai/#Load-data","title":"Load data\u00b6","text":"<p>In this section we'll load embedded data that has already been converted into vectors. We'll use this data to create an index in Redis and then search for similar vectors.</p>"},{"location":"examples/vector_databases/redis/getting-started-with-redis-and-openai/#Connect-to-Redis","title":"Connect to Redis\u00b6","text":"<p>Now that we have our Redis database running, we can connect to it using the Redis-py client. We will use the default host and port for the Redis database which is <code>localhost:6379</code>.</p>"},{"location":"examples/vector_databases/redis/getting-started-with-redis-and-openai/#Creating-a-Search-Index-in-Redis","title":"Creating a Search Index in Redis\u00b6","text":"<p>The below cells will show how to specify and create a search index in Redis. We will</p> <ol> <li>Set some constants for defining our index like the distance metric and the index name</li> <li>Define the index schema with RediSearch fields</li> <li>Create the index</li> </ol>"},{"location":"examples/vector_databases/redis/getting-started-with-redis-and-openai/#Load-Documents-into-the-Index","title":"Load Documents into the Index\u00b6","text":"<p>Now that we have a search index, we can load documents into it. We will use the same documents we used in the previous examples. In Redis, either the Hash or JSON (if using RedisJSON in addition to RediSearch) data types can be used to store documents. We will use the HASH data type in this example. The below cells will show how to load documents into the index.</p>"},{"location":"examples/vector_databases/redis/getting-started-with-redis-and-openai/#Simple-Vector-Search-Queries-with-OpenAI-Query-Embeddings","title":"Simple Vector Search Queries with OpenAI Query Embeddings\u00b6","text":"<p>Now that we have a search index and documents loaded into it, we can run search queries. Below we will provide a function that will run a search query and return the results. Using this function we run a few queries that will show how you can utilize Redis as a vector database.</p>"},{"location":"examples/vector_databases/redis/getting-started-with-redis-and-openai/#Hybrid-Queries-with-Redis","title":"Hybrid Queries with Redis\u00b6","text":"<p>The previous examples showed how run vector search queries with RediSearch. In this section, we will show how to combine vector search with other RediSearch fields for hybrid search. In the below example, we will combine vector search with full text search.</p>"},{"location":"examples/vector_databases/redis/getting-started-with-redis-and-openai/#HNSW-Index","title":"HNSW Index\u00b6","text":"<p>Up until now, we've been using the <code>FLAT</code> or \"brute-force\" index to run our queries. Redis also supports the <code>HNSW</code> index which is a fast, approximate index. The <code>HNSW</code> index is a graph-based index that uses a hierarchical navigable small world graph to store vectors. The <code>HNSW</code> index is a good choice for large datasets where you want to run approximate queries.</p> <p><code>HNSW</code> will take longer to build and consume more memory for most cases than <code>FLAT</code> but will be faster to run queries on, especially for large datasets.</p> <p>The following cells will show how to create an <code>HNSW</code> index and run queries with it using the same data as before.</p>"},{"location":"examples/vector_databases/redis/nbutils/","title":"Nbutils","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport wget\nimport zipfile\nimport numpy as np\nimport pandas as pd\nfrom ast import literal_eval\n</pre> import os import wget import zipfile import numpy as np import pandas as pd from ast import literal_eval In\u00a0[\u00a0]: Copied! <pre>def download_wikipedia_data(\n    data_path: str = '../../data/',\n    download_path: str = \"./\",\n    file_name: str = \"vector_database_wikipedia_articles_embedded\") -&gt; pd.DataFrame:\n\n    data_url = 'https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip'\n\n    csv_file_path = os.path.join(data_path, file_name + \".csv\")\n    zip_file_path = os.path.join(download_path, file_name + \".zip\")\n    if os.path.isfile(csv_file_path):\n        print(\"File Downloaded\")\n    else:\n        if os.path.isfile(zip_file_path):\n            print(\"Zip downloaded but not unzipped, unzipping now...\")\n        else:\n            print(\"File not found, downloading now...\")\n            # Download the data\n            wget.download(data_url, out=download_path, bar=True)\n\n        # Unzip the data\n        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n            zip_ref.extractall(data_path)\n\n        # Remove the zip file\n        os.remove('vector_database_wikipedia_articles_embedded.zip')\n        print(f\"File downloaded to {data_path}\")\n</pre> def download_wikipedia_data(     data_path: str = '../../data/',     download_path: str = \"./\",     file_name: str = \"vector_database_wikipedia_articles_embedded\") -&gt; pd.DataFrame:      data_url = 'https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip'      csv_file_path = os.path.join(data_path, file_name + \".csv\")     zip_file_path = os.path.join(download_path, file_name + \".zip\")     if os.path.isfile(csv_file_path):         print(\"File Downloaded\")     else:         if os.path.isfile(zip_file_path):             print(\"Zip downloaded but not unzipped, unzipping now...\")         else:             print(\"File not found, downloading now...\")             # Download the data             wget.download(data_url, out=download_path, bar=True)          # Unzip the data         with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:             zip_ref.extractall(data_path)          # Remove the zip file         os.remove('vector_database_wikipedia_articles_embedded.zip')         print(f\"File downloaded to {data_path}\") In\u00a0[\u00a0]: Copied! <pre>def read_wikipedia_data(data_path: str = '../../data/', file_name: str = \"vector_database_wikipedia_articles_embedded\") -&gt; pd.DataFrame:\n\n    csv_file_path = os.path.join(data_path, file_name + \".csv\")\n    data = pd.read_csv(csv_file_path)\n    # Read vectors from strings back into a list\n    data['title_vector'] = data.title_vector.apply(literal_eval)\n    data['content_vector'] = data.content_vector.apply(literal_eval)\n    # Set vector_id to be a string\n    data['vector_id'] = data['vector_id'].apply(str)\n    return data\n</pre> def read_wikipedia_data(data_path: str = '../../data/', file_name: str = \"vector_database_wikipedia_articles_embedded\") -&gt; pd.DataFrame:      csv_file_path = os.path.join(data_path, file_name + \".csv\")     data = pd.read_csv(csv_file_path)     # Read vectors from strings back into a list     data['title_vector'] = data.title_vector.apply(literal_eval)     data['content_vector'] = data.content_vector.apply(literal_eval)     # Set vector_id to be a string     data['vector_id'] = data['vector_id'].apply(str)     return data"},{"location":"examples/vector_databases/weaviate/","title":"Weaviate &lt;&gt; OpenAI","text":"<p>\u200bWeaviate is an open-source vector search engine (docs - Github) that can store and search through OpenAI embeddings and data objects. The database allows you to do similarity search, hybrid search (the combining of multiple search techniques, such as keyword-based and vector search), and generative search (like Q&amp;A). Weaviate also supports a wide variety of OpenAI-based modules (e.g., <code>text2vec-openai</code>, <code>qna-openai</code>), allowing you to vectorize and query data fast and efficiently.</p> <p>You can run Weaviate (including the OpenAI modules if desired) in three ways:</p> <ol> <li>Open source inside a Docker-container (example)</li> <li>Using the Weaviate Cloud Service (get started)</li> <li>In a Kubernetes cluster (learn more)</li> </ol>"},{"location":"examples/vector_databases/weaviate/#examples","title":"Examples","text":"<p>This folder contains a variety of Weaviate and OpenAI examples. </p> Name Description lanugage Google Colab Getting Started with Weaviate and OpenAI A simple getting started for semantic vector search using the OpenAI vectorization module in Weaviate (<code>text2vec-openai</code>) Python Notebook link Hybrid Search with Weaviate and OpenAI A simple getting started for hybrid search using the OpenAI vectorization module in Weaviate (<code>text2vec-openai</code>) Python Notebook link Question Answering with Weaviate and OpenAI A simple getting started for question answering (Q&amp;A) using the OpenAI Q&amp;A module in Weaviate (<code>qna-openai</code>) Python Notebook link Docker-compose example A Docker-compose file with all OpenAI modules enabled Docker"},{"location":"examples/vector_databases/weaviate/getting-started-with-weaviate-and-openai/","title":"Using Weaviate with OpenAI vectorize module for Embeddings Search","text":"<p>===========================================================</p> In\u00a0[\u00a0]: Copied! <pre># Install the Weaviate client for Python\n!pip install weaviate-client&gt;=3.11.0\n\n# Install datasets and apache-beam to load the sample datasets\n!pip install datasets apache-beam\n</pre> # Install the Weaviate client for Python !pip install weaviate-client&gt;=3.11.0  # Install datasets and apache-beam to load the sample datasets !pip install datasets apache-beam <p>===========================================================</p> In\u00a0[\u00a0]: Copied! <pre># Test that your OpenAI API key is correctly set as an environment variable\n# Note. if you run this notebook locally, you will need to reload your terminal and the notebook for the env variables to be live.\nimport os\n\n# Note. alternatively you can set a temporary env variable like this:\n# os.environ[\"OPENAI_API_KEY\"] = 'your-key-goes-here'\n\nif os.getenv(\"OPENAI_API_KEY\") is not None:\n    print (\"OPENAI_API_KEY is ready\")\nelse:\n    print (\"OPENAI_API_KEY environment variable not found\")\n</pre> # Test that your OpenAI API key is correctly set as an environment variable # Note. if you run this notebook locally, you will need to reload your terminal and the notebook for the env variables to be live. import os  # Note. alternatively you can set a temporary env variable like this: # os.environ[\"OPENAI_API_KEY\"] = 'your-key-goes-here'  if os.getenv(\"OPENAI_API_KEY\") is not None:     print (\"OPENAI_API_KEY is ready\") else:     print (\"OPENAI_API_KEY environment variable not found\") In\u00a0[\u00a0]: Copied! <pre>import weaviate\nfrom datasets import load_dataset\nimport os\n\n# Connect to your Weaviate instance\nclient = weaviate.Client(\n    url=\"https://your-wcs-instance-name.weaviate.network/\",\n#   url=\"http://localhost:8080/\",\n    additional_headers={\n        \"X-OpenAI-Api-Key\": os.getenv(\"OPENAI_API_KEY\")\n    }\n)\n\n# Check if your instance is live and ready\n# This should return `True`\nclient.is_ready()\n</pre> import weaviate from datasets import load_dataset import os  # Connect to your Weaviate instance client = weaviate.Client(     url=\"https://your-wcs-instance-name.weaviate.network/\", #   url=\"http://localhost:8080/\",     additional_headers={         \"X-OpenAI-Api-Key\": os.getenv(\"OPENAI_API_KEY\")     } )  # Check if your instance is live and ready # This should return `True` client.is_ready() In\u00a0[\u00a0]: Copied! <pre># Clear up the schema, so that we can recreate it\nclient.schema.delete_all()\nclient.schema.get()\n\n# Define the Schema object to use `text-embedding-ada-002` on `title` and `content`, but skip it for `url`\narticle_schema = {\n    \"class\": \"Article\",\n    \"description\": \"A collection of articles\",\n    \"vectorizer\": \"text2vec-openai\",\n    \"moduleConfig\": {\n        \"text2vec-openai\": {\n          \"model\": \"ada\",\n          \"modelVersion\": \"002\",\n          \"type\": \"text\"\n        }\n    },\n    \"properties\": [{\n        \"name\": \"title\",\n        \"description\": \"Title of the article\",\n        \"dataType\": [\"string\"]\n    },\n    {\n        \"name\": \"content\",\n        \"description\": \"Contents of the article\",\n        \"dataType\": [\"text\"]\n    },\n    {\n        \"name\": \"url\",\n        \"description\": \"URL to the article\",\n        \"dataType\": [\"string\"],\n        \"moduleConfig\": { \"text2vec-openai\": { \"skip\": True } }\n    }]\n}\n\n# add the Article schema\nclient.schema.create_class(article_schema)\n\n# get the schema to make sure it worked\nclient.schema.get()\n</pre> # Clear up the schema, so that we can recreate it client.schema.delete_all() client.schema.get()  # Define the Schema object to use `text-embedding-ada-002` on `title` and `content`, but skip it for `url` article_schema = {     \"class\": \"Article\",     \"description\": \"A collection of articles\",     \"vectorizer\": \"text2vec-openai\",     \"moduleConfig\": {         \"text2vec-openai\": {           \"model\": \"ada\",           \"modelVersion\": \"002\",           \"type\": \"text\"         }     },     \"properties\": [{         \"name\": \"title\",         \"description\": \"Title of the article\",         \"dataType\": [\"string\"]     },     {         \"name\": \"content\",         \"description\": \"Contents of the article\",         \"dataType\": [\"text\"]     },     {         \"name\": \"url\",         \"description\": \"URL to the article\",         \"dataType\": [\"string\"],         \"moduleConfig\": { \"text2vec-openai\": { \"skip\": True } }     }] }  # add the Article schema client.schema.create_class(article_schema)  # get the schema to make sure it worked client.schema.get() In\u00a0[\u00a0]: Copied! <pre>### STEP 1 - load the dataset\n\nfrom datasets import load_dataset\nfrom typing import List, Iterator\n\n# We'll use the datasets library to pull the Simple Wikipedia dataset for embedding\ndataset = list(load_dataset(\"wikipedia\", \"20220301.simple\")[\"train\"])\n\n# For testing, limited to 2.5k articles for demo purposes\ndataset = dataset[:2_500]\n\n# Limited to 25k articles for larger demo purposes\n# dataset = dataset[:25_000]\n\n# for free OpenAI acounts, you can use 50 objects\n# dataset = dataset[:50]\n</pre> ### STEP 1 - load the dataset  from datasets import load_dataset from typing import List, Iterator  # We'll use the datasets library to pull the Simple Wikipedia dataset for embedding dataset = list(load_dataset(\"wikipedia\", \"20220301.simple\")[\"train\"])  # For testing, limited to 2.5k articles for demo purposes dataset = dataset[:2_500]  # Limited to 25k articles for larger demo purposes # dataset = dataset[:25_000]  # for free OpenAI acounts, you can use 50 objects # dataset = dataset[:50] In\u00a0[\u00a0]: Copied! <pre>### Step 2 - configure Weaviate Batch, with\n# - starting batch size of 100\n# - dynamically increase/decrease based on performance\n# - add timeout retries if something goes wrong\n\nclient.batch.configure(\n    batch_size=10, \n    dynamic=True,\n    timeout_retries=3,\n#   callback=None,\n)\n</pre> ### Step 2 - configure Weaviate Batch, with # - starting batch size of 100 # - dynamically increase/decrease based on performance # - add timeout retries if something goes wrong  client.batch.configure(     batch_size=10,      dynamic=True,     timeout_retries=3, #   callback=None, ) In\u00a0[\u00a0]: Copied! <pre>### Step 3 - import data\n\nprint(\"Importing Articles\")\n\ncounter=0\n\nwith client.batch as batch:\n    for article in dataset:\n        if (counter %10 == 0):\n            print(f\"Import {counter} / {len(dataset)} \")\n\n        properties = {\n            \"title\": article[\"title\"],\n            \"content\": article[\"text\"],\n            \"url\": article[\"url\"]\n        }\n        \n        batch.add_data_object(properties, \"Article\")\n        counter = counter+1\n\nprint(\"Importing Articles complete\")\n</pre> ### Step 3 - import data  print(\"Importing Articles\")  counter=0  with client.batch as batch:     for article in dataset:         if (counter %10 == 0):             print(f\"Import {counter} / {len(dataset)} \")          properties = {             \"title\": article[\"title\"],             \"content\": article[\"text\"],             \"url\": article[\"url\"]         }                  batch.add_data_object(properties, \"Article\")         counter = counter+1  print(\"Importing Articles complete\")        In\u00a0[\u00a0]: Copied! <pre># Test that all data has loaded \u2013 get object count\nresult = (\n    client.query.aggregate(\"Article\")\n    .with_fields(\"meta { count }\")\n    .do()\n)\nprint(\"Object count: \", result[\"data\"][\"Aggregate\"][\"Article\"], \"\\n\")\n</pre> # Test that all data has loaded \u2013 get object count result = (     client.query.aggregate(\"Article\")     .with_fields(\"meta { count }\")     .do() ) print(\"Object count: \", result[\"data\"][\"Aggregate\"][\"Article\"], \"\\n\") In\u00a0[\u00a0]: Copied! <pre># Test one article has worked by checking one object\ntest_article = (\n    client.query\n    .get(\"Article\", [\"title\", \"url\", \"content\"])\n    .with_limit(1)\n    .do()\n)[\"data\"][\"Get\"][\"Article\"][0]\n\nprint(test_article['title'])\nprint(test_article['url'])\nprint(test_article['content'])\n</pre> # Test one article has worked by checking one object test_article = (     client.query     .get(\"Article\", [\"title\", \"url\", \"content\"])     .with_limit(1)     .do() )[\"data\"][\"Get\"][\"Article\"][0]  print(test_article['title']) print(test_article['url']) print(test_article['content']) In\u00a0[\u00a0]: Copied! <pre>def query_weaviate(query, collection_name):\n    \n    nearText = {\n        \"concepts\": [query],\n        \"distance\": 0.7,\n    }\n\n    properties = [\n        \"title\", \"content\", \"url\",\n        \"_additional {certainty distance}\"\n    ]\n\n    result = (\n        client.query\n        .get(collection_name, properties)\n        .with_near_text(nearText)\n        .with_limit(10)\n        .do()\n    )\n    \n    # Check for errors\n    if (\"errors\" in result):\n        print (\"\\033[91mYou probably have run out of OpenAI API calls for the current minute \u2013 the limit is set at 60 per minute.\")\n        raise Exception(result[\"errors\"][0]['message'])\n    \n    return result[\"data\"][\"Get\"][collection_name]\n</pre> def query_weaviate(query, collection_name):          nearText = {         \"concepts\": [query],         \"distance\": 0.7,     }      properties = [         \"title\", \"content\", \"url\",         \"_additional {certainty distance}\"     ]      result = (         client.query         .get(collection_name, properties)         .with_near_text(nearText)         .with_limit(10)         .do()     )          # Check for errors     if (\"errors\" in result):         print (\"\\033[91mYou probably have run out of OpenAI API calls for the current minute \u2013 the limit is set at 60 per minute.\")         raise Exception(result[\"errors\"][0]['message'])          return result[\"data\"][\"Get\"][collection_name] In\u00a0[\u00a0]: Copied! <pre>query_result = query_weaviate(\"modern art in Europe\", \"Article\")\n\nfor i, article in enumerate(query_result):\n    print(f\"{i+1}. { article['title']} (Score: {round(article['_additional']['certainty'],3) })\")\n</pre> query_result = query_weaviate(\"modern art in Europe\", \"Article\")  for i, article in enumerate(query_result):     print(f\"{i+1}. { article['title']} (Score: {round(article['_additional']['certainty'],3) })\") In\u00a0[\u00a0]: Copied! <pre>query_result = query_weaviate(\"Famous battles in Scottish history\", \"Article\")\n\nfor i, article in enumerate(query_result):\n    print(f\"{i+1}. { article['title']} (Score: {round(article['_additional']['certainty'],3) })\")\n</pre> query_result = query_weaviate(\"Famous battles in Scottish history\", \"Article\")  for i, article in enumerate(query_result):     print(f\"{i+1}. { article['title']} (Score: {round(article['_additional']['certainty'],3) })\") <p>Thanks for following along, you're now equipped to set up your own vector databases and use embeddings to do all kinds of cool things - enjoy! For more complex use cases please continue to work through other cookbook examples in this repo.</p>"},{"location":"examples/vector_databases/weaviate/getting-started-with-weaviate-and-openai/#Using-Weaviate-with-OpenAI-vectorize-module-for-Embeddings-Search","title":"Using Weaviate with OpenAI vectorize module for Embeddings Search\u00b6","text":"<p>This notebook is prepared for a scenario where:</p> <ul> <li>Your data is not vectorized</li> <li>You want to run Vector Search on your data</li> <li>You want to use Weaviate with the OpenAI module (text2vec-openai), to generate vector embeddings for you.</li> </ul> <p>This notebook takes you through a simple flow to set up a Weaviate instance, connect to it (with OpenAI API key), configure data schema, import data (which will automatically generate vector embeddings for your data), and run semantic search.</p> <p>This is a common requirement for customers who want to store and search our embeddings with their own data in a secure environment to support production use cases such as chatbots, topic modelling and more.</p>"},{"location":"examples/vector_databases/weaviate/getting-started-with-weaviate-and-openai/#What-is-Weaviate","title":"What is Weaviate\u00b6","text":"<p>Weaviate is an open-source vector search engine that stores data objects together with their vectors. This allows for combining vector search with structured filtering.</p> <p>Weaviate uses KNN algorithms to create an vector-optimized index, which allows your queries to run extremely fast. Learn more here.</p> <p>Weaviate let's you use your favorite ML-models, and scale seamlessly into billions of data objects.</p>"},{"location":"examples/vector_databases/weaviate/getting-started-with-weaviate-and-openai/#Deployment-options","title":"Deployment options\u00b6","text":"<p>Whatever your scenario or production setup, Weaviate has an option for you. You can deploy Weaviate in the following setups:</p> <ul> <li>Self-hosted \u2013 you can deploy Weaviate with docker locally, or any server you want.</li> <li>SaaS \u2013 you can use Weaviate Cloud Service (WCS) to host your Weaviate instances.</li> <li>Hybrid-Saas \u2013 you can deploy Weaviate in your own private Cloud Service</li> </ul>"},{"location":"examples/vector_databases/weaviate/getting-started-with-weaviate-and-openai/#Programming-languages","title":"Programming languages\u00b6","text":"<p>Weaviate offers four client libraries, which allow you to communicate from your apps:</p> <ul> <li>Python</li> <li>JavaScript</li> <li>Java</li> <li>Go</li> </ul> <p>Additionally, Weavaite has a REST layer. Basically you can call Weaviate from any language that supports REST requests.</p>"},{"location":"examples/vector_databases/weaviate/getting-started-with-weaviate-and-openai/#Demo-Flow","title":"Demo Flow\u00b6","text":"<p>The demo flow is:</p> <ul> <li>Prerequisites Setup: Create a Weaviate instance and install required libraries</li> <li>Connect: Connect to your Weaviate instance</li> <li>Schema Configuration: Configure the schema of your data<ul> <li>Note: Here we can define which OpenAI Embedding Model to use</li> <li>Note: Here we can configure which properties to index on</li> </ul> </li> <li>Import data: Load a demo dataset and import it into Weaviate<ul> <li>Note: The import process will automatically index your data - based on the configuration in the schema</li> <li>Note: You don't need to explicitly vectorize your data, Weaviate will communicate with OpenAI to do it for you.</li> </ul> </li> <li>Run Queries: Query<ul> <li>Note: You don't need to explicitly vectorize your queries, Weaviate will communicate with OpenAI to do it for you.</li> </ul> </li> </ul> <p>Once you've run through this notebook you should have a basic understanding of how to setup and use vector databases, and can move on to more complex use cases making use of our embeddings.</p>"},{"location":"examples/vector_databases/weaviate/getting-started-with-weaviate-and-openai/#OpenAI-Module-in-Weaviate","title":"OpenAI Module in Weaviate\u00b6","text":"<p>All Weaviate instances come equiped with the text2vec-openai module.</p> <p>This module is responsible handling vectorization at import (or any CRUD operations) and when you run a query.</p>"},{"location":"examples/vector_databases/weaviate/getting-started-with-weaviate-and-openai/#No-need-to-manually-vectorize-data","title":"No need to manually vectorize data\u00b6","text":"<p>This is great news for you. With text2vec-openai you don't need to manually vectorize your data, as Weaviate will call OpenAI for you whenever necessary.</p> <p>All you need to do is:</p> <ol> <li>provide your OpenAI API Key \u2013 when you connected to the Weaviate Client</li> <li>define which OpenAI vectorizer to use in your Schema</li> </ol>"},{"location":"examples/vector_databases/weaviate/getting-started-with-weaviate-and-openai/#Prerequisites","title":"Prerequisites\u00b6","text":"<p>Before we start this project, we need setup the following:</p> <ul> <li>create a <code>Weaviate</code> instance</li> <li>install libraries<ul> <li><code>weaviate-client</code></li> <li><code>datasets</code></li> <li><code>apache-beam</code></li> </ul> </li> <li>get your OpenAI API key</li> </ul> <p>===========================================================</p>"},{"location":"examples/vector_databases/weaviate/getting-started-with-weaviate-and-openai/#Create-a-Weaviate-instance","title":"Create a Weaviate instance\u00b6","text":"<p>To create a Weaviate instance we have 2 options:</p> <ol> <li>(Recommended path) Weaviate Cloud Service \u2013 to host your Weaviate instance in the cloud. The free sandbox should be more than enough for this cookbook.</li> <li>Install and run Weaviate locally with Docker.</li> </ol>"},{"location":"examples/vector_databases/weaviate/getting-started-with-weaviate-and-openai/#Option-1-%E2%80%93-WCS-Installation-Steps","title":"Option 1 \u2013 WCS Installation Steps\u00b6","text":"<p>Use Weaviate Cloud Service (WCS) to create a free Weaviate cluster.</p> <ol> <li>create a free account and/or login to WCS</li> <li>create a <code>Weaviate Cluster</code> with the following settings:<ul> <li>Sandbox: <code>Sandbox Free</code></li> <li>Weaviate Version: Use default (latest)</li> <li>OIDC Authentication: <code>Disabled</code></li> </ul> </li> <li>your instance should be ready in a minute or two</li> <li>make a note of the <code>Cluster Id</code>. The link will take you to the full path of your cluster (you will need it later to connect to it). It should be something like: <code>https://your-project-name.weaviate.network</code></li> </ol>"},{"location":"examples/vector_databases/weaviate/getting-started-with-weaviate-and-openai/#Option-2-%E2%80%93-local-Weaviate-instance-with-Docker","title":"Option 2 \u2013 local Weaviate instance with Docker\u00b6","text":"<p>Install and run Weaviate locally with Docker.</p> <ol> <li>Download the ./docker-compose.yml file</li> <li>Then open your terminal, navigate to where your docker-compose.yml folder, and start docker with: <code>docker-compose up -d</code></li> <li>Once this is ready, your instance should be available at http://localhost:8080</li> </ol> <p>Note. To shut down your docker instance you can call: <code>docker-compose down</code></p>"},{"location":"examples/vector_databases/weaviate/getting-started-with-weaviate-and-openai/#Learn-more","title":"Learn more\u00b6","text":"<p>To learn more, about using Weaviate with Docker see the installation documentation.</p>"},{"location":"examples/vector_databases/weaviate/getting-started-with-weaviate-and-openai/#Install-required-libraries","title":"Install required libraries\u00b6","text":"<p>Before running this project make sure to have the following libraries:</p>"},{"location":"examples/vector_databases/weaviate/getting-started-with-weaviate-and-openai/#Weaviate-Python-client","title":"Weaviate Python client\u00b6","text":"<p>The Weaviate Python client allows you to communicate with your Weaviate instance from your Python project.</p>"},{"location":"examples/vector_databases/weaviate/getting-started-with-weaviate-and-openai/#datasets-&-apache-beam","title":"datasets &amp; apache-beam\u00b6","text":"<p>To load sample data, you need the <code>datasets</code> library and its' dependency <code>apache-beam</code>.</p>"},{"location":"examples/vector_databases/weaviate/getting-started-with-weaviate-and-openai/#Prepare-your-OpenAI-API-key","title":"Prepare your OpenAI API key\u00b6","text":"<p>The <code>OpenAI API key</code> is used for vectorization of your data at import, and for queries.</p> <p>If you don't have an OpenAI API key, you can get one from https://beta.openai.com/account/api-keys.</p> <p>Once you get your key, please add it to your environment variables as <code>OPENAI_API_KEY</code>.</p>"},{"location":"examples/vector_databases/weaviate/getting-started-with-weaviate-and-openai/#Connect-to-your-Weaviate-instance","title":"Connect to your Weaviate instance\u00b6","text":"<p>In this section, we will:</p> <ol> <li>test env variable <code>OPENAI_API_KEY</code> \u2013 make sure you completed the step in #Prepare-your-OpenAI-API-key</li> <li>connect to your Weaviate your <code>OpenAI API Key</code></li> <li>and test the client connection</li> </ol>"},{"location":"examples/vector_databases/weaviate/getting-started-with-weaviate-and-openai/#The-client","title":"The client\u00b6","text":"<p>After this step, the <code>client</code> object will be used to perform all Weaviate-related operations.</p>"},{"location":"examples/vector_databases/weaviate/getting-started-with-weaviate-and-openai/#Schema","title":"Schema\u00b6","text":"<p>In this section, we will:</p> <ol> <li>configure the data schema for your data</li> <li>select OpenAI module</li> </ol> <p>This is the second and final step, which requires OpenAI specific configuration. After this step, the rest of instructions wlll only touch on Weaviate, as the OpenAI tasks will be handled automatically.</p>"},{"location":"examples/vector_databases/weaviate/getting-started-with-weaviate-and-openai/#What-is-a-schema","title":"What is a schema\u00b6","text":"<p>In Weaviate you create schemas to capture each of the entities you will be searching.</p> <p>A schema is how you tell Weaviate:</p> <ul> <li>what embedding model should be used to vectorize the data</li> <li>what your data is made of (property names and types)</li> <li>which properties should be vectorized and indexed</li> </ul> <p>In this cookbook we will use a dataset for <code>Articles</code>, which contains:</p> <ul> <li><code>title</code></li> <li><code>content</code></li> <li><code>url</code></li> </ul> <p>We want to vectorize <code>title</code> and <code>content</code>, but not the <code>url</code>.</p> <p>To vectorize and query the data, we will use <code>text-embedding-ada-002</code>.</p>"},{"location":"examples/vector_databases/weaviate/getting-started-with-weaviate-and-openai/#Import-data","title":"Import data\u00b6","text":"<p>In this section we will:</p> <ol> <li>load the Simple Wikipedia dataset</li> <li>configure Weaviate Batch import (to make the import more efficient)</li> <li>import the data into Weaviate</li> </ol> <p>Note:  Like mentioned before. We don't need to manually vectorize the data. The text2vec-openai module will take care of that.</p>"},{"location":"examples/vector_databases/weaviate/getting-started-with-weaviate-and-openai/#Search-Data","title":"Search Data\u00b6","text":"<p>As above, we'll fire some queries at our new Index and get back results based on the closeness to our existing vectors</p>"},{"location":"examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai/","title":"Using Weaviate with OpenAI vectorize module for Hybrid Search","text":"<p>===========================================================</p> In\u00a0[\u00a0]: Copied! <pre># Install the Weaviate client for Python\n!pip install weaviate-client&gt;3.11.0\n\n# Install datasets and apache-beam to load the sample datasets\n!pip install datasets apache-beam\n</pre> # Install the Weaviate client for Python !pip install weaviate-client&gt;3.11.0  # Install datasets and apache-beam to load the sample datasets !pip install datasets apache-beam <p>===========================================================</p> In\u00a0[\u00a0]: Copied! <pre># Test that your OpenAI API key is correctly set as an environment variable\n# Note. if you run this notebook locally, you will need to reload your terminal and the notebook for the env variables to be live.\nimport os\n\n# Note. alternatively you can set a temporary env variable like this:\n# os.environ['OPENAI_API_KEY'] = 'your-key-goes-here'\n\nif os.getenv(\"OPENAI_API_KEY\") is not None:\n    print (\"OPENAI_API_KEY is ready\")\nelse:\n    print (\"OPENAI_API_KEY environment variable not found\")\n</pre> # Test that your OpenAI API key is correctly set as an environment variable # Note. if you run this notebook locally, you will need to reload your terminal and the notebook for the env variables to be live. import os  # Note. alternatively you can set a temporary env variable like this: # os.environ['OPENAI_API_KEY'] = 'your-key-goes-here'  if os.getenv(\"OPENAI_API_KEY\") is not None:     print (\"OPENAI_API_KEY is ready\") else:     print (\"OPENAI_API_KEY environment variable not found\") In\u00a0[\u00a0]: Copied! <pre>import weaviate\nfrom datasets import load_dataset\nimport os\n\n# Connect to your Weaviate instance\nclient = weaviate.Client(\n    url=\"https://your-wcs-instance-name.weaviate.network/\",\n#   url=\"http://localhost:8080/\",\n    additional_headers={\n        \"X-OpenAI-Api-Key\": os.getenv(\"OPENAI_API_KEY\")\n    }\n)\n\n# Check if your instance is live and ready\n# This should return `True`\nclient.is_ready()\n</pre> import weaviate from datasets import load_dataset import os  # Connect to your Weaviate instance client = weaviate.Client(     url=\"https://your-wcs-instance-name.weaviate.network/\", #   url=\"http://localhost:8080/\",     additional_headers={         \"X-OpenAI-Api-Key\": os.getenv(\"OPENAI_API_KEY\")     } )  # Check if your instance is live and ready # This should return `True` client.is_ready() In\u00a0[\u00a0]: Copied! <pre># Clear up the schema, so that we can recreate it\nclient.schema.delete_all()\nclient.schema.get()\n\n# Define the Schema object to use `text-embedding-ada-002` on `title` and `content`, but skip it for `url`\narticle_schema = {\n    \"class\": \"Article\",\n    \"description\": \"A collection of articles\",\n    \"vectorizer\": \"text2vec-openai\",\n    \"moduleConfig\": {\n        \"text2vec-openai\": {\n          \"model\": \"ada\",\n          \"modelVersion\": \"002\",\n          \"type\": \"text\"\n        }\n    },\n    \"properties\": [{\n        \"name\": \"title\",\n        \"description\": \"Title of the article\",\n        \"dataType\": [\"string\"]\n    },\n    {\n        \"name\": \"content\",\n        \"description\": \"Contents of the article\",\n        \"dataType\": [\"text\"]\n    },\n    {\n        \"name\": \"url\",\n        \"description\": \"URL to the article\",\n        \"dataType\": [\"string\"],\n        \"moduleConfig\": { \"text2vec-openai\": { \"skip\": True } }\n    }]\n}\n\n# add the Article schema\nclient.schema.create_class(article_schema)\n\n# get the schema to make sure it worked\nclient.schema.get()\n</pre> # Clear up the schema, so that we can recreate it client.schema.delete_all() client.schema.get()  # Define the Schema object to use `text-embedding-ada-002` on `title` and `content`, but skip it for `url` article_schema = {     \"class\": \"Article\",     \"description\": \"A collection of articles\",     \"vectorizer\": \"text2vec-openai\",     \"moduleConfig\": {         \"text2vec-openai\": {           \"model\": \"ada\",           \"modelVersion\": \"002\",           \"type\": \"text\"         }     },     \"properties\": [{         \"name\": \"title\",         \"description\": \"Title of the article\",         \"dataType\": [\"string\"]     },     {         \"name\": \"content\",         \"description\": \"Contents of the article\",         \"dataType\": [\"text\"]     },     {         \"name\": \"url\",         \"description\": \"URL to the article\",         \"dataType\": [\"string\"],         \"moduleConfig\": { \"text2vec-openai\": { \"skip\": True } }     }] }  # add the Article schema client.schema.create_class(article_schema)  # get the schema to make sure it worked client.schema.get() In\u00a0[\u00a0]: Copied! <pre>### STEP 1 - load the dataset\n\nfrom datasets import load_dataset\nfrom typing import List, Iterator\n\n# We'll use the datasets library to pull the Simple Wikipedia dataset for embedding\ndataset = list(load_dataset(\"wikipedia\", \"20220301.simple\")[\"train\"])\n\n# For testing, limited to 2.5k articles for demo purposes\ndataset = dataset[:2_500]\n\n# Limited to 25k articles for larger demo purposes\n# dataset = dataset[:25_000]\n\n# for free OpenAI acounts, you can use 50 objects\n# dataset = dataset[:50]\n</pre> ### STEP 1 - load the dataset  from datasets import load_dataset from typing import List, Iterator  # We'll use the datasets library to pull the Simple Wikipedia dataset for embedding dataset = list(load_dataset(\"wikipedia\", \"20220301.simple\")[\"train\"])  # For testing, limited to 2.5k articles for demo purposes dataset = dataset[:2_500]  # Limited to 25k articles for larger demo purposes # dataset = dataset[:25_000]  # for free OpenAI acounts, you can use 50 objects # dataset = dataset[:50] In\u00a0[\u00a0]: Copied! <pre>### Step 2 - configure Weaviate Batch, with\n# - starting batch size of 100\n# - dynamically increase/decrease based on performance\n# - add timeout retries if something goes wrong\n\nclient.batch.configure(\n    batch_size=10, \n    dynamic=True,\n    timeout_retries=3,\n#   callback=None,\n)\n</pre> ### Step 2 - configure Weaviate Batch, with # - starting batch size of 100 # - dynamically increase/decrease based on performance # - add timeout retries if something goes wrong  client.batch.configure(     batch_size=10,      dynamic=True,     timeout_retries=3, #   callback=None, ) In\u00a0[\u00a0]: Copied! <pre>### Step 3 - import data\n\nprint(\"Importing Articles\")\n\ncounter=0\n\nwith client.batch as batch:\n    for article in dataset:\n        if (counter %10 == 0):\n            print(f\"Import {counter} / {len(dataset)} \")\n\n        properties = {\n            \"title\": article[\"title\"],\n            \"content\": article[\"text\"],\n            \"url\": article[\"url\"]\n        }\n        \n        batch.add_data_object(properties, \"Article\")\n        counter = counter+1\n\nprint(\"Importing Articles complete\")\n</pre> ### Step 3 - import data  print(\"Importing Articles\")  counter=0  with client.batch as batch:     for article in dataset:         if (counter %10 == 0):             print(f\"Import {counter} / {len(dataset)} \")          properties = {             \"title\": article[\"title\"],             \"content\": article[\"text\"],             \"url\": article[\"url\"]         }                  batch.add_data_object(properties, \"Article\")         counter = counter+1  print(\"Importing Articles complete\")        In\u00a0[\u00a0]: Copied! <pre># Test that all data has loaded \u2013 get object count\nresult = (\n    client.query.aggregate(\"Article\")\n    .with_fields(\"meta { count }\")\n    .do()\n)\nprint(\"Object count: \", result[\"data\"][\"Aggregate\"][\"Article\"], \"\\n\")\n</pre> # Test that all data has loaded \u2013 get object count result = (     client.query.aggregate(\"Article\")     .with_fields(\"meta { count }\")     .do() ) print(\"Object count: \", result[\"data\"][\"Aggregate\"][\"Article\"], \"\\n\") In\u00a0[\u00a0]: Copied! <pre># Test one article has worked by checking one object\ntest_article = (\n    client.query\n    .get(\"Article\", [\"title\", \"url\", \"content\"])\n    .with_limit(1)\n    .do()\n)[\"data\"][\"Get\"][\"Article\"][0]\n\nprint(test_article['title'])\nprint(test_article['url'])\nprint(test_article['content'])\n</pre> # Test one article has worked by checking one object test_article = (     client.query     .get(\"Article\", [\"title\", \"url\", \"content\"])     .with_limit(1)     .do() )[\"data\"][\"Get\"][\"Article\"][0]  print(test_article['title']) print(test_article['url']) print(test_article['content']) In\u00a0[\u00a0]: Copied! <pre>def hybrid_query_weaviate(query, collection_name, alpha_val):\n    \n    nearText = {\n        \"concepts\": [query],\n        \"distance\": 0.7,\n    }\n\n    properties = [\n        \"title\", \"content\", \"url\",\n        \"_additional { score }\"\n    ]\n\n    result = (\n        client.query\n        .get(collection_name, properties)\n        .with_hybrid(nearText, alpha=alpha_val)\n        .with_limit(10)\n        .do()\n    )\n    \n    # Check for errors\n    if (\"errors\" in result):\n        print (\"\\033[91mYou probably have run out of OpenAI API calls for the current minute \u2013 the limit is set at 60 per minute.\")\n        raise Exception(result[\"errors\"][0]['message'])\n    \n    return result[\"data\"][\"Get\"][collection_name]\n</pre> def hybrid_query_weaviate(query, collection_name, alpha_val):          nearText = {         \"concepts\": [query],         \"distance\": 0.7,     }      properties = [         \"title\", \"content\", \"url\",         \"_additional { score }\"     ]      result = (         client.query         .get(collection_name, properties)         .with_hybrid(nearText, alpha=alpha_val)         .with_limit(10)         .do()     )          # Check for errors     if (\"errors\" in result):         print (\"\\033[91mYou probably have run out of OpenAI API calls for the current minute \u2013 the limit is set at 60 per minute.\")         raise Exception(result[\"errors\"][0]['message'])          return result[\"data\"][\"Get\"][collection_name] In\u00a0[\u00a0]: Copied! <pre>query_result = hybrid_query_weaviate(\"modern art in Europe\", \"Article\", 0.5)\n\nfor i, article in enumerate(query_result):\n    print(f\"{i+1}. { article['title']} (Score: {article['_additional']['score']})\")\n</pre> query_result = hybrid_query_weaviate(\"modern art in Europe\", \"Article\", 0.5)  for i, article in enumerate(query_result):     print(f\"{i+1}. { article['title']} (Score: {article['_additional']['score']})\") In\u00a0[\u00a0]: Copied! <pre>query_result = hybrid_query_weaviate(\"Famous battles in Scottish history\", \"Article\", 0.5)\n\nfor i, article in enumerate(query_result):\n    print(f\"{i+1}. { article['title']} (Score: {article['_additional']['score']})\")\n</pre> query_result = hybrid_query_weaviate(\"Famous battles in Scottish history\", \"Article\", 0.5)  for i, article in enumerate(query_result):     print(f\"{i+1}. { article['title']} (Score: {article['_additional']['score']})\") <p>Thanks for following along, you're now equipped to set up your own vector databases and use embeddings to do all kinds of cool things - enjoy! For more complex use cases please continue to work through other cookbook examples in this repo.</p>"},{"location":"examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai/#Using-Weaviate-with-OpenAI-vectorize-module-for-Hybrid-Search","title":"Using Weaviate with OpenAI vectorize module for Hybrid Search\u00b6","text":"<p>This notebook is prepared for a scenario where:</p> <ul> <li>Your data is not vectorized</li> <li>You want to run Hybrid Search (learn more) on your data</li> <li>You want to use Weaviate with the OpenAI module (text2vec-openai), to generate vector embeddings for you.</li> </ul> <p>This notebook takes you through a simple flow to set up a Weaviate instance, connect to it (with OpenAI API key), configure data schema, import data (which will automatically generate vector embeddings for your data), and run hybrid search (mixing of vector and BM25 search).</p> <p>This is a common requirement for customers who want to store and search our embeddings with their own data in a secure environment to support production use cases such as chatbots, topic modelling and more.</p>"},{"location":"examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai/#What-is-Weaviate","title":"What is Weaviate\u00b6","text":"<p>Weaviate is an open-source vector search engine that stores data objects together with their vectors. This allows for combining vector search with structured filtering.</p> <p>Weaviate uses KNN algorithms to create an vector-optimized index, which allows your queries to run extremely fast. Learn more here.</p> <p>Weaviate let's you use your favorite ML-models, and scale seamlessly into billions of data objects.</p>"},{"location":"examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai/#Deployment-options","title":"Deployment options\u00b6","text":"<p>Whatever your scenario or production setup, Weaviate has an option for you. You can deploy Weaviate in the following setups:</p> <ul> <li>Self-hosted \u2013 you can deploy Weaviate with docker locally, or any server you want.</li> <li>SaaS \u2013 you can use Weaviate Cloud Service (WCS) to host your Weaviate instances.</li> <li>Hybrid-Saas \u2013 you can deploy Weaviate in your own private Cloud Service</li> </ul>"},{"location":"examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai/#Programming-languages","title":"Programming languages\u00b6","text":"<p>Weaviate offers four client libraries, which allow you to communicate from your apps:</p> <ul> <li>Python</li> <li>JavaScript</li> <li>Java</li> <li>Go</li> </ul> <p>Additionally, Weavaite has a REST layer. Basically you can call Weaviate from any language that supports REST requests.</p>"},{"location":"examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai/#Demo-Flow","title":"Demo Flow\u00b6","text":"<p>The demo flow is:</p> <ul> <li>Prerequisites Setup: Create a Weaviate instance and install required libraries</li> <li>Connect: Connect to your Weaviate instance</li> <li>Schema Configuration: Configure the schema of your data<ul> <li>Note: Here we can define which OpenAI Embedding Model to use</li> <li>Note: Here we can configure which properties to index on</li> </ul> </li> <li>Import data: Load a demo dataset and import it into Weaviate<ul> <li>Note: The import process will automatically index your data - based on the configuration in the schema</li> <li>Note: You don't need to explicitly vectorize your data, Weaviate will communicate with OpenAI to do it for you.</li> </ul> </li> <li>Run Queries: Query<ul> <li>Note: You don't need to explicitly vectorize your queries, Weaviate will communicate with OpenAI to do it for you.</li> </ul> </li> </ul> <p>Once you've run through this notebook you should have a basic understanding of how to setup and use vector databases, and can move on to more complex use cases making use of our embeddings.</p>"},{"location":"examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai/#OpenAI-Module-in-Weaviate","title":"OpenAI Module in Weaviate\u00b6","text":"<p>All Weaviate instances come equiped with the text2vec-openai module.</p> <p>This module is responsible handling vectorization at import (or any CRUD operations) and when you run a query.</p>"},{"location":"examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai/#No-need-to-manually-vectorize-data","title":"No need to manually vectorize data\u00b6","text":"<p>This is great news for you. With text2vec-openai you don't need to manually vectorize your data, as Weaviate will call OpenAI for you whenever necessary.</p> <p>All you need to do is:</p> <ol> <li>provide your OpenAI API Key \u2013 when you connected to the Weaviate Client</li> <li>define which OpenAI vectorizer to use in your Schema</li> </ol>"},{"location":"examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai/#Prerequisites","title":"Prerequisites\u00b6","text":"<p>Before we start this project, we need setup the following:</p> <ul> <li>create a <code>Weaviate</code> instance</li> <li>install libraries<ul> <li><code>weaviate-client</code></li> <li><code>datasets</code></li> <li><code>apache-beam</code></li> </ul> </li> <li>get your OpenAI API key</li> </ul> <p>===========================================================</p>"},{"location":"examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai/#Create-a-Weaviate-instance","title":"Create a Weaviate instance\u00b6","text":"<p>To create a Weaviate instance we have 2 options:</p> <ol> <li>(Recommended path) Weaviate Cloud Service \u2013 to host your Weaviate instance in the cloud. The free sandbox should be more than enough for this cookbook.</li> <li>Install and run Weaviate locally with Docker.</li> </ol>"},{"location":"examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai/#Option-1-%E2%80%93-WCS-Installation-Steps","title":"Option 1 \u2013 WCS Installation Steps\u00b6","text":"<p>Use Weaviate Cloud Service (WCS) to create a free Weaviate cluster.</p> <ol> <li>create a free account and/or login to WCS</li> <li>create a <code>Weaviate Cluster</code> with the following settings:<ul> <li>Sandbox: <code>Sandbox Free</code></li> <li>Weaviate Version: Use default (latest)</li> <li>OIDC Authentication: <code>Disabled</code></li> </ul> </li> <li>your instance should be ready in a minute or two</li> <li>make a note of the <code>Cluster Id</code>. The link will take you to the full path of your cluster (you will need it later to connect to it). It should be something like: <code>https://your-project-name.weaviate.network</code></li> </ol>"},{"location":"examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai/#Option-2-%E2%80%93-local-Weaviate-instance-with-Docker","title":"Option 2 \u2013 local Weaviate instance with Docker\u00b6","text":"<p>Install and run Weaviate locally with Docker.</p> <ol> <li>Download the ./docker-compose.yml file</li> <li>Then open your terminal, navigate to where your docker-compose.yml folder, and start docker with: <code>docker-compose up -d</code></li> <li>Once this is ready, your instance should be available at http://localhost:8080</li> </ol> <p>Note. To shut down your docker instance you can call: <code>docker-compose down</code></p>"},{"location":"examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai/#Learn-more","title":"Learn more\u00b6","text":"<p>To learn more, about using Weaviate with Docker see the installation documentation.</p>"},{"location":"examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai/#Install-required-libraries","title":"Install required libraries\u00b6","text":"<p>Before running this project make sure to have the following libraries:</p>"},{"location":"examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai/#Weaviate-Python-client","title":"Weaviate Python client\u00b6","text":"<p>The Weaviate Python client allows you to communicate with your Weaviate instance from your Python project.</p>"},{"location":"examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai/#datasets-&-apache-beam","title":"datasets &amp; apache-beam\u00b6","text":"<p>To load sample data, you need the <code>datasets</code> library and its' dependency <code>apache-beam</code>.</p>"},{"location":"examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai/#Prepare-your-OpenAI-API-key","title":"Prepare your OpenAI API key\u00b6","text":"<p>The <code>OpenAI API key</code> is used for vectorization of your data at import, and for queries.</p> <p>If you don't have an OpenAI API key, you can get one from https://beta.openai.com/account/api-keys.</p> <p>Once you get your key, please add it to your environment variables as <code>OPENAI_API_KEY</code>.</p>"},{"location":"examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai/#Connect-to-your-Weaviate-instance","title":"Connect to your Weaviate instance\u00b6","text":"<p>In this section, we will:</p> <ol> <li>test env variable <code>OPENAI_API_KEY</code> \u2013 make sure you completed the step in #Prepare-your-OpenAI-API-key</li> <li>connect to your Weaviate your <code>OpenAI API Key</code></li> <li>and test the client connection</li> </ol>"},{"location":"examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai/#The-client","title":"The client\u00b6","text":"<p>After this step, the <code>client</code> object will be used to perform all Weaviate-related operations.</p>"},{"location":"examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai/#Schema","title":"Schema\u00b6","text":"<p>In this section, we will:</p> <ol> <li>configure the data schema for your data</li> <li>select OpenAI module</li> </ol> <p>This is the second and final step, which requires OpenAI specific configuration. After this step, the rest of instructions wlll only touch on Weaviate, as the OpenAI tasks will be handled automatically.</p>"},{"location":"examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai/#What-is-a-schema","title":"What is a schema\u00b6","text":"<p>In Weaviate you create schemas to capture each of the entities you will be searching.</p> <p>A schema is how you tell Weaviate:</p> <ul> <li>what embedding model should be used to vectorize the data</li> <li>what your data is made of (property names and types)</li> <li>which properties should be vectorized and indexed</li> </ul> <p>In this cookbook we will use a dataset for <code>Articles</code>, which contains:</p> <ul> <li><code>title</code></li> <li><code>content</code></li> <li><code>url</code></li> </ul> <p>We want to vectorize <code>title</code> and <code>content</code>, but not the <code>url</code>.</p> <p>To vectorize and query the data, we will use <code>text-embedding-ada-002</code>.</p>"},{"location":"examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai/#Import-data","title":"Import data\u00b6","text":"<p>In this section we will:</p> <ol> <li>load the Simple Wikipedia dataset</li> <li>configure Weaviate Batch import (to make the import more efficient)</li> <li>import the data into Weaviate</li> </ol> <p>Note:  Like mentioned before. We don't need to manually vectorize the data. The text2vec-openai module will take care of that.</p>"},{"location":"examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai/#Search-Data","title":"Search Data\u00b6","text":"<p>As above, we'll fire some queries at our new Index and get back results based on the closeness to our existing vectors</p> <p>Learn more about the <code>alpha</code> setting here</p>"},{"location":"examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai/","title":"Question Answering in Weaviate with OpenAI Q&amp;A module","text":"<p>===========================================================</p> In\u00a0[\u00a0]: Copied! <pre># Install the Weaviate client for Python\n!pip install weaviate-client&gt;3.11.0\n\n# Install datasets and apache-beam to load the sample datasets\n!pip install datasets apache-beam\n</pre> # Install the Weaviate client for Python !pip install weaviate-client&gt;3.11.0  # Install datasets and apache-beam to load the sample datasets !pip install datasets apache-beam <p>===========================================================</p> In\u00a0[\u00a0]: Copied! <pre># Test that your OpenAI API key is correctly set as an environment variable\n# Note. if you run this notebook locally, you will need to reload your terminal and the notebook for the env variables to be live.\nimport os\n\n# Note. alternatively you can set a temporary env variable like this:\n# os.environ['OPENAI_API_KEY'] = 'your-key-goes-here'\n\nif os.getenv(\"OPENAI_API_KEY\") is not None:\n    print (\"OPENAI_API_KEY is ready\")\nelse:\n    print (\"OPENAI_API_KEY environment variable not found\")\n</pre> # Test that your OpenAI API key is correctly set as an environment variable # Note. if you run this notebook locally, you will need to reload your terminal and the notebook for the env variables to be live. import os  # Note. alternatively you can set a temporary env variable like this: # os.environ['OPENAI_API_KEY'] = 'your-key-goes-here'  if os.getenv(\"OPENAI_API_KEY\") is not None:     print (\"OPENAI_API_KEY is ready\") else:     print (\"OPENAI_API_KEY environment variable not found\") In\u00a0[\u00a0]: Copied! <pre>import weaviate\nfrom datasets import load_dataset\nimport os\n\n# Connect to your Weaviate instance\nclient = weaviate.Client(\n    url=\"https://your-wcs-instance-name.weaviate.network/\",\n#   url=\"http://localhost:8080/\",\n    additional_headers={\n        \"X-OpenAI-Api-Key\": os.getenv(\"OPENAI_API_KEY\")\n    }\n)\n\n# Check if your instance is live and ready\n# This should return `True`\nclient.is_ready()\n</pre> import weaviate from datasets import load_dataset import os  # Connect to your Weaviate instance client = weaviate.Client(     url=\"https://your-wcs-instance-name.weaviate.network/\", #   url=\"http://localhost:8080/\",     additional_headers={         \"X-OpenAI-Api-Key\": os.getenv(\"OPENAI_API_KEY\")     } )  # Check if your instance is live and ready # This should return `True` client.is_ready() In\u00a0[\u00a0]: Copied! <pre># Clear up the schema, so that we can recreate it\nclient.schema.delete_all()\nclient.schema.get()\n\n# Define the Schema object to use `text-embedding-ada-002` on `title` and `content`, but skip it for `url`\narticle_schema = {\n    \"class\": \"Article\",\n    \"description\": \"A collection of articles\",\n    \"vectorizer\": \"text2vec-openai\",\n    \"moduleConfig\": {\n        \"text2vec-openai\": {\n          \"model\": \"ada\",\n          \"modelVersion\": \"002\",\n          \"type\": \"text\"\n        }, \n        \"qna-openai\": {\n          \"model\": \"text-davinci-002\",\n          \"maxTokens\": 16,\n          \"temperature\": 0.0,\n          \"topP\": 1,\n          \"frequencyPenalty\": 0.0,\n          \"presencePenalty\": 0.0\n        }\n    },\n    \"properties\": [{\n        \"name\": \"title\",\n        \"description\": \"Title of the article\",\n        \"dataType\": [\"string\"]\n    },\n    {\n        \"name\": \"content\",\n        \"description\": \"Contents of the article\",\n        \"dataType\": [\"text\"]\n    },\n    {\n        \"name\": \"url\",\n        \"description\": \"URL to the article\",\n        \"dataType\": [\"string\"],\n        \"moduleConfig\": { \"text2vec-openai\": { \"skip\": True } }\n    }]\n}\n\n# add the Article schema\nclient.schema.create_class(article_schema)\n\n# get the schema to make sure it worked\nclient.schema.get()\n</pre> # Clear up the schema, so that we can recreate it client.schema.delete_all() client.schema.get()  # Define the Schema object to use `text-embedding-ada-002` on `title` and `content`, but skip it for `url` article_schema = {     \"class\": \"Article\",     \"description\": \"A collection of articles\",     \"vectorizer\": \"text2vec-openai\",     \"moduleConfig\": {         \"text2vec-openai\": {           \"model\": \"ada\",           \"modelVersion\": \"002\",           \"type\": \"text\"         },          \"qna-openai\": {           \"model\": \"text-davinci-002\",           \"maxTokens\": 16,           \"temperature\": 0.0,           \"topP\": 1,           \"frequencyPenalty\": 0.0,           \"presencePenalty\": 0.0         }     },     \"properties\": [{         \"name\": \"title\",         \"description\": \"Title of the article\",         \"dataType\": [\"string\"]     },     {         \"name\": \"content\",         \"description\": \"Contents of the article\",         \"dataType\": [\"text\"]     },     {         \"name\": \"url\",         \"description\": \"URL to the article\",         \"dataType\": [\"string\"],         \"moduleConfig\": { \"text2vec-openai\": { \"skip\": True } }     }] }  # add the Article schema client.schema.create_class(article_schema)  # get the schema to make sure it worked client.schema.get() In\u00a0[\u00a0]: Copied! <pre>### STEP 1 - load the dataset\n\nfrom datasets import load_dataset\nfrom typing import List, Iterator\n\n# We'll use the datasets library to pull the Simple Wikipedia dataset for embedding\ndataset = list(load_dataset(\"wikipedia\", \"20220301.simple\")[\"train\"])\n\n# For testing, limited to 2.5k articles for demo purposes\ndataset = dataset[:2_500]\n\n# Limited to 25k articles for larger demo purposes\n# dataset = dataset[:25_000]\n\n# for free OpenAI acounts, you can use 50 objects\n# dataset = dataset[:50]\n</pre> ### STEP 1 - load the dataset  from datasets import load_dataset from typing import List, Iterator  # We'll use the datasets library to pull the Simple Wikipedia dataset for embedding dataset = list(load_dataset(\"wikipedia\", \"20220301.simple\")[\"train\"])  # For testing, limited to 2.5k articles for demo purposes dataset = dataset[:2_500]  # Limited to 25k articles for larger demo purposes # dataset = dataset[:25_000]  # for free OpenAI acounts, you can use 50 objects # dataset = dataset[:50] In\u00a0[\u00a0]: Copied! <pre>### Step 2 - configure Weaviate Batch, with\n# - starting batch size of 100\n# - dynamically increase/decrease based on performance\n# - add timeout retries if something goes wrong\n\nclient.batch.configure(\n    batch_size=10, \n    dynamic=True,\n    timeout_retries=3,\n#   callback=None,\n)\n</pre> ### Step 2 - configure Weaviate Batch, with # - starting batch size of 100 # - dynamically increase/decrease based on performance # - add timeout retries if something goes wrong  client.batch.configure(     batch_size=10,      dynamic=True,     timeout_retries=3, #   callback=None, ) In\u00a0[\u00a0]: Copied! <pre>### Step 3 - import data\n\nprint(\"Importing Articles\")\n\ncounter=0\n\nwith client.batch as batch:\n    for article in dataset:\n        if (counter %10 == 0):\n            print(f\"Import {counter} / {len(dataset)} \")\n\n        properties = {\n            \"title\": article[\"title\"],\n            \"content\": article[\"text\"],\n            \"url\": article[\"url\"]\n        }\n        \n        batch.add_data_object(properties, \"Article\")\n        counter = counter+1\n\nprint(\"Importing Articles complete\")\n</pre> ### Step 3 - import data  print(\"Importing Articles\")  counter=0  with client.batch as batch:     for article in dataset:         if (counter %10 == 0):             print(f\"Import {counter} / {len(dataset)} \")          properties = {             \"title\": article[\"title\"],             \"content\": article[\"text\"],             \"url\": article[\"url\"]         }                  batch.add_data_object(properties, \"Article\")         counter = counter+1  print(\"Importing Articles complete\") In\u00a0[\u00a0]: Copied! <pre># Test that all data has loaded \u2013 get object count\nresult = (\n    client.query.aggregate(\"Article\")\n    .with_fields(\"meta { count }\")\n    .do()\n)\nprint(\"Object count: \", result[\"data\"][\"Aggregate\"][\"Article\"], \"\\n\")\n</pre> # Test that all data has loaded \u2013 get object count result = (     client.query.aggregate(\"Article\")     .with_fields(\"meta { count }\")     .do() ) print(\"Object count: \", result[\"data\"][\"Aggregate\"][\"Article\"], \"\\n\") In\u00a0[\u00a0]: Copied! <pre># Test one article has worked by checking one object\ntest_article = (\n    client.query\n    .get(\"Article\", [\"title\", \"url\", \"content\"])\n    .with_limit(1)\n    .do()\n)[\"data\"][\"Get\"][\"Article\"][0]\n\nprint(test_article['title'])\nprint(test_article['url'])\nprint(test_article['content'])\n</pre> # Test one article has worked by checking one object test_article = (     client.query     .get(\"Article\", [\"title\", \"url\", \"content\"])     .with_limit(1)     .do() )[\"data\"][\"Get\"][\"Article\"][0]  print(test_article['title']) print(test_article['url']) print(test_article['content']) In\u00a0[\u00a0]: Copied! <pre>def qna(query, collection_name):\n    \n    properties = [\n        \"title\", \"content\", \"url\",\n        \"_additional { answer { hasAnswer property result startPosition endPosition } distance }\"\n    ]\n\n    ask = {\n        \"question\": query,\n        \"properties\": [\"content\"]\n    }\n\n    result = (\n        client.query\n        .get(collection_name, properties)\n        .with_ask(ask)\n        .with_limit(1)\n        .do()\n    )\n    \n    # Check for errors\n    if (\"errors\" in result):\n        print (\"\\033[91mYou probably have run out of OpenAI API calls for the current minute \u2013 the limit is set at 60 per minute.\")\n        raise Exception(result[\"errors\"][0]['message'])\n    \n    return result[\"data\"][\"Get\"][collection_name]\n</pre> def qna(query, collection_name):          properties = [         \"title\", \"content\", \"url\",         \"_additional { answer { hasAnswer property result startPosition endPosition } distance }\"     ]      ask = {         \"question\": query,         \"properties\": [\"content\"]     }      result = (         client.query         .get(collection_name, properties)         .with_ask(ask)         .with_limit(1)         .do()     )          # Check for errors     if (\"errors\" in result):         print (\"\\033[91mYou probably have run out of OpenAI API calls for the current minute \u2013 the limit is set at 60 per minute.\")         raise Exception(result[\"errors\"][0]['message'])          return result[\"data\"][\"Get\"][collection_name] In\u00a0[\u00a0]: Copied! <pre>query_result = qna(\"Did Alanis Morissette win a Grammy?\", \"Article\")\n\nfor i, article in enumerate(query_result):\n    print(f\"{i+1}. { article['_additional']['answer']['result']} (Distance: {round(article['_additional']['distance'],3) })\")\n</pre> query_result = qna(\"Did Alanis Morissette win a Grammy?\", \"Article\")  for i, article in enumerate(query_result):     print(f\"{i+1}. { article['_additional']['answer']['result']} (Distance: {round(article['_additional']['distance'],3) })\") In\u00a0[\u00a0]: Copied! <pre>query_result = qna(\"What is the capital of China?\", \"Article\")\n\nfor i, article in enumerate(query_result):\n    if article['_additional']['answer']['hasAnswer'] == False:\n      print('No answer found')\n    else:\n      print(f\"{i+1}. { article['_additional']['answer']['result']} (Distance: {round(article['_additional']['distance'],3) })\")\n</pre> query_result = qna(\"What is the capital of China?\", \"Article\")  for i, article in enumerate(query_result):     if article['_additional']['answer']['hasAnswer'] == False:       print('No answer found')     else:       print(f\"{i+1}. { article['_additional']['answer']['result']} (Distance: {round(article['_additional']['distance'],3) })\") <p>Thanks for following along, you're now equipped to set up your own vector databases and use embeddings to do all kinds of cool things - enjoy! For more complex use cases please continue to work through other cookbook examples in this repo.</p>"},{"location":"examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai/#Question-Answering-in-Weaviate-with-OpenAI-Q&A-module","title":"Question Answering in Weaviate with OpenAI Q&amp;A module\u00b6","text":"<p>This notebook is prepared for a scenario where:</p> <ul> <li>Your data is not vectorized</li> <li>You want to run Q&amp;A (learn more) on your data based on the OpenAI completions endpoint.</li> <li>You want to use Weaviate with the OpenAI module (text2vec-openai), to generate vector embeddings for you.</li> </ul> <p>This notebook takes you through a simple flow to set up a Weaviate instance, connect to it (with OpenAI API key), configure data schema, import data (which will automatically generate vector embeddings for your data), and run question answering.</p>"},{"location":"examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai/#What-is-Weaviate","title":"What is Weaviate\u00b6","text":"<p>Weaviate is an open-source vector search engine that stores data objects together with their vectors. This allows for combining vector search with structured filtering.</p> <p>Weaviate uses KNN algorithms to create an vector-optimized index, which allows your queries to run extremely fast. Learn more here.</p> <p>Weaviate let's you use your favorite ML-models, and scale seamlessly into billions of data objects.</p>"},{"location":"examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai/#Deployment-options","title":"Deployment options\u00b6","text":"<p>Whatever your scenario or production setup, Weaviate has an option for you. You can deploy Weaviate in the following setups:</p> <ul> <li>Self-hosted \u2013 you can deploy Weaviate with docker locally, or any server you want.</li> <li>SaaS \u2013 you can use Weaviate Cloud Service (WCS) to host your Weaviate instances.</li> <li>Hybrid-Saas \u2013 you can deploy Weaviate in your own private Cloud Service</li> </ul>"},{"location":"examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai/#Programming-languages","title":"Programming languages\u00b6","text":"<p>Weaviate offers four client libraries, which allow you to communicate from your apps:</p> <ul> <li>Python</li> <li>JavaScript</li> <li>Java</li> <li>Go</li> </ul> <p>Additionally, Weavaite has a REST layer. Basically you can call Weaviate from any language that supports REST requests.</p>"},{"location":"examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai/#Demo-Flow","title":"Demo Flow\u00b6","text":"<p>The demo flow is:</p> <ul> <li>Prerequisites Setup: Create a Weaviate instance and install required libraries</li> <li>Connect: Connect to your Weaviate instance</li> <li>Schema Configuration: Configure the schema of your data<ul> <li>Note: Here we can define which OpenAI Embedding Model to use</li> <li>Note: Here we can configure which properties to index on</li> </ul> </li> <li>Import data: Load a demo dataset and import it into Weaviate<ul> <li>Note: The import process will automatically index your data - based on the configuration in the schema</li> <li>Note: You don't need to explicitly vectorize your data, Weaviate will communicate with OpenAI to do it for you.</li> </ul> </li> <li>Run Queries: Query<ul> <li>Note: You don't need to explicitly vectorize your queries, Weaviate will communicate with OpenAI to do it for you.</li> <li>Note: The <code>qna-openai</code> module automatically communicates with the OpenAI completions endpoint.</li> </ul> </li> </ul> <p>Once you've run through this notebook you should have a basic understanding of how to setup and use vector databases for question answering.</p>"},{"location":"examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai/#OpenAI-Module-in-Weaviate","title":"OpenAI Module in Weaviate\u00b6","text":"<p>All Weaviate instances come equiped with the text2vec-openai and the qna-openai modules.</p> <p>The first module is responsible for handling vectorization at import (or any CRUD operations) and when you run a search query. The second module communicates with the OpenAI completions endpoint.</p>"},{"location":"examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai/#No-need-to-manually-vectorize-data","title":"No need to manually vectorize data\u00b6","text":"<p>This is great news for you. With text2vec-openai you don't need to manually vectorize your data, as Weaviate will call OpenAI for you whenever necessary.</p> <p>All you need to do is:</p> <ol> <li>provide your OpenAI API Key \u2013 when you connected to the Weaviate Client</li> <li>define which OpenAI vectorizer to use in your Schema</li> </ol>"},{"location":"examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai/#Prerequisites","title":"Prerequisites\u00b6","text":"<p>Before we start this project, we need setup the following:</p> <ul> <li>create a <code>Weaviate</code> instance</li> <li>install libraries<ul> <li><code>weaviate-client</code></li> <li><code>datasets</code></li> <li><code>apache-beam</code></li> </ul> </li> <li>get your OpenAI API key</li> </ul> <p>===========================================================</p>"},{"location":"examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai/#Create-a-Weaviate-instance","title":"Create a Weaviate instance\u00b6","text":"<p>To create a Weaviate instance we have 2 options:</p> <ol> <li>(Recommended path) Weaviate Cloud Service \u2013 to host your Weaviate instance in the cloud. The free sandbox should be more than enough for this cookbook.</li> <li>Install and run Weaviate locally with Docker.</li> </ol>"},{"location":"examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai/#Option-1-%E2%80%93-WCS-Installation-Steps","title":"Option 1 \u2013 WCS Installation Steps\u00b6","text":"<p>Use Weaviate Cloud Service (WCS) to create a free Weaviate cluster.</p> <ol> <li>create a free account and/or login to WCS</li> <li>create a <code>Weaviate Cluster</code> with the following settings:<ul> <li>Sandbox: <code>Sandbox Free</code></li> <li>Weaviate Version: Use default (latest)</li> <li>OIDC Authentication: <code>Disabled</code></li> </ul> </li> <li>your instance should be ready in a minute or two</li> <li>make a note of the <code>Cluster Id</code>. The link will take you to the full path of your cluster (you will need it later to connect to it). It should be something like: <code>https://your-project-name.weaviate.network</code></li> </ol>"},{"location":"examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai/#Option-2-%E2%80%93-local-Weaviate-instance-with-Docker","title":"Option 2 \u2013 local Weaviate instance with Docker\u00b6","text":"<p>Install and run Weaviate locally with Docker.</p> <ol> <li>Download the ./docker-compose.yml file</li> <li>Then open your terminal, navigate to where your docker-compose.yml folder, and start docker with: <code>docker-compose up -d</code></li> <li>Once this is ready, your instance should be available at http://localhost:8080</li> </ol> <p>Note. To shut down your docker instance you can call: <code>docker-compose down</code></p>"},{"location":"examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai/#Learn-more","title":"Learn more\u00b6","text":"<p>To learn more, about using Weaviate with Docker see the installation documentation.</p>"},{"location":"examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai/#Install-required-libraries","title":"Install required libraries\u00b6","text":"<p>Before running this project make sure to have the following libraries:</p>"},{"location":"examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai/#Weaviate-Python-client","title":"Weaviate Python client\u00b6","text":"<p>The Weaviate Python client allows you to communicate with your Weaviate instance from your Python project.</p>"},{"location":"examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai/#datasets-&-apache-beam","title":"datasets &amp; apache-beam\u00b6","text":"<p>To load sample data, you need the <code>datasets</code> library and its' dependency <code>apache-beam</code>.</p>"},{"location":"examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai/#Prepare-your-OpenAI-API-key","title":"Prepare your OpenAI API key\u00b6","text":"<p>The <code>OpenAI API key</code> is used for vectorization of your data at import, and for queries.</p> <p>If you don't have an OpenAI API key, you can get one from https://beta.openai.com/account/api-keys.</p> <p>Once you get your key, please add it to your environment variables as <code>OPENAI_API_KEY</code>.</p>"},{"location":"examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai/#Connect-to-your-Weaviate-instance","title":"Connect to your Weaviate instance\u00b6","text":"<p>In this section, we will:</p> <ol> <li>test env variable <code>OPENAI_API_KEY</code> \u2013 make sure you completed the step in #Prepare-your-OpenAI-API-key</li> <li>connect to your Weaviate your <code>OpenAI API Key</code></li> <li>and test the client connection</li> </ol>"},{"location":"examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai/#The-client","title":"The client\u00b6","text":"<p>After this step, the <code>client</code> object will be used to perform all Weaviate-related operations.</p>"},{"location":"examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai/#Schema","title":"Schema\u00b6","text":"<p>In this section, we will:</p> <ol> <li>configure the data schema for your data</li> <li>select OpenAI module</li> </ol> <p>This is the second and final step, which requires OpenAI specific configuration. After this step, the rest of instructions wlll only touch on Weaviate, as the OpenAI tasks will be handled automatically.</p>"},{"location":"examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai/#What-is-a-schema","title":"What is a schema\u00b6","text":"<p>In Weaviate you create schemas to capture each of the entities you will be searching.</p> <p>A schema is how you tell Weaviate:</p> <ul> <li>what embedding model should be used to vectorize the data</li> <li>what your data is made of (property names and types)</li> <li>which properties should be vectorized and indexed</li> </ul> <p>In this cookbook we will use a dataset for <code>Articles</code>, which contains:</p> <ul> <li><code>title</code></li> <li><code>content</code></li> <li><code>url</code></li> </ul> <p>We want to vectorize <code>title</code> and <code>content</code>, but not the <code>url</code>.</p> <p>To vectorize and query the data, we will use <code>text-embedding-ada-002</code>. For Q&amp;A we will use <code>text-davinci-002</code>.</p>"},{"location":"examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai/#Import-data","title":"Import data\u00b6","text":"<p>In this section we will:</p> <ol> <li>load the Simple Wikipedia dataset</li> <li>configure Weaviate Batch import (to make the import more efficient)</li> <li>import the data into Weaviate</li> </ol> <p>Note:  Like mentioned before. We don't need to manually vectorize the data. The text2vec-openai module will take care of that.</p>"},{"location":"examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai/#Question-Answering-on-the-Data","title":"Question Answering on the Data\u00b6","text":"<p>As above, we'll fire some queries at our new Index and get back results based on the closeness to our existing vectors</p>"},{"location":"transition_guides_for_deprecated_API_endpoints/","title":"Deprecation of Answers, Classification, and Search","text":"<p>In 2021, OpenAI released specialized endpoints in beta for Answers, Classification, and Search.</p> <p>While these specialized endpoints were convenient, they had two drawbacks:</p> <ol> <li>These specialized endpoints were eclipsed by techniques that achieved better results.</li> <li>These specialized endpoints were more difficult to customize and optimize for individual use cases.</li> </ol> <p>As a result, the Answers, Classifications, and Search endpoints are being deprecated.</p>"},{"location":"transition_guides_for_deprecated_API_endpoints/#timeline-of-deprecation","title":"Timeline of deprecation","text":"<p>For those who have not used these endpoints, nothing will change except that access will no longer be available.</p> <p>For existing users of these endpoints, access will continue until December 3, 2022. Before that date, we strongly encourage developers to switch over to newer techniques which produce better results.</p>"},{"location":"transition_guides_for_deprecated_API_endpoints/#how-to-transition","title":"How to transition","text":"<p>We've written guides and code examples for transitioning from the deprecated API endpoints to better methods.</p>"},{"location":"transition_guides_for_deprecated_API_endpoints/#answers","title":"Answers","text":"<p>Guide: How to transition off the Answers endpoint</p> <ul> <li>Option 1: transition to embeddings-based search (recommended)</li> <li> <p>Example code: Semantic_text_search_using_embeddings.ipynb</p> </li> <li> <p>Option 2: reimplement Answers endpoint functionality</p> </li> <li>Example code: answers_functionality_example.py</li> </ul>"},{"location":"transition_guides_for_deprecated_API_endpoints/#classification","title":"Classification","text":"<p>Guide: How to transition off the Classifications endpoint</p> <ul> <li>Option 1: transition to fine-tuning (recommended)</li> <li>Example code: Fine-tuned_classification.ipynb</li> <li>Option 2: transition to embeddings</li> <li>Example code: Semantic_text_search_using_embeddings.ipynb</li> <li>Option 3: reimplement Classifications endpoint functionality</li> <li>Example code: classification_functionality_example.py</li> </ul>"},{"location":"transition_guides_for_deprecated_API_endpoints/#search","title":"Search","text":"<p>Guide: How to transition off the Search endpoint</p> <ul> <li>Option 1: transition to embeddings-based search (recommended)</li> <li>Example code: Semantic_text_search_using_embeddings.ipynb</li> <li>Option 2: reimplement Search endpoint functionality</li> <li>Example code: search_functionality_example.py</li> </ul>"},{"location":"transition_guides_for_deprecated_API_endpoints/answers_functionality_example/","title":"Answers functionality example","text":"In\u00a0[\u00a0]: Copied! <pre>from transformers import GPT2TokenizerFast\n</pre> from transformers import GPT2TokenizerFast In\u00a0[\u00a0]: Copied! <pre>import openai\n</pre> import openai In\u00a0[\u00a0]: Copied! <pre>tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n</pre> tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\") In\u00a0[\u00a0]: Copied! <pre>MAX_TOKENS_LIMIT = 2048\nANSWERS_INSTRUCTION = \"Please answer the question according to the above context.\\n\"\nCONTEXT_TEMPLATE = \"===\\nContext: {context}\\n===\\n\"\n</pre> MAX_TOKENS_LIMIT = 2048 ANSWERS_INSTRUCTION = \"Please answer the question according to the above context.\\n\" CONTEXT_TEMPLATE = \"===\\nContext: {context}\\n===\\n\" In\u00a0[\u00a0]: Copied! <pre>def extract_instruction(instruction):\n\"\"\"\n    Extract `instruction` parameter and format it properly.\n    If not exist, return empty string.\n    \"\"\"\n    if instruction is None:\n        return \"\"\n\n    return f\"{instruction.strip()}\\n\\n\"\n</pre> def extract_instruction(instruction):     \"\"\"     Extract `instruction` parameter and format it properly.     If not exist, return empty string.     \"\"\"     if instruction is None:         return \"\"      return f\"{instruction.strip()}\\n\\n\" In\u00a0[\u00a0]: Copied! <pre>def semantic_search(\n    search_model, query_for_search, file_id=None, max_documents=None, examples=None\n):\n\"\"\"\n    :param examples: A list of {\"text\":...} or {\"text\": ..., \"label\": ...}.\n    :return:\n        a list of semantic search result dict of documents sorted by \"score\":\n        [\n            {\n                \"document\": ...,\n                \"object\": \"search_result\",\n                \"score\": ...,\n                \"text\": ...,\n            },\n            ...\n        ]\n    \"\"\"\n    assert (examples is None) ^ (file_id is None)  # xor\n\n    if file_id is not None:\n        # This is where you'd do an elastic search call.  Since there isn't an example of this\n        # we can query, we'll raise an error.\n        # The return value from this would be a list of examples\n        raise NotImplementedError()\n\n    # This isn't quite accurate since Search is also being deprecated. See our search guide for more\n    # information.\n\n    search_result = openai.Search.create(\n        model=search_model,\n        documents=[x[\"text\"] for x in examples],\n        query=query_for_search,\n    )\n\n    info_dict = {d[\"document\"]: d for d in search_result[\"data\"]}\n    sorted_doc_ids = sorted(\n        info_dict.keys(), key=lambda x: info_dict[x][\"score\"], reverse=True\n    )\n    if max_documents:\n        sorted_doc_ids = sorted_doc_ids[:max_documents]\n    return [info_dict[i] for i in sorted_doc_ids]\n</pre> def semantic_search(     search_model, query_for_search, file_id=None, max_documents=None, examples=None ):     \"\"\"     :param examples: A list of {\"text\":...} or {\"text\": ..., \"label\": ...}.     :return:         a list of semantic search result dict of documents sorted by \"score\":         [             {                 \"document\": ...,                 \"object\": \"search_result\",                 \"score\": ...,                 \"text\": ...,             },             ...         ]     \"\"\"     assert (examples is None) ^ (file_id is None)  # xor      if file_id is not None:         # This is where you'd do an elastic search call.  Since there isn't an example of this         # we can query, we'll raise an error.         # The return value from this would be a list of examples         raise NotImplementedError()      # This isn't quite accurate since Search is also being deprecated. See our search guide for more     # information.      search_result = openai.Search.create(         model=search_model,         documents=[x[\"text\"] for x in examples],         query=query_for_search,     )      info_dict = {d[\"document\"]: d for d in search_result[\"data\"]}     sorted_doc_ids = sorted(         info_dict.keys(), key=lambda x: info_dict[x][\"score\"], reverse=True     )     if max_documents:         sorted_doc_ids = sorted_doc_ids[:max_documents]     return [info_dict[i] for i in sorted_doc_ids] In\u00a0[\u00a0]: Copied! <pre>def select_by_length(\n    sorted_doc_infos,\n    max_token_len,\n    lambda_fn=None,\n):\n\"\"\"\n    Give a list of (document ID, document content in string), we will select as many\n    documents as possible as long as the total length does not go above `max_token_len`.\n\n    :param sorted_doc_infos: A list of semantic search result dict of documents sorted by \"score\".\n    :param max_token_len: The maximum token length for selected documents.\n    :param lambda_fn: A function that takes in search results dict and output a formatted\n        example for context stuffing.\n    :return: A tuple of (\n        A concatenation of selected documents used as context,\n        A list of selected document IDs\n    )\n    \"\"\"\n    if not sorted_doc_infos:\n        return \"\", []\n\n    selected_indices = []\n    total_doc_tokens = 0\n    doc_dict = {}\n    for i, doc_info in enumerate(sorted_doc_infos):\n        doc = lambda_fn(doc_info) if lambda_fn else doc_info[\"text\"]\n        n_doc_tokens = len(tokenizer.encode(doc))\n        if total_doc_tokens + n_doc_tokens &lt; max_token_len:\n            total_doc_tokens += n_doc_tokens\n            selected_indices.append(i)\n            doc_dict[i] = doc\n\n    # The top ranked documents should go at the end.\n    selected_indices = selected_indices[::-1]\n\n    context = \"\".join([doc_dict[i] for i in selected_indices])\n    selected_doc_infos = [sorted_doc_infos[i] for i in selected_indices]\n    return context, selected_doc_infos\n</pre> def select_by_length(     sorted_doc_infos,     max_token_len,     lambda_fn=None, ):     \"\"\"     Give a list of (document ID, document content in string), we will select as many     documents as possible as long as the total length does not go above `max_token_len`.      :param sorted_doc_infos: A list of semantic search result dict of documents sorted by \"score\".     :param max_token_len: The maximum token length for selected documents.     :param lambda_fn: A function that takes in search results dict and output a formatted         example for context stuffing.     :return: A tuple of (         A concatenation of selected documents used as context,         A list of selected document IDs     )     \"\"\"     if not sorted_doc_infos:         return \"\", []      selected_indices = []     total_doc_tokens = 0     doc_dict = {}     for i, doc_info in enumerate(sorted_doc_infos):         doc = lambda_fn(doc_info) if lambda_fn else doc_info[\"text\"]         n_doc_tokens = len(tokenizer.encode(doc))         if total_doc_tokens + n_doc_tokens &lt; max_token_len:             total_doc_tokens += n_doc_tokens             selected_indices.append(i)             doc_dict[i] = doc      # The top ranked documents should go at the end.     selected_indices = selected_indices[::-1]      context = \"\".join([doc_dict[i] for i in selected_indices])     selected_doc_infos = [sorted_doc_infos[i] for i in selected_indices]     return context, selected_doc_infos In\u00a0[\u00a0]: Copied! <pre>def answers(\n    examples,\n    question,\n    model,\n    examples_context,\n    file_id=None,\n    documents=None,\n    logit_bias=None,\n    max_rerank=200,\n    max_tokens=16,\n    alternative_question=None,\n    search_model=\"ada\",\n    temperature=0.0,\n    logprobs=0,\n    stop=None,\n    n=1,\n):\n\"\"\"\n    Given a prompt, a question, a list of (question, answer) pairs as examples, and\n    a list of documents for context, it tries to include all the QA examples and top\n    relevant context documents.\n\n    The constructed prompt for the final completion call:\n    ```\n    Please answer the question according to the above context.\n\n    ===\n    Context: {{ the context for example QA pairs. }}\n    ===\n    Q: example 1 question\n    A: example 1 answer\n    ---\n    Q: example 2 question\n    A: example 2 answer\n    ===\n    Context: {{ a list of relevant documents sorted via search(question, documents) }}\n    ===\n    Q: question\n    A:\n    ```\n\n    The returned object has a structure like:\n    {\n      \"answers\": [\n        \"Beijing\",\n        \"Beijing, China\"\n      ],\n      \"completion_id\": \"xxx-xxx\",\n      \"object\": \"answer\",\n      \"selected_documents\": [\n        {\n            \"document\": ...,    # document index, same as in search/ results.\n            \"object\": \"search_result\",\n            \"text\": ...,\n        },\n        ...\n      ],\n    }\n    \"\"\"\n\n    examples = examples if examples else []\n\n    example_prompts = [f\"Q: {x}\\nA: {y}\" for x, y in examples]\n    prompt = f\"Q: {question}\\nA:\"\n\n    # Append all the QA examples into the prompt.\n    if examples_context:\n        examples_context = CONTEXT_TEMPLATE.format(context=examples_context)\n    instruction = (\n        ANSWERS_INSTRUCTION + examples_context + \"\\n---\\n\".join(example_prompts) + \"\\n\"\n    )\n\n    logit_bias = logit_bias if logit_bias is not None else {}\n\n    if file_id is None and documents is None:\n        raise Exception(\"Please submit at least one of `documents` or `file`.\")\n    if file_id is not None and documents is not None:\n        raise Exception(\"Please submit only one of `documents` or `file`.\")\n\n    instruction = extract_instruction(instruction)\n\n    n_instruction_tokens = len(tokenizer.encode(instruction))\n    n_prompt_tokens = len(tokenizer.encode(prompt))\n    n_query_tokens = len(tokenizer.encode(question))\n    n_context_tokens = len(tokenizer.encode(CONTEXT_TEMPLATE.format(context=\"\")))\n\n    if documents is not None:\n        documents = [doc.strip() + \" \" for doc in documents]\n        n_docs_tokens = [len(tokenizer.encode(doc)) for doc in documents]\n\n    # Except all the required content, how many tokens left for context stuffing.\n    leftover_token_len = MAX_TOKENS_LIMIT - (\n        n_instruction_tokens + n_context_tokens + n_prompt_tokens + max_tokens\n    )\n    sorted_doc_infos = []\n\n    question_for_search = (\n        alternative_question if alternative_question is not None else question\n    )\n    if file_id is not None:\n        search_model_, sorted_doc_infos = semantic_search(\n            search_model,\n            question_for_search,\n            file_id=file_id,\n            max_documents=max_rerank,\n        )\n\n    elif len(documents) == 0:\n        # If no context document is provided, do nothing.\n        pass\n\n    elif min(n_docs_tokens) &gt;= leftover_token_len:\n        # If there is no room for adding any context doc.\n        pass\n\n    elif (max_rerank is None or max_rerank &gt;= len(documents)) and sum(\n        n_docs_tokens\n    ) &lt; leftover_token_len:\n        # If the total length of docs is short enough to be added all.\n        selected_indices = list(range(len(documents)))\n\n        sorted_doc_infos = [\n            {\"document\": i, \"text\": documents[i]} for i in selected_indices\n        ]\n\n    elif n_query_tokens + max(n_docs_tokens) &gt;= MAX_TOKENS_LIMIT:\n        # If the prompt and the longest document together go above the limit.\n        total_tokens = n_query_tokens + max(n_docs_tokens)\n        raise Exception(\n            f\"The longest document and prompt pair together contains {total_tokens} \"\n            f\"tokens, above the limit {MAX_TOKENS_LIMIT} for semantic search. Please consider \"\n            f\"shortening the prompt or the longest document.\"\n        )\n\n    else:\n        # If we can add some context documents but not all of them, we should\n        # query search endpoint to rank docs by score.\n        sorted_doc_infos = semantic_search(\n            search_model,\n            question_for_search,\n            examples=[{\"text\": doc} for doc in documents],\n            max_documents=max_rerank,\n        )\n\n    # Select documents w.r.t. the context length limitation.\n    context, sorted_doc_infos = select_by_length(\n        sorted_doc_infos,\n        leftover_token_len,\n        lambda_fn=lambda x: x[\"text\"].strip() + \" \",\n    )\n\n    # Add instruction before the context and the prompt after the context.\n    if context:\n        context = CONTEXT_TEMPLATE.format(context=context.strip())\n    full_prompt = instruction + context + prompt\n\n    completion_result = openai.Completion.create(\n        engine=model,\n        prompt=full_prompt,\n        logit_bias=logit_bias,\n        temperature=temperature,\n        n=n,\n        max_tokens=max_tokens,\n        stop=stop,\n        logprobs=logprobs,\n    )\n\n    completion_result[\"selected_documents\"] = sorted_doc_infos\n\n    result = dict(\n        object=\"answer\",\n        selected_documents=completion_result.pop(\"selected_documents\"),\n        completion=completion_result[\"id\"],\n    )\n\n    result[\"answers\"] = [\n        item[\"text\"].replace(\"A:\", \"\").split(\"Q:\")[0].strip()\n        for item in completion_result[\"choices\"]\n    ]\n\n    return result\n</pre> def answers(     examples,     question,     model,     examples_context,     file_id=None,     documents=None,     logit_bias=None,     max_rerank=200,     max_tokens=16,     alternative_question=None,     search_model=\"ada\",     temperature=0.0,     logprobs=0,     stop=None,     n=1, ):     \"\"\"     Given a prompt, a question, a list of (question, answer) pairs as examples, and     a list of documents for context, it tries to include all the QA examples and top     relevant context documents.      The constructed prompt for the final completion call:     ```     Please answer the question according to the above context.      ===     Context: {{ the context for example QA pairs. }}     ===     Q: example 1 question     A: example 1 answer     ---     Q: example 2 question     A: example 2 answer     ===     Context: {{ a list of relevant documents sorted via search(question, documents) }}     ===     Q: question     A:     ```      The returned object has a structure like:     {       \"answers\": [         \"Beijing\",         \"Beijing, China\"       ],       \"completion_id\": \"xxx-xxx\",       \"object\": \"answer\",       \"selected_documents\": [         {             \"document\": ...,    # document index, same as in search/ results.             \"object\": \"search_result\",             \"text\": ...,         },         ...       ],     }     \"\"\"      examples = examples if examples else []      example_prompts = [f\"Q: {x}\\nA: {y}\" for x, y in examples]     prompt = f\"Q: {question}\\nA:\"      # Append all the QA examples into the prompt.     if examples_context:         examples_context = CONTEXT_TEMPLATE.format(context=examples_context)     instruction = (         ANSWERS_INSTRUCTION + examples_context + \"\\n---\\n\".join(example_prompts) + \"\\n\"     )      logit_bias = logit_bias if logit_bias is not None else {}      if file_id is None and documents is None:         raise Exception(\"Please submit at least one of `documents` or `file`.\")     if file_id is not None and documents is not None:         raise Exception(\"Please submit only one of `documents` or `file`.\")      instruction = extract_instruction(instruction)      n_instruction_tokens = len(tokenizer.encode(instruction))     n_prompt_tokens = len(tokenizer.encode(prompt))     n_query_tokens = len(tokenizer.encode(question))     n_context_tokens = len(tokenizer.encode(CONTEXT_TEMPLATE.format(context=\"\")))      if documents is not None:         documents = [doc.strip() + \" \" for doc in documents]         n_docs_tokens = [len(tokenizer.encode(doc)) for doc in documents]      # Except all the required content, how many tokens left for context stuffing.     leftover_token_len = MAX_TOKENS_LIMIT - (         n_instruction_tokens + n_context_tokens + n_prompt_tokens + max_tokens     )     sorted_doc_infos = []      question_for_search = (         alternative_question if alternative_question is not None else question     )     if file_id is not None:         search_model_, sorted_doc_infos = semantic_search(             search_model,             question_for_search,             file_id=file_id,             max_documents=max_rerank,         )      elif len(documents) == 0:         # If no context document is provided, do nothing.         pass      elif min(n_docs_tokens) &gt;= leftover_token_len:         # If there is no room for adding any context doc.         pass      elif (max_rerank is None or max_rerank &gt;= len(documents)) and sum(         n_docs_tokens     ) &lt; leftover_token_len:         # If the total length of docs is short enough to be added all.         selected_indices = list(range(len(documents)))          sorted_doc_infos = [             {\"document\": i, \"text\": documents[i]} for i in selected_indices         ]      elif n_query_tokens + max(n_docs_tokens) &gt;= MAX_TOKENS_LIMIT:         # If the prompt and the longest document together go above the limit.         total_tokens = n_query_tokens + max(n_docs_tokens)         raise Exception(             f\"The longest document and prompt pair together contains {total_tokens} \"             f\"tokens, above the limit {MAX_TOKENS_LIMIT} for semantic search. Please consider \"             f\"shortening the prompt or the longest document.\"         )      else:         # If we can add some context documents but not all of them, we should         # query search endpoint to rank docs by score.         sorted_doc_infos = semantic_search(             search_model,             question_for_search,             examples=[{\"text\": doc} for doc in documents],             max_documents=max_rerank,         )      # Select documents w.r.t. the context length limitation.     context, sorted_doc_infos = select_by_length(         sorted_doc_infos,         leftover_token_len,         lambda_fn=lambda x: x[\"text\"].strip() + \" \",     )      # Add instruction before the context and the prompt after the context.     if context:         context = CONTEXT_TEMPLATE.format(context=context.strip())     full_prompt = instruction + context + prompt      completion_result = openai.Completion.create(         engine=model,         prompt=full_prompt,         logit_bias=logit_bias,         temperature=temperature,         n=n,         max_tokens=max_tokens,         stop=stop,         logprobs=logprobs,     )      completion_result[\"selected_documents\"] = sorted_doc_infos      result = dict(         object=\"answer\",         selected_documents=completion_result.pop(\"selected_documents\"),         completion=completion_result[\"id\"],     )      result[\"answers\"] = [         item[\"text\"].replace(\"A:\", \"\").split(\"Q:\")[0].strip()         for item in completion_result[\"choices\"]     ]      return result In\u00a0[\u00a0]: Copied! <pre>print(\n    answers(\n        examples=[\n            [\"What is the capital of Washington\", \"Olympia\"],\n            [\"What is the capital of Oregon\", \"Salem\"],\n        ],\n        question=\"What is the capital of China?\",\n        examples_context=\"I am a bot that names country capitals\",\n        documents=[\"I am a bot that names country capitals\"],\n        model=\"davinci\",\n        search_model=\"ada\",\n        alternative_question=\"different test\",\n        max_tokens=16,\n        stop=[\"\\n\\n\"],\n    )\n)\n</pre> print(     answers(         examples=[             [\"What is the capital of Washington\", \"Olympia\"],             [\"What is the capital of Oregon\", \"Salem\"],         ],         question=\"What is the capital of China?\",         examples_context=\"I am a bot that names country capitals\",         documents=[\"I am a bot that names country capitals\"],         model=\"davinci\",         search_model=\"ada\",         alternative_question=\"different test\",         max_tokens=16,         stop=[\"\\n\\n\"],     ) )"},{"location":"transition_guides_for_deprecated_API_endpoints/classification_functionality_example/","title":"Classification functionality example","text":"In\u00a0[\u00a0]: Copied! <pre>import itertools\nfrom collections import defaultdict\n</pre> import itertools from collections import defaultdict In\u00a0[\u00a0]: Copied! <pre>from transformers import GPT2TokenizerFast\n</pre> from transformers import GPT2TokenizerFast In\u00a0[\u00a0]: Copied! <pre>import openai\n</pre> import openai In\u00a0[\u00a0]: Copied! <pre>tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n</pre> tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\") In\u00a0[\u00a0]: Copied! <pre>MAX_TOKENS_LIMIT = 2048\n</pre> MAX_TOKENS_LIMIT = 2048 In\u00a0[\u00a0]: Copied! <pre>def create_instruction(labels) -&gt; str:\n\"\"\"\n    Construct an instruction for a classification task.\n    \"\"\"\n    instruction = f\"Please classify a piece of text into the following categories: {', '.join(labels)}.\"\n\n    return f\"{instruction.strip()}\\n\\n\"\n</pre> def create_instruction(labels) -&gt; str:     \"\"\"     Construct an instruction for a classification task.     \"\"\"     instruction = f\"Please classify a piece of text into the following categories: {', '.join(labels)}.\"      return f\"{instruction.strip()}\\n\\n\" In\u00a0[\u00a0]: Copied! <pre>def semantic_search(\n    search_model, query_for_search, file_id=None, max_documents=None, examples=None\n):\n\"\"\"\n    :param examples: A list of {\"text\":...} or {\"text\": ..., \"label\": ...}.\n    :return:\n        a list of semantic search result dict of documents sorted by \"score\":\n        [\n            {\n                \"document\": ...,\n                \"object\": \"search_result\",\n                \"score\": ...,\n                \"text\": ...,\n            },\n            ...\n        ]\n\n    \"\"\"\n    assert (examples is None) ^ (file_id is None)  # xor\n\n    if file_id is not None:\n        # This is where you'd do an elastic search call.  Since there isn't an example of this\n        # we can query, we'll raise an error.\n        # The return value from this would be a list of examples\n        raise NotImplementedError()\n\n    # This isn't quite accurate since Search is also being deprecated. See our search guide for more\n    # information.\n\n    search_result = openai.Search.create(\n        model=search_model,\n        documents=[x[\"text\"] for x in examples],\n        query=query_for_search,\n    )\n\n    info_dict = {d[\"document\"]: d for d in search_result[\"data\"]}\n    sorted_doc_ids = sorted(\n        info_dict.keys(), key=lambda x: info_dict[x][\"score\"], reverse=True\n    )\n    if max_documents:\n        sorted_doc_ids = sorted_doc_ids[:max_documents]\n    return [info_dict[i] for i in sorted_doc_ids]\n</pre> def semantic_search(     search_model, query_for_search, file_id=None, max_documents=None, examples=None ):     \"\"\"     :param examples: A list of {\"text\":...} or {\"text\": ..., \"label\": ...}.     :return:         a list of semantic search result dict of documents sorted by \"score\":         [             {                 \"document\": ...,                 \"object\": \"search_result\",                 \"score\": ...,                 \"text\": ...,             },             ...         ]      \"\"\"     assert (examples is None) ^ (file_id is None)  # xor      if file_id is not None:         # This is where you'd do an elastic search call.  Since there isn't an example of this         # we can query, we'll raise an error.         # The return value from this would be a list of examples         raise NotImplementedError()      # This isn't quite accurate since Search is also being deprecated. See our search guide for more     # information.      search_result = openai.Search.create(         model=search_model,         documents=[x[\"text\"] for x in examples],         query=query_for_search,     )      info_dict = {d[\"document\"]: d for d in search_result[\"data\"]}     sorted_doc_ids = sorted(         info_dict.keys(), key=lambda x: info_dict[x][\"score\"], reverse=True     )     if max_documents:         sorted_doc_ids = sorted_doc_ids[:max_documents]     return [info_dict[i] for i in sorted_doc_ids] In\u00a0[\u00a0]: Copied! <pre>def select_by_length(\n    sorted_doc_infos,\n    max_token_len,\n    lambda_fn=None,\n):\n\"\"\"\n    Give a list of (document ID, document content in string), we will select as many\n    documents as possible as long as the total length does not go above `max_token_len`.\n\n    :param sorted_doc_infos: A list of semantic search result dict of documents sorted by \"score\".\n    :param max_token_len: The maximum token length for selected documents.\n    :param lambda_fn: A function that takes in search results dict and output a formatted\n        example for context stuffing.\n    :return: A tuple of (\n        A concatenation of selected documents used as context,\n        A list of selected document IDs\n    )\n    \"\"\"\n    if not sorted_doc_infos:\n        return \"\", []\n\n    selected_indices = []\n    total_doc_tokens = 0\n    doc_dict = {}\n    for i, doc_info in enumerate(sorted_doc_infos):\n        doc = lambda_fn(doc_info) if lambda_fn else doc_info[\"text\"]\n        n_doc_tokens = len(tokenizer.encode(doc))\n        if total_doc_tokens + n_doc_tokens &lt; max_token_len:\n            total_doc_tokens += n_doc_tokens\n            selected_indices.append(i)\n            doc_dict[i] = doc\n\n    # The top ranked documents should go at the end.\n    selected_indices = selected_indices[::-1]\n\n    context = \"\".join([doc_dict[i] for i in selected_indices])\n    selected_doc_infos = [sorted_doc_infos[i] for i in selected_indices]\n    return context, selected_doc_infos\n</pre> def select_by_length(     sorted_doc_infos,     max_token_len,     lambda_fn=None, ):     \"\"\"     Give a list of (document ID, document content in string), we will select as many     documents as possible as long as the total length does not go above `max_token_len`.      :param sorted_doc_infos: A list of semantic search result dict of documents sorted by \"score\".     :param max_token_len: The maximum token length for selected documents.     :param lambda_fn: A function that takes in search results dict and output a formatted         example for context stuffing.     :return: A tuple of (         A concatenation of selected documents used as context,         A list of selected document IDs     )     \"\"\"     if not sorted_doc_infos:         return \"\", []      selected_indices = []     total_doc_tokens = 0     doc_dict = {}     for i, doc_info in enumerate(sorted_doc_infos):         doc = lambda_fn(doc_info) if lambda_fn else doc_info[\"text\"]         n_doc_tokens = len(tokenizer.encode(doc))         if total_doc_tokens + n_doc_tokens &lt; max_token_len:             total_doc_tokens += n_doc_tokens             selected_indices.append(i)             doc_dict[i] = doc      # The top ranked documents should go at the end.     selected_indices = selected_indices[::-1]      context = \"\".join([doc_dict[i] for i in selected_indices])     selected_doc_infos = [sorted_doc_infos[i] for i in selected_indices]     return context, selected_doc_infos In\u00a0[\u00a0]: Copied! <pre>def format_example_fn(x: dict) -&gt; str:\n    return \"Text: {text}\\nCategory: {label}\\n---\\n\".format(\n        text=x[\"text\"].replace(\"\\n\", \" \").strip(),\n        label=x[\"label\"].replace(\"\\n\", \" \").strip(),\n    )\n</pre> def format_example_fn(x: dict) -&gt; str:     return \"Text: {text}\\nCategory: {label}\\n---\\n\".format(         text=x[\"text\"].replace(\"\\n\", \" \").strip(),         label=x[\"label\"].replace(\"\\n\", \" \").strip(),     ) In\u00a0[\u00a0]: Copied! <pre>def classifications(\n    query,\n    model,\n    search_model=\"ada\",\n    examples=None,\n    file=None,\n    labels=None,\n    temperature=0.0,\n    logprobs=None,\n    max_examples=200,\n    logit_bias=None,\n    alternative_query=None,\n    max_tokens=16,\n) -&gt; dict:\n\"\"\"\n    Given a prompt, a question and a list of examples, containing (text, label) pairs,\n    it selects top relevant examples to construct a prompt for few-shot classification.\n\n    The constructed prompt for the final completion call:\n    ```\n    {{ an optional instruction }}\n\n    Text: example 1 text\n    Category: example 1 label\n    ---\n    Text: example 1 text\n    Category: example 2 label\n    ---\n    Text: question\n    Category:\n    ```\n\n    The returned object has a structure like:\n    {\n      \"label\": \"Happy\",\n      \"model\": \"ada\",\n      \"object\": \"classification\",\n      \"selected_examples\": [\n        {\n            \"document\": ...,    # document index, same as in search/ results.\n            \"text\": ...,\n            \"label\": ...,\n        },\n        ...\n      ],\n    }\n    \"\"\"\n\n    query = query.replace(\"\\n\", \" \").strip()\n    logit_bias = logit_bias if logit_bias else {}\n    labels = labels if labels else []\n\n    if file is None and examples is None:\n        raise Exception(\"Please submit at least one of `examples` or `file`.\")\n    if file is not None and examples is not None:\n        raise Exception(\"Please submit only one of `examples` or `file`.\")\n\n    instruction = create_instruction(labels)\n\n    query_for_search = alternative_query if alternative_query is not None else query\n\n    # Extract examples and example labels first.\n    if file is not None:\n        sorted_doc_infos = semantic_search(\n            search_model,\n            query_for_search,\n            file_id=file,\n            max_documents=max_examples,\n        )\n\n    else:\n        example_prompts = [\n            format_example_fn(dict(text=x, label=y)) for x, y in examples\n        ]\n        n_examples_tokens = [len(tokenizer.encode(x)) for x in example_prompts]\n\n    query_prompt = f\"Text: {query}\\nCategory:\"\n    n_instruction_tokens = len(tokenizer.encode(instruction))\n    n_query_tokens = len(tokenizer.encode(query_prompt))\n\n    # Except all the required content, how many tokens left for context stuffing.\n    leftover_token_len = MAX_TOKENS_LIMIT - (\n        n_instruction_tokens + n_query_tokens + max_tokens\n    )\n\n    # Process when `examples` are provided but no `file` is provided.\n    if examples:\n        if (max_examples is None or max_examples &gt;= len(examples)) and sum(\n            n_examples_tokens\n        ) &lt; leftover_token_len:\n            # If the total length of docs is short enough that we can add all examples, no search call.\n            selected_indices = list(range(len(examples)))\n\n            sorted_doc_infos = [\n                {\"document\": i, \"text\": examples[i][0], \"label\": examples[i][1]}\n                for i in selected_indices\n            ]\n\n        elif max(n_examples_tokens) + n_query_tokens &gt;= MAX_TOKENS_LIMIT:\n            # If the prompt and the longest example together go above the limit:\n            total_tokens = max(n_examples_tokens) + n_query_tokens\n            raise Exception(\n                user_message=f\"The longest classification example, query and prompt together contain \"\n                f\"{total_tokens} tokens, above the limit {MAX_TOKENS_LIMIT} for semantic search. \"\n                f\"Please consider shortening your instruction, query or the longest example.\"\n            )\n\n        else:\n            # If we can add some context documents but not all of them, we should\n            # query search endpoint to rank docs by score.\n            sorted_doc_infos = semantic_search(\n                search_model,\n                query_for_search,\n                examples=[{\"text\": x, \"label\": y} for x, y in examples],\n                max_documents=max_examples,\n            )\n\n    # Per label, we have a list of doc id sorted by its relevancy to the query.\n    label_to_indices = defaultdict(list)\n    for idx, d in enumerate(sorted_doc_infos):\n        label_to_indices[d[\"label\"]].append(idx)\n\n    # Do a round robin for each of the different labels, taking the best match for each label.\n    label_indices = [label_to_indices[label] for label in labels]\n    mixed_indices = [\n        i for x in itertools.zip_longest(*label_indices) for i in x if i is not None\n    ]\n    sorted_doc_infos = [sorted_doc_infos[i] for i in mixed_indices]\n\n    # Try to select as many examples as needed to fit into the context\n    context, sorted_doc_infos = select_by_length(\n        sorted_doc_infos,\n        leftover_token_len,\n        lambda_fn=format_example_fn,\n    )\n\n    prompt = instruction + context + query_prompt\n\n    completion_params = {\n        \"engine\": model,\n        \"prompt\": prompt,\n        \"temperature\": temperature,\n        \"logprobs\": logprobs,\n        \"logit_bias\": logit_bias,\n        \"max_tokens\": max_tokens,\n        \"stop\": \"\\n\",\n        \"n\": 1,\n    }\n\n    completion_resp = openai.Completion.create(\n        **completion_params,\n    )\n\n    label = completion_resp[\"choices\"][0][\"text\"]\n    label = label.split(\"\\n\")[0].strip().lower().capitalize()\n    if label not in labels:\n        label = \"Unknown\"\n\n    result = dict(\n        # TODO: Add id for object persistence.\n        object=\"classification\",\n        model=completion_resp[\"model\"],\n        label=label,\n        completion=completion_resp[\"id\"],\n    )\n\n    result[\"selected_examples\"] = sorted_doc_infos\n\n    return result\n</pre> def classifications(     query,     model,     search_model=\"ada\",     examples=None,     file=None,     labels=None,     temperature=0.0,     logprobs=None,     max_examples=200,     logit_bias=None,     alternative_query=None,     max_tokens=16, ) -&gt; dict:     \"\"\"     Given a prompt, a question and a list of examples, containing (text, label) pairs,     it selects top relevant examples to construct a prompt for few-shot classification.      The constructed prompt for the final completion call:     ```     {{ an optional instruction }}      Text: example 1 text     Category: example 1 label     ---     Text: example 1 text     Category: example 2 label     ---     Text: question     Category:     ```      The returned object has a structure like:     {       \"label\": \"Happy\",       \"model\": \"ada\",       \"object\": \"classification\",       \"selected_examples\": [         {             \"document\": ...,    # document index, same as in search/ results.             \"text\": ...,             \"label\": ...,         },         ...       ],     }     \"\"\"      query = query.replace(\"\\n\", \" \").strip()     logit_bias = logit_bias if logit_bias else {}     labels = labels if labels else []      if file is None and examples is None:         raise Exception(\"Please submit at least one of `examples` or `file`.\")     if file is not None and examples is not None:         raise Exception(\"Please submit only one of `examples` or `file`.\")      instruction = create_instruction(labels)      query_for_search = alternative_query if alternative_query is not None else query      # Extract examples and example labels first.     if file is not None:         sorted_doc_infos = semantic_search(             search_model,             query_for_search,             file_id=file,             max_documents=max_examples,         )      else:         example_prompts = [             format_example_fn(dict(text=x, label=y)) for x, y in examples         ]         n_examples_tokens = [len(tokenizer.encode(x)) for x in example_prompts]      query_prompt = f\"Text: {query}\\nCategory:\"     n_instruction_tokens = len(tokenizer.encode(instruction))     n_query_tokens = len(tokenizer.encode(query_prompt))      # Except all the required content, how many tokens left for context stuffing.     leftover_token_len = MAX_TOKENS_LIMIT - (         n_instruction_tokens + n_query_tokens + max_tokens     )      # Process when `examples` are provided but no `file` is provided.     if examples:         if (max_examples is None or max_examples &gt;= len(examples)) and sum(             n_examples_tokens         ) &lt; leftover_token_len:             # If the total length of docs is short enough that we can add all examples, no search call.             selected_indices = list(range(len(examples)))              sorted_doc_infos = [                 {\"document\": i, \"text\": examples[i][0], \"label\": examples[i][1]}                 for i in selected_indices             ]          elif max(n_examples_tokens) + n_query_tokens &gt;= MAX_TOKENS_LIMIT:             # If the prompt and the longest example together go above the limit:             total_tokens = max(n_examples_tokens) + n_query_tokens             raise Exception(                 user_message=f\"The longest classification example, query and prompt together contain \"                 f\"{total_tokens} tokens, above the limit {MAX_TOKENS_LIMIT} for semantic search. \"                 f\"Please consider shortening your instruction, query or the longest example.\"             )          else:             # If we can add some context documents but not all of them, we should             # query search endpoint to rank docs by score.             sorted_doc_infos = semantic_search(                 search_model,                 query_for_search,                 examples=[{\"text\": x, \"label\": y} for x, y in examples],                 max_documents=max_examples,             )      # Per label, we have a list of doc id sorted by its relevancy to the query.     label_to_indices = defaultdict(list)     for idx, d in enumerate(sorted_doc_infos):         label_to_indices[d[\"label\"]].append(idx)      # Do a round robin for each of the different labels, taking the best match for each label.     label_indices = [label_to_indices[label] for label in labels]     mixed_indices = [         i for x in itertools.zip_longest(*label_indices) for i in x if i is not None     ]     sorted_doc_infos = [sorted_doc_infos[i] for i in mixed_indices]      # Try to select as many examples as needed to fit into the context     context, sorted_doc_infos = select_by_length(         sorted_doc_infos,         leftover_token_len,         lambda_fn=format_example_fn,     )      prompt = instruction + context + query_prompt      completion_params = {         \"engine\": model,         \"prompt\": prompt,         \"temperature\": temperature,         \"logprobs\": logprobs,         \"logit_bias\": logit_bias,         \"max_tokens\": max_tokens,         \"stop\": \"\\n\",         \"n\": 1,     }      completion_resp = openai.Completion.create(         **completion_params,     )      label = completion_resp[\"choices\"][0][\"text\"]     label = label.split(\"\\n\")[0].strip().lower().capitalize()     if label not in labels:         label = \"Unknown\"      result = dict(         # TODO: Add id for object persistence.         object=\"classification\",         model=completion_resp[\"model\"],         label=label,         completion=completion_resp[\"id\"],     )      result[\"selected_examples\"] = sorted_doc_infos      return result In\u00a0[\u00a0]: Copied! <pre>print(\n    classifications(\n        query=\"this is my test\",\n        model=\"davinci\",\n        search_model=\"ada\",\n        examples=[\n            [\"this is my test\", \"davinci\"],\n            [\"this is other test\", \"blahblah\"],\n        ],\n        file=None,\n        labels=[\"davinci\", \"blahblah\"],\n        temperature=0.1,\n        logprobs=0,\n        max_examples=200,\n        logit_bias=None,\n        alternative_query=\"different test\",\n        max_tokens=16,\n    )\n)\n</pre> print(     classifications(         query=\"this is my test\",         model=\"davinci\",         search_model=\"ada\",         examples=[             [\"this is my test\", \"davinci\"],             [\"this is other test\", \"blahblah\"],         ],         file=None,         labels=[\"davinci\", \"blahblah\"],         temperature=0.1,         logprobs=0,         max_examples=200,         logit_bias=None,         alternative_query=\"different test\",         max_tokens=16,     ) )"},{"location":"transition_guides_for_deprecated_API_endpoints/search_functionality_example/","title":"Search functionality example","text":"In\u00a0[\u00a0]: Copied! <pre>from transformers import GPT2TokenizerFast\n</pre> from transformers import GPT2TokenizerFast In\u00a0[\u00a0]: Copied! <pre>import openai\n</pre> import openai In\u00a0[\u00a0]: Copied! <pre>tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n</pre> tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\") In\u00a0[\u00a0]: Copied! <pre>docs = [\"test1\", \"asdklgjnasdv\", \"banana\", \"lord lollipop\"]\nquery = \"apple orang asdansbdausd\"\n</pre> docs = [\"test1\", \"asdklgjnasdv\", \"banana\", \"lord lollipop\"] query = \"apple orang asdansbdausd\" In\u00a0[\u00a0]: Copied! <pre>def construct_context(query, document):\n    return \"&lt;|endoftext|&gt;{document}\\n\\n---\\n\\nThe above passage is related to: {query}\".format(\n        document=document, query=query\n    )\n</pre> def construct_context(query, document):     return \"&lt;|endoftext|&gt;{document}\\n\\n---\\n\\nThe above passage is related to: {query}\".format(         document=document, query=query     ) In\u00a0[\u00a0]: Copied! <pre>def get_score(context, query, log_probs, text_offsets) -&gt; float:\n    SCORE_MULTIPLIER = 100.0\n\n    log_prob = 0\n    count = 0\n    cutoff = len(context) - len(query)\n\n    for i in range(len(text_offsets) - 1, 0, -1):\n        log_prob += log_probs[i]\n        count += 1\n\n        if text_offsets[i] &lt;= cutoff and text_offsets[i] != text_offsets[i - 1]:\n            break\n\n    return log_prob / float(count) * SCORE_MULTIPLIER\n</pre> def get_score(context, query, log_probs, text_offsets) -&gt; float:     SCORE_MULTIPLIER = 100.0      log_prob = 0     count = 0     cutoff = len(context) - len(query)      for i in range(len(text_offsets) - 1, 0, -1):         log_prob += log_probs[i]         count += 1          if text_offsets[i] &lt;= cutoff and text_offsets[i] != text_offsets[i - 1]:             break      return log_prob / float(count) * SCORE_MULTIPLIER In\u00a0[\u00a0]: Copied! <pre>def search(query, documents, engine):\n\n    prompts = [construct_context(query, doc) for doc in [\"\"] + documents]\n\n    resps = openai.Completion.create(\n        model=engine,\n        prompt=prompts,\n        temperature=1.0,\n        top_p=1.0,\n        max_tokens=0,\n        logprobs=0,\n        n=1,\n        echo=True,\n    )\n\n    resps_by_index = {choice[\"index\"]: choice for choice in resps[\"choices\"]}\n\n    scores = [\n        get_score(\n            prompts[i],\n            query,\n            resps_by_index[i][\"logprobs\"][\"token_logprobs\"],\n            resps_by_index[i][\"logprobs\"][\"text_offset\"],\n        )\n        for i in range(len(prompts))\n    ]\n\n    # Process results\n    scores = [score - scores[0] for score in scores][1:]\n\n    return [\n        {\n            \"object\": \"search_result\",\n            \"document\": document_idx,\n            \"score\": round(score, 3),\n        }\n        for document_idx, score in enumerate(scores)\n    ]\n</pre> def search(query, documents, engine):      prompts = [construct_context(query, doc) for doc in [\"\"] + documents]      resps = openai.Completion.create(         model=engine,         prompt=prompts,         temperature=1.0,         top_p=1.0,         max_tokens=0,         logprobs=0,         n=1,         echo=True,     )      resps_by_index = {choice[\"index\"]: choice for choice in resps[\"choices\"]}      scores = [         get_score(             prompts[i],             query,             resps_by_index[i][\"logprobs\"][\"token_logprobs\"],             resps_by_index[i][\"logprobs\"][\"text_offset\"],         )         for i in range(len(prompts))     ]      # Process results     scores = [score - scores[0] for score in scores][1:]      return [         {             \"object\": \"search_result\",             \"document\": document_idx,             \"score\": round(score, 3),         }         for document_idx, score in enumerate(scores)     ] In\u00a0[\u00a0]: Copied! <pre>print(search(query=query, documents=docs, engine=\"davinci\"))\n</pre> print(search(query=query, documents=docs, engine=\"davinci\"))"}]}